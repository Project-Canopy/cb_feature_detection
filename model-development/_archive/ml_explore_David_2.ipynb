{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "#import rasterio\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import layers\n",
    "import keras\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with Simple CNN and Data Loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data augmentation enabled \n",
      "Training on 200 images \n",
      "Validation on 366 images \n",
      "Your training file is missing positive labels for classes ['0', '2', '3', '7', '8', '9']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "gen = DataLoader(label_file_path_train=\"labels_test_v1.csv\",\n",
    "                label_file_path_val=\"val_labels.csv\",\n",
    "                label_mapping_path=\"labels.json\",\n",
    "                bucket_name='canopy-production-ml',\n",
    "                data_extension_type='.tif',\n",
    "                training_data_shape=(100, 100, 18),\n",
    "                augment=True,\n",
    "                random_flip_up_down=False, #Randomly flips an image vertically (upside down). With a 1 in 2 chance, outputs the contents of `image` flipped along the first dimension, which is `height`.\n",
    "                random_flip_left_right=False,\n",
    "                flip_left_right=False,\n",
    "                flip_up_down=False,\n",
    "                rot90=True,\n",
    "                transpose=False,\n",
    "                enable_shuffle=False,\n",
    "                training_data_shuffle_buffer_size=10,\n",
    "                training_data_batch_size=batch_size,\n",
    "                training_data_type=tf.float32,\n",
    "                label_data_type=tf.uint8,\n",
    "                num_parallel_calls=int(2))\n",
    "# TODO add data augmentation in DataLoader \n",
    "\n",
    "# no_of_val_imgs = len(gen.validation_filenames)\n",
    "# no_of_train_imgs = len(gen.training_filenames)\n",
    "# print(\"Validation on {} images \".format(str(no_of_val_imgs)))\n",
    "# print(\"Training on {} images \".format(str(no_of_train_imgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<data_loader.DataLoader at 0x155732b90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.76875, 4: 0.4270833333333333, 5: 10.25, 6: 3.84375}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0\n",
       "     ..\n",
       "95    1\n",
       "96    1\n",
       "97    1\n",
       "98    0\n",
       "99    0\n",
       "Name: 1, Length: 100, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing positive chips for class 0\n",
      "Missing positive chips for class 2\n",
      "Missing positive chips for class 3\n",
      "Missing positive chips for class 7\n",
      "Missing positive chips for class 8\n",
      "Missing positive chips for class 9\n"
     ]
    }
   ],
   "source": [
    "labels = {}\n",
    "\n",
    "for column in df.columns:\n",
    "    \n",
    "    col_count = df[column].value_counts()\n",
    "#     print(\"column:\",column)\n",
    "#     print(col_count)\n",
    "    \n",
    "    try:\n",
    "        col_count = df[column].value_counts()[1]\n",
    "        labels[column] = col_count\n",
    "    except:\n",
    "        print(f\"Missing positive chips for class {column}. Class weight \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 40, '4': 72, '5': 3, '6': 8, 'paths': 1}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 86s 18s/step - loss: 8.7035 - val_loss: 1.5012\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 81s 17s/step - loss: 1.4694 - val_loss: 0.8578\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 82s 17s/step - loss: 0.6244 - val_loss: 0.2065\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 86s 17s/step - loss: 0.1674 - val_loss: 0.2439\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 84s 17s/step - loss: 0.1836 - val_loss: 0.2107\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 87s 18s/step - loss: 0.1562 - val_loss: 0.1956\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 82s 17s/step - loss: 0.1483 - val_loss: 0.1829\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 83s 17s/step - loss: 0.1326 - val_loss: 0.1831\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 87s 17s/step - loss: 0.1373 - val_loss: 0.1843\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 83s 17s/step - loss: 0.1324 - val_loss: 0.1902\n"
     ]
    }
   ],
   "source": [
    "def Simple_CNN(numclasses, input_shape): #TODO use a more complex CNN\n",
    "        model = Sequential([\n",
    "            layers.Input(input_shape),\n",
    "            layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "            layers.MaxPooling2D(),\n",
    "            layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "            layers.MaxPooling2D(),\n",
    "            layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "            layers.MaxPooling2D(),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(numclasses)\n",
    "        ])\n",
    "        return model\n",
    "\n",
    "model_simpleCNN = Simple_CNN(10, input_shape=(100, 100, 18))\n",
    "callbacks_list = []\n",
    "\n",
    "model_simpleCNN.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "          optimizer=keras.optimizers.Adam()) #TODO add callbacks to save checkpoints and maybe lr reducer,etc \n",
    "\n",
    "epochs = 10\n",
    "history = model_simpleCNN.fit(gen.training_dataset, validation_data=gen.validation_dataset, epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100, 18)\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "obj = s3.Object('canopy-production-ml', \"chips/cloudfree-merge-polygons/split/test/100/100_1000_1000.tif\")\n",
    "obj_bytes = io.BytesIO(obj.get()['Body'].read())\n",
    "with rasterio.open(obj_bytes) as src:\n",
    "    img_test = np.transpose(src.read(), (1, 2, 0))\n",
    "print(img_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['Habitation', 'ISL', 'Industrial_agriculture', 'Mining',\n",
    "    'Rainforest', 'River', 'Roads', 'Savannah', 'Shifting_cultivation',\n",
    "    'Water'\n",
    "]\n",
    "# TODO Need to weight labels since they are pretty unbalanced (Rainforest is largely represented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 = boto3.resource('s3')\n",
    "# obj = s3.Object('canopy-production-ml', \"chips/cloudfree-merge-polygons/split/train/58/58_1300_1000.tif\")\n",
    "# obj_bytes = io.BytesIO(obj.get()['Body'].read())\n",
    "# with rasterio.open(obj_bytes) as src:\n",
    "#     img_test = np.transpose(src.read(), (1, 2, 0)) / 255\n",
    "# print(img_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This chip was predicted to belong to class Rainforest\n"
     ]
    }
   ],
   "source": [
    "predictions = model_simpleCNN.predict(np.array([img_test]))\n",
    "highest_score_predictions = np.argmax(predictions) # TODO: read more about multi classes PER IMAGE classification, what is the threshold?\n",
    "\n",
    "print(\"This chip was predicted to belong to class {}\".format(label_list[highest_score_predictions]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2020.7277        4.4248075 -1614.4033    -3687.283       157.33759\n",
      "  -1660.8308     -644.8952    -2039.6393    -1582.1956    -1557.5543   ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3, 7, 0, 5, 2, 8, 9, 6, 1, 4]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(predictions)\n",
    "predictions.argsort() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 35s 10s/step - loss: 0.1902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.19022126495838165"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_simpleCNN.evaluate(gen.validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Resnet50(numclasses, input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(keras.applications.ResNet50(include_top=False, pooling='avg', weights=None, input_shape=input_shape))\n",
    "    model.add(Flatten())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(2048, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(numclasses, activation='softmax'))\n",
    "    model.layers[0].trainable = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/applications/imagenet_utils.py:331: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 18 input channels.\n",
      "  warnings.warn('This model usually expects 1 or 3 input channels. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 100s 19s/step - loss: 0.7197 - val_loss: 0.7552\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 92s 18s/step - loss: 0.7108 - val_loss: 0.7552\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 89s 18s/step - loss: 0.6910 - val_loss: 0.7552\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 92s 18s/step - loss: 0.6992 - val_loss: 0.7552\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 88s 18s/step - loss: 0.6945 - val_loss: 0.7552\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 90s 17s/step - loss: 0.6954 - val_loss: 0.7552\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 91s 18s/step - loss: 0.6927 - val_loss: 0.7552\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 90s 17s/step - loss: 0.6903 - val_loss: 0.7552\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 91s 19s/step - loss: 0.6820 - val_loss: 0.7552\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 94s 18s/step - loss: 0.6781 - val_loss: 0.7552\n"
     ]
    }
   ],
   "source": [
    "model_resnet50 = Resnet50(10, input_shape=(100, 100, 18))\n",
    "callbacks_list = []\n",
    "\n",
    "model_resnet50.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "          optimizer=keras.optimizers.Adam()) #TODO add callbacks to save checkpoints and maybe lr reducer, earlystop,etc \n",
    "\n",
    "epochs = 10\n",
    "history = model_resnet50.fit(gen.training_dataset, validation_data=gen.validation_dataset, epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This chip was predicted to belong to class Industrial_agriculture\n"
     ]
    }
   ],
   "source": [
    "predictions = model_resnet50.predict(np.array([img_test]))\n",
    "highest_score_predictions = np.argmax(predictions) # TODO: read more about multi classes PER IMAGE classification, what is the threshold?\n",
    "\n",
    "print(\"This chip was predicted to belong to class {}\".format(label_list[highest_score_predictions]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 29s 9s/step - loss: 0.7552\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7551586031913757"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_resnet50.evaluate(gen.validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Industrial_agriculture\n",
      "Water\n",
      "Shifting_cultivation\n"
     ]
    }
   ],
   "source": [
    "# https://kgptalkie.com/multi-label-image-classification-on-movies-poster-using-cnn/\n",
    "top3 = np.argsort(predictions[0])[:-4:-1]\n",
    "for i in range(3):\n",
    "  print(label_list[top3[i]]) # We need to define a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_learningCurve(history, epoch):\n",
    "  # Plot training & validation accuracy values\n",
    "#   plt.plot(history.history['accuracy'])\n",
    "#   plt.plot(history.history['val_accuracy'])\n",
    "#   plt.title('Model accuracy')\n",
    "#   plt.ylabel('Accuracy')\n",
    "#   plt.xlabel('Epoch')\n",
    "#   plt.legend(['Train', 'Val'], loc='upper left')\n",
    "#   plt.show()\n",
    "\n",
    "  # Plot training & validation loss values\n",
    "  plt.plot(history.history['loss'])\n",
    "  plt.plot(history.history['val_loss'])\n",
    "  plt.title('Model loss')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Val'], loc='upper left')\n",
    "  plt.show()\n",
    "\n",
    "plot_learningCurve(history, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medium_CNN(numclasses, input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, (3,3), activation='relu', input_shape = input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(2,2))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(32, (3,3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(2,2))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(2,2))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(128, (3,3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(2,2))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "    model.add(Dense(numclasses, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 86s 17s/step - loss: 0.9270 - accuracy: 0.1209 - val_loss: 0.8956 - val_accuracy: 0.1200\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 91s 19s/step - loss: 0.9144 - accuracy: 0.1223 - val_loss: 0.9091 - val_accuracy: 0.1200\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 85s 17s/step - loss: 0.9159 - accuracy: 0.1198 - val_loss: 0.9205 - val_accuracy: 0.1200\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 86s 18s/step - loss: 0.9162 - accuracy: 0.1193 - val_loss: 0.9494 - val_accuracy: 0.1200\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 84s 17s/step - loss: 0.9036 - accuracy: 0.1200 - val_loss: 0.9662 - val_accuracy: 0.1200\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 85s 17s/step - loss: 0.9077 - accuracy: 0.1201 - val_loss: 0.9763 - val_accuracy: 0.1200\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 89s 17s/step - loss: 0.9053 - accuracy: 0.1200 - val_loss: 0.9755 - val_accuracy: 0.1200\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 84s 17s/step - loss: 0.8920 - accuracy: 0.1197 - val_loss: 0.9652 - val_accuracy: 0.1200\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 85s 17s/step - loss: 0.8925 - accuracy: 0.1195 - val_loss: 0.9509 - val_accuracy: 0.1200\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 84s 17s/step - loss: 0.8863 - accuracy: 0.1211 - val_loss: 0.9204 - val_accuracy: 0.1200\n"
     ]
    }
   ],
   "source": [
    "model_medium_CNN = medium_CNN(10, input_shape=(100, 100, 18))\n",
    "callbacks_list = []\n",
    "\n",
    "model_medium_CNN.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                          optimizer=keras.optimizers.Adam(),\n",
    "                          metrics=[tf.metrics.BinaryAccuracy(threshold=0.0, name='accuracy')]) #TODO add callbacks to save checkpoints and maybe lr reducer, earlystop,etc \n",
    "\n",
    "epochs = 10\n",
    "history = model_medium_CNN.fit(gen.training_dataset, validation_data=gen.validation_dataset, epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This chip was predicted to belong to top 3 classes:\n",
      "Water\n",
      "Shifting_cultivation\n",
      "Savannah\n"
     ]
    }
   ],
   "source": [
    "predictions = model_medium_CNN.predict(np.array([img_test]))\n",
    "highest_score_predictions = np.argmax(predictions) # TODO: read more about multi classes PER IMAGE classification, what is the threshold?\n",
    "\n",
    "print(\"This chip was predicted to belong to top 3 classes:\")\n",
    "\n",
    "top3 = np.argsort(predictions[0])[:-4:-1]\n",
    "for i in range(3):\n",
    "  print(label_list[top3[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 32s 11s/step - loss: 0.9204 - accuracy: 0.1200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9204237461090088, 0.12000000476837158]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_medium_CNN.evaluate(gen.validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_learningCurve(history, epoch):\n",
    "  # Plot training & validation accuracy values\n",
    "  plt.plot(history.history['accuracy'])\n",
    "  plt.plot(history.history['val_accuracy'])\n",
    "  plt.title('Model accuracy')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Val'], loc='upper left')\n",
    "  plt.show()\n",
    "\n",
    "  # Plot training & validation loss values\n",
    "  plt.plot(history.history['loss'])\n",
    "  plt.plot(history.history['val_loss'])\n",
    "  plt.title('Model loss')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Val'], loc='upper left')\n",
    "  plt.show()\n",
    "\n",
    "plot_learningCurve(history, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST with bigearthnet-resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow_hub/keras_layer.py:228 call  *\n        result = f()\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1669 __call__  **\n        return self._call_impl(args, kwargs)\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py:246 _call_impl\n        return super(WrappedFunction, self)._call_impl(\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1687 _call_impl\n        return self._call_with_flat_signature(args, kwargs, cancellation_manager)\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1736 _call_with_flat_signature\n        return self._call_flat(args, self.captured_inputs, cancellation_manager)\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1900 _call_flat\n        raise ValueError(\n\n    ValueError: The argument 'images' (value Tensor(\"Placeholder:0\", shape=(None, 100, 100, 4), dtype=float32)) is not compatible with the shape this function was traced with. Expected shape (None, None, None, 3), but got shape (None, 100, 100, 4).\n    \n    If you called get_concrete_function, you may need to pass a tf.TensorSpec(..., shape=...) with a less specific shape, having None on axes which can vary.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-9359f750e35b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://tfhub.dev/google/remote_sensing/bigearthnet-resnet50/1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m model_bigearthnet = tf.keras.Sequential([\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInputLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMAGE_SIZE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# reshape?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layers, name)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    221\u001b[0m       \u001b[0;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m       \u001b[0;31m# refresh its output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m       \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSINGLE_LAYER_OUTPUT_ERROR_MSG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;31m# >> model = tf.keras.Model(inputs, outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0m\u001b[1;32m    952\u001b[0m                                                 input_list)\n\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1088\u001b[0m           layer=self, inputs=inputs, build_graph=True, training=training_value):\n\u001b[1;32m   1089\u001b[0m         \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m         outputs = self._keras_tensor_symbolic_call(\n\u001b[0m\u001b[1;32m   1091\u001b[0m             inputs, input_masks, args, kwargs)\n\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    861\u001b[0m           \u001b[0;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow_hub/keras_layer.py:228 call  *\n        result = f()\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1669 __call__  **\n        return self._call_impl(args, kwargs)\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py:246 _call_impl\n        return super(WrappedFunction, self)._call_impl(\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1687 _call_impl\n        return self._call_with_flat_signature(args, kwargs, cancellation_manager)\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1736 _call_with_flat_signature\n        return self._call_flat(args, self.captured_inputs, cancellation_manager)\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1900 _call_flat\n        raise ValueError(\n\n    ValueError: The argument 'images' (value Tensor(\"Placeholder:0\", shape=(None, 100, 100, 4), dtype=float32)) is not compatible with the shape this function was traced with. Expected shape (None, None, None, 3), but got shape (None, 100, 100, 4).\n    \n    If you called get_concrete_function, you may need to pass a tf.TensorSpec(..., shape=...) with a less specific shape, having None on axes which can vary.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "IMAGE_SIZE = (100,100)\n",
    "num_classes = 10\n",
    "model_handle = \"https://tfhub.dev/google/remote_sensing/bigearthnet-resnet50/1\"\n",
    "model_bigearthnet = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
    "    # reshape? \n",
    "    hub.KerasLayer(model_handle, trainable=False, input_shape=IMAGE_SIZE + (3,)),\n",
    "#     (model.layers[-1].output)\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(num_classes,\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
    "])\n",
    "model_bigearthnet.build((None,)+IMAGE_SIZE+(4,))\n",
    "model_bigearthnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_bigearthnet.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#           optimizer=keras.optimizers.Adam()) #TODO add callbacks to save checkpoints and maybe lr reducer, earlystop,etc \n",
    "\n",
    "# epochs = 10\n",
    "# history = model_bigearthnet.fit(gen.training_dataset, validation_data=gen.validation_dataset, epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = model_resnet50.predict(np.array([img_test]))\n",
    "# highest_score_predictions = np.argmax(predictions) # TODO: read more about multi classes PER IMAGE classification, what is the threshold?\n",
    "\n",
    "# print(\"This chip was predicted to belong to class {}\".format(label_list[highest_score_predictions]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preproduction Candidate: ResNet50 pretrained on ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-8a299c28d6d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m gen = DataLoader(label_file_path_train=\"labels_test_v1.csv\", #or labels.csv\n\u001b[0m\u001b[0;32m      2\u001b[0m                         \u001b[0mlabel_file_path_val\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"val_labels.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                         \u001b[0mbucket_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'canopy-production-ml'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                         \u001b[0mdata_extension_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'.tif'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                         \u001b[0mtraining_data_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m18\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "gen = DataLoader(label_file_path_train=\"labels_test_v1.csv\", #or labels.csv\n",
    "                        label_file_path_val=\"val_labels.csv\",\n",
    "                        bucket_name='canopy-production-ml',\n",
    "                        data_extension_type='.tif',\n",
    "                        training_data_shape=(100, 100, 18),\n",
    "                        shuffle_and_repeat=False,\n",
    "                        enable_just_shuffle=False,\n",
    "                        enable_just_repeat=False,\n",
    "                        training_data_shuffle_buffer_size=10,\n",
    "                        data_repeat_count=None,\n",
    "                        training_data_batch_size=20,\n",
    "                        normalization_value=255.0,  #normalization TODO double check other channels than RGB \n",
    "                        training_data_type=tf.float32,\n",
    "                        label_data_type=tf.uint8,\n",
    "                        enable_data_prefetch=False,\n",
    "                        data_prefetch_size=tf.data.experimental.AUTOTUNE,\n",
    "                        num_parallel_calls=int(2))\n",
    "# TODO add data augmentation in DataLoader \n",
    "\n",
    "no_of_val_imgs = len(gen.validation_filenames)\n",
    "no_of_train_imgs = len(gen.training_filenames)\n",
    "print(\"Validation on {} images \".format(str(no_of_val_imgs)))\n",
    "print(\"Training on {} images \".format(str(no_of_train_imgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(numclasses,input_shape):\n",
    "    # parameters for CNN\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "\n",
    "    # introduce a additional layer to get from bands to 3 input channels\n",
    "    input_tensor = Conv2D(3, (1, 1))(input_tensor)\n",
    "\n",
    "    base_model_resnet50 = keras.applications.ResNet50(include_top=False,\n",
    "                              weights='imagenet',\n",
    "                              input_shape=(100, 100, 3))\n",
    "    base_model = keras.applications.ResNet50(include_top=False,\n",
    "                     weights=None,\n",
    "                     input_tensor=input_tensor)\n",
    "\n",
    "    for i, layer in enumerate(base_model_resnet50.layers):\n",
    "        # we must skip input layer, which has no weights\n",
    "        if i == 0:\n",
    "            continue\n",
    "        base_model.layers[i+1].set_weights(layer.get_weights())\n",
    "\n",
    "    # add a global spatial average pooling layer\n",
    "    top_model = base_model.output\n",
    "    top_model = GlobalAveragePooling2D()(top_model)\n",
    "\n",
    "    # let's add a fully-connected layer\n",
    "    top_model = Dense(2048, activation='relu')(top_model)\n",
    "    top_model = Dense(2048, activation='relu')(top_model)\n",
    "    # and a logistic layer\n",
    "    predictions = Dense(numclasses, activation='softmax')(top_model)\n",
    "\n",
    "    # this is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 100, 100, 18 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 100, 100, 3)  57          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 106, 106, 3)  0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 50, 50, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 50, 50, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 50, 50, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 52, 52, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 25, 25, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 25, 25, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 25, 25, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 25, 25, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 25, 25, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 25, 25, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 25, 25, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 25, 25, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 25, 25, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 25, 25, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 25, 25, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 25, 25, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 25, 25, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 25, 25, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 25, 25, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 25, 25, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 25, 25, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 25, 25, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 25, 25, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 25, 25, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 25, 25, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 25, 25, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 25, 25, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 25, 25, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 25, 25, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 25, 25, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 25, 25, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 13, 13, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 13, 13, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 13, 13, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 13, 13, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 13, 13, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 13, 13, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 13, 13, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 13, 13, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 13, 13, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 13, 13, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 13, 13, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 13, 13, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 13, 13, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 13, 13, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 13, 13, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 13, 13, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 13, 13, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 13, 13, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 13, 13, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 13, 13, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 13, 13, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 13, 13, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 13, 13, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 13, 13, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 13, 13, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 13, 13, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 13, 13, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 13, 13, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 13, 13, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 7, 7, 256)    131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 7, 7, 256)    0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 7, 7, 256)    0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 7, 7, 1024)   525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 7, 7, 1024)   0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 7, 7, 1024)   0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 7, 7, 256)    0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 7, 7, 256)    0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 7, 7, 1024)   0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 7, 7, 1024)   0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 7, 7, 256)    0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 7, 7, 256)    0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 7, 7, 1024)   0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 7, 7, 1024)   0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 7, 7, 256)    0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 7, 7, 256)    0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 7, 7, 1024)   0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 7, 7, 1024)   0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 7, 7, 256)    0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 7, 7, 256)    0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 7, 7, 1024)   0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 7, 7, 1024)   0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 7, 7, 256)    0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 7, 7, 256)    0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 7, 7, 1024)   0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 7, 7, 1024)   0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 4, 4, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 4, 4, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 4, 4, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 4, 4, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 4, 4, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 4, 4, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 4, 4, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 4, 4, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 4, 4, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 4, 4, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 4, 4, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 4, 4, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 4, 4, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 4, 4, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 4, 4, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 4, 4, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2048)         4196352     global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2048)         4196352     dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           20490       dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 32,000,963\n",
      "Trainable params: 31,947,843\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "random_id = 5555 #TODO\n",
    "checkpoint_file = 'checkpoint_{}.h5'.format(random_id)\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "  filepath= checkpoint_file,\n",
    "  format='h5',\n",
    "  verbose=1,\n",
    "  save_weights_only=True,\n",
    "  monitor='val_loss',\n",
    "  mode='min',\n",
    "  save_best_only=True)\n",
    "\n",
    "reducelronplateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "  monitor='val_loss', factor=0.1, patience=10, verbose=1,\n",
    "  mode='min', min_lr=1e-10)\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',mode='min', patience=20, verbose=1)\n",
    "\n",
    "callbacks_list = [model_checkpoint_callback, reducelronplateau, early_stop]\n",
    "\n",
    "model = define_model(10, (100,100,18))\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                          optimizer=keras.optimizers.Adam(),\n",
    "                          metrics=[tf.metrics.BinaryAccuracy(name='accuracy')]) #TODO add callbacks to save checkpoints and maybe lr reducer, earlystop,etc \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "      1/Unknown - 0s 0s/step - loss: 0.7377 - accuracy: 0.8900"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-2fbc1874cc41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m history = model.fit(gen.training_dataset, validation_data=gen.validation_dataset, \n\u001b[0;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                     callbacks=callbacks_list)\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\envs\\canopy_test_models\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\canopy_test_models\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\canopy_test_models\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\canopy_test_models\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\canopy_test_models\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\canopy_test_models\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\canopy_test_models\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\canopy_test_models\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\canopy_test_models\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "history = model.fit(gen.training_dataset, validation_data=gen.validation_dataset, \n",
    "                    epochs=epochs, \n",
    "                    callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 9s 3s/step - loss: 0.6661 - accuracy: 0.9473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6660677194595337, 0.9472727179527283]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(gen.validation_dataset) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on BatchDataset in module tensorflow.python.data.ops.dataset_ops object:\n",
      "\n",
      "class BatchDataset(UnaryDataset)\n",
      " |  BatchDataset(input_dataset, batch_size, drop_remainder)\n",
      " |  \n",
      " |  A `Dataset` that batches contiguous elements from its input.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BatchDataset\n",
      " |      UnaryDataset\n",
      " |      DatasetV2\n",
      " |      collections.abc.Iterable\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      tensorflow.python.framework.composite_tensor.CompositeTensor\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, input_dataset, batch_size, drop_remainder)\n",
      " |      See `Dataset.batch()` for details.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  element_spec\n",
      " |      The type specification of an element of this dataset.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
      " |      >>> dataset.element_spec\n",
      " |      TensorSpec(shape=(), dtype=tf.int32, name=None)\n",
      " |      \n",
      " |      Returns:\n",
      " |        A nested structure of `tf.TypeSpec` objects matching the structure of an\n",
      " |        element of this dataset and specifying the type of individual components.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from DatasetV2:\n",
      " |  \n",
      " |  __bool__(self)\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Creates an iterator for elements of this dataset.\n",
      " |      \n",
      " |      The returned iterator implements the Python Iterator protocol.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An `tf.data.Iterator` for the elements of this dataset.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If not inside of tf.function and not executing eagerly.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Returns the length of the dataset if it is known and finite.\n",
      " |      \n",
      " |      This method requires that you are running in eager mode, and that the\n",
      " |      length of the dataset is known and non-infinite. When the length may be\n",
      " |      unknown or infinite, or if you are running in graph mode, use\n",
      " |      `tf.data.Dataset.cardinality` instead.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An integer representing the length of the dataset.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If the dataset length is unknown or infinite, or if eager\n",
      " |          execution is not enabled.\n",
      " |  \n",
      " |  __nonzero__ = __bool__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  apply(self, transformation_func)\n",
      " |      Applies a transformation function to this dataset.\n",
      " |      \n",
      " |      `apply` enables chaining of custom `Dataset` transformations, which are\n",
      " |      represented as functions that take one `Dataset` argument and return a\n",
      " |      transformed `Dataset`.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(100)\n",
      " |      >>> def dataset_fn(ds):\n",
      " |      ...   return ds.filter(lambda x: x < 5)\n",
      " |      >>> dataset = dataset.apply(dataset_fn)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [0, 1, 2, 3, 4]\n",
      " |      \n",
      " |      Args:\n",
      " |        transformation_func: A function that takes one `Dataset` argument and\n",
      " |          returns a `Dataset`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: The `Dataset` returned by applying `transformation_func` to this\n",
      " |            dataset.\n",
      " |  \n",
      " |  as_numpy_iterator(self)\n",
      " |      Returns an iterator which converts all elements of the dataset to numpy.\n",
      " |      \n",
      " |      Use `as_numpy_iterator` to inspect the content of your dataset. To see\n",
      " |      element shapes and types, print dataset elements directly instead of using\n",
      " |      `as_numpy_iterator`.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
      " |      >>> for element in dataset:\n",
      " |      ...   print(element)\n",
      " |      tf.Tensor(1, shape=(), dtype=int32)\n",
      " |      tf.Tensor(2, shape=(), dtype=int32)\n",
      " |      tf.Tensor(3, shape=(), dtype=int32)\n",
      " |      \n",
      " |      This method requires that you are running in eager mode and the dataset's\n",
      " |      element_spec contains only `TensorSpec` components.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
      " |      >>> for element in dataset.as_numpy_iterator():\n",
      " |      ...   print(element)\n",
      " |      1\n",
      " |      2\n",
      " |      3\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
      " |      >>> print(list(dataset.as_numpy_iterator()))\n",
      " |      [1, 2, 3]\n",
      " |      \n",
      " |      `as_numpy_iterator()` will preserve the nested structure of dataset\n",
      " |      elements.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices({'a': ([1, 2], [3, 4]),\n",
      " |      ...                                               'b': [5, 6]})\n",
      " |      >>> list(dataset.as_numpy_iterator()) == [{'a': (1, 3), 'b': 5},\n",
      " |      ...                                       {'a': (2, 4), 'b': 6}]\n",
      " |      True\n",
      " |      \n",
      " |      Returns:\n",
      " |        An iterable over the elements of the dataset, with their tensors converted\n",
      " |        to numpy arrays.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if an element contains a non-`Tensor` value.\n",
      " |        RuntimeError: if eager execution is not enabled.\n",
      " |  \n",
      " |  batch(self, batch_size, drop_remainder=False)\n",
      " |      Combines consecutive elements of this dataset into batches.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(8)\n",
      " |      >>> dataset = dataset.batch(3)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [array([0, 1, 2]), array([3, 4, 5]), array([6, 7])]\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(8)\n",
      " |      >>> dataset = dataset.batch(3, drop_remainder=True)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [array([0, 1, 2]), array([3, 4, 5])]\n",
      " |      \n",
      " |      The components of the resulting element will have an additional outer\n",
      " |      dimension, which will be `batch_size` (or `N % batch_size` for the last\n",
      " |      element if `batch_size` does not divide the number of input elements `N`\n",
      " |      evenly and `drop_remainder` is `False`). If your program depends on the\n",
      " |      batches having the same outer dimension, you should set the `drop_remainder`\n",
      " |      argument to `True` to prevent the smaller batch from being produced.\n",
      " |      \n",
      " |      Args:\n",
      " |        batch_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          consecutive elements of this dataset to combine in a single batch.\n",
      " |        drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\n",
      " |          whether the last batch should be dropped in the case it has fewer than\n",
      " |          `batch_size` elements; the default behavior is not to drop the smaller\n",
      " |          batch.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  cache(self, filename='')\n",
      " |      Caches the elements in this dataset.\n",
      " |      \n",
      " |      The first time the dataset is iterated over, its elements will be cached\n",
      " |      either in the specified file or in memory. Subsequent iterations will\n",
      " |      use the cached data.\n",
      " |      \n",
      " |      Note: For the cache to be finalized, the input dataset must be iterated\n",
      " |      through in its entirety. Otherwise, subsequent iterations will not use\n",
      " |      cached data.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(5)\n",
      " |      >>> dataset = dataset.map(lambda x: x**2)\n",
      " |      >>> dataset = dataset.cache()\n",
      " |      >>> # The first time reading through the data will generate the data using\n",
      " |      >>> # `range` and `map`.\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [0, 1, 4, 9, 16]\n",
      " |      >>> # Subsequent iterations read from the cache.\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [0, 1, 4, 9, 16]\n",
      " |      \n",
      " |      When caching to a file, the cached data will persist across runs. Even the\n",
      " |      first iteration through the data will read from the cache file. Changing\n",
      " |      the input pipeline before the call to `.cache()` will have no effect until\n",
      " |      the cache file is removed or the filename is changed.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(5)\n",
      " |      >>> dataset = dataset.cache(\"/path/to/file\")  # doctest: +SKIP\n",
      " |      >>> list(dataset.as_numpy_iterator())  # doctest: +SKIP\n",
      " |      [0, 1, 2, 3, 4]\n",
      " |      >>> dataset = tf.data.Dataset.range(10)\n",
      " |      >>> dataset = dataset.cache(\"/path/to/file\")  # Same file! # doctest: +SKIP\n",
      " |      >>> list(dataset.as_numpy_iterator())  # doctest: +SKIP\n",
      " |      [0, 1, 2, 3, 4]\n",
      " |      \n",
      " |      Note: `cache` will produce exactly the same elements during each iteration\n",
      " |      through the dataset. If you wish to randomize the iteration order, make sure\n",
      " |      to call `shuffle` *after* calling `cache`.\n",
      " |      \n",
      " |      Args:\n",
      " |        filename: A `tf.string` scalar `tf.Tensor`, representing the name of a\n",
      " |          directory on the filesystem to use for caching elements in this Dataset.\n",
      " |          If a filename is not provided, the dataset will be cached in memory.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  cardinality(self)\n",
      " |      Returns the cardinality of the dataset, if known.\n",
      " |      \n",
      " |      `cardinality` may return `tf.data.INFINITE_CARDINALITY` if the dataset\n",
      " |      contains an infinite number of elements or `tf.data.UNKNOWN_CARDINALITY` if\n",
      " |      the analysis fails to determine the number of elements in the dataset\n",
      " |      (e.g. when the dataset source is a file).\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(42)\n",
      " |      >>> print(dataset.cardinality().numpy())\n",
      " |      42\n",
      " |      >>> dataset = dataset.repeat()\n",
      " |      >>> cardinality = dataset.cardinality()\n",
      " |      >>> print((cardinality == tf.data.INFINITE_CARDINALITY).numpy())\n",
      " |      True\n",
      " |      >>> dataset = dataset.filter(lambda x: True)\n",
      " |      >>> cardinality = dataset.cardinality()\n",
      " |      >>> print((cardinality == tf.data.UNKNOWN_CARDINALITY).numpy())\n",
      " |      True\n",
      " |      \n",
      " |      Returns:\n",
      " |        A scalar `tf.int64` `Tensor` representing the cardinality of the dataset.\n",
      " |        If the cardinality is infinite or unknown, `cardinality` returns the\n",
      " |        named constants `tf.data.INFINITE_CARDINALITY` and\n",
      " |        `tf.data.UNKNOWN_CARDINALITY` respectively.\n",
      " |  \n",
      " |  concatenate(self, dataset)\n",
      " |      Creates a `Dataset` by concatenating the given dataset with this dataset.\n",
      " |      \n",
      " |      >>> a = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\n",
      " |      >>> b = tf.data.Dataset.range(4, 8)  # ==> [ 4, 5, 6, 7 ]\n",
      " |      >>> ds = a.concatenate(b)\n",
      " |      >>> list(ds.as_numpy_iterator())\n",
      " |      [1, 2, 3, 4, 5, 6, 7]\n",
      " |      >>> # The input dataset and dataset to be concatenated should have the same\n",
      " |      >>> # nested structures and output types.\n",
      " |      >>> c = tf.data.Dataset.zip((a, b))\n",
      " |      >>> a.concatenate(c)\n",
      " |      Traceback (most recent call last):\n",
      " |      TypeError: Two datasets to concatenate have different types\n",
      " |      <dtype: 'int64'> and (tf.int64, tf.int64)\n",
      " |      >>> d = tf.data.Dataset.from_tensor_slices([\"a\", \"b\", \"c\"])\n",
      " |      >>> a.concatenate(d)\n",
      " |      Traceback (most recent call last):\n",
      " |      TypeError: Two datasets to concatenate have different types\n",
      " |      <dtype: 'int64'> and <dtype: 'string'>\n",
      " |      \n",
      " |      Args:\n",
      " |        dataset: `Dataset` to be concatenated.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  enumerate(self, start=0)\n",
      " |      Enumerates the elements of this dataset.\n",
      " |      \n",
      " |      It is similar to python's `enumerate`.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
      " |      >>> dataset = dataset.enumerate(start=5)\n",
      " |      >>> for element in dataset.as_numpy_iterator():\n",
      " |      ...   print(element)\n",
      " |      (5, 1)\n",
      " |      (6, 2)\n",
      " |      (7, 3)\n",
      " |      \n",
      " |      >>> # The nested structure of the input dataset determines the structure of\n",
      " |      >>> # elements in the resulting dataset.\n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices([(7, 8), (9, 10)])\n",
      " |      >>> dataset = dataset.enumerate()\n",
      " |      >>> for element in dataset.as_numpy_iterator():\n",
      " |      ...   print(element)\n",
      " |      (0, array([7, 8], dtype=int32))\n",
      " |      (1, array([ 9, 10], dtype=int32))\n",
      " |      \n",
      " |      Args:\n",
      " |        start: A `tf.int64` scalar `tf.Tensor`, representing the start value for\n",
      " |          enumeration.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  filter(self, predicate)\n",
      " |      Filters this dataset according to `predicate`.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
      " |      >>> dataset = dataset.filter(lambda x: x < 3)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [1, 2]\n",
      " |      >>> # `tf.math.equal(x, y)` is required for equality comparison\n",
      " |      >>> def filter_fn(x):\n",
      " |      ...   return tf.math.equal(x, 1)\n",
      " |      >>> dataset = dataset.filter(filter_fn)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [1]\n",
      " |      \n",
      " |      Args:\n",
      " |        predicate: A function mapping a dataset element to a boolean.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: The `Dataset` containing the elements of this dataset for which\n",
      " |            `predicate` is `True`.\n",
      " |  \n",
      " |  flat_map(self, map_func)\n",
      " |      Maps `map_func` across this dataset and flattens the result.\n",
      " |      \n",
      " |      Use `flat_map` if you want to make sure that the order of your dataset\n",
      " |      stays the same. For example, to flatten a dataset of batches into a\n",
      " |      dataset of their elements:\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices(\n",
      " |      ...                [[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
      " |      >>> dataset = dataset.flat_map(lambda x: Dataset.from_tensor_slices(x))\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      " |      \n",
      " |      `tf.data.Dataset.interleave()` is a generalization of `flat_map`, since\n",
      " |      `flat_map` produces the same output as\n",
      " |      `tf.data.Dataset.interleave(cycle_length=1)`\n",
      " |      \n",
      " |      Args:\n",
      " |        map_func: A function mapping a dataset element to a dataset.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  interleave(self, map_func, cycle_length=None, block_length=None, num_parallel_calls=None, deterministic=None)\n",
      " |      Maps `map_func` across this dataset, and interleaves the results.\n",
      " |      \n",
      " |      For example, you can use `Dataset.interleave()` to process many input files\n",
      " |      concurrently:\n",
      " |      \n",
      " |      >>> # Preprocess 4 files concurrently, and interleave blocks of 16 records\n",
      " |      >>> # from each file.\n",
      " |      >>> filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n",
      " |      ...              \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
      " |      >>> def parse_fn(filename):\n",
      " |      ...   return tf.data.Dataset.range(10)\n",
      " |      >>> dataset = dataset.interleave(lambda x:\n",
      " |      ...     tf.data.TextLineDataset(x).map(parse_fn, num_parallel_calls=1),\n",
      " |      ...     cycle_length=4, block_length=16)\n",
      " |      \n",
      " |      The `cycle_length` and `block_length` arguments control the order in which\n",
      " |      elements are produced. `cycle_length` controls the number of input elements\n",
      " |      that are processed concurrently. If you set `cycle_length` to 1, this\n",
      " |      transformation will handle one input element at a time, and will produce\n",
      " |      identical results to `tf.data.Dataset.flat_map`. In general,\n",
      " |      this transformation will apply `map_func` to `cycle_length` input elements,\n",
      " |      open iterators on the returned `Dataset` objects, and cycle through them\n",
      " |      producing `block_length` consecutive elements from each iterator, and\n",
      " |      consuming the next input element each time it reaches the end of an\n",
      " |      iterator.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      >>> dataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
      " |      >>> # NOTE: New lines indicate \"block\" boundaries.\n",
      " |      >>> dataset = dataset.interleave(\n",
      " |      ...     lambda x: Dataset.from_tensors(x).repeat(6),\n",
      " |      ...     cycle_length=2, block_length=4)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [1, 1, 1, 1,\n",
      " |       2, 2, 2, 2,\n",
      " |       1, 1,\n",
      " |       2, 2,\n",
      " |       3, 3, 3, 3,\n",
      " |       4, 4, 4, 4,\n",
      " |       3, 3,\n",
      " |       4, 4,\n",
      " |       5, 5, 5, 5,\n",
      " |       5, 5]\n",
      " |      \n",
      " |      Note: The order of elements yielded by this transformation is\n",
      " |      deterministic, as long as `map_func` is a pure function and\n",
      " |      `deterministic=True`. If `map_func` contains any stateful operations, the\n",
      " |      order in which that state is accessed is undefined.\n",
      " |      \n",
      " |      Performance can often be improved by setting `num_parallel_calls` so that\n",
      " |      `interleave` will use multiple threads to fetch elements. If determinism\n",
      " |      isn't required, it can also improve performance to set\n",
      " |      `deterministic=False`.\n",
      " |      \n",
      " |      >>> filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n",
      " |      ...              \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
      " |      >>> dataset = dataset.interleave(lambda x: tf.data.TFRecordDataset(x),\n",
      " |      ...     cycle_length=4, num_parallel_calls=tf.data.AUTOTUNE,\n",
      " |      ...     deterministic=False)\n",
      " |      \n",
      " |      Args:\n",
      " |        map_func: A function mapping a dataset element to a dataset.\n",
      " |        cycle_length: (Optional.) The number of input elements that will be\n",
      " |          processed concurrently. If not set, the tf.data runtime decides what it\n",
      " |          should be based on available CPU. If `num_parallel_calls` is set to\n",
      " |          `tf.data.AUTOTUNE`, the `cycle_length` argument identifies\n",
      " |          the maximum degree of parallelism.\n",
      " |        block_length: (Optional.) The number of consecutive elements to produce\n",
      " |          from each input element before cycling to another input element. If not\n",
      " |          set, defaults to 1.\n",
      " |        num_parallel_calls: (Optional.) If specified, the implementation creates a\n",
      " |          threadpool, which is used to fetch inputs from cycle elements\n",
      " |          asynchronously and in parallel. The default behavior is to fetch inputs\n",
      " |          from cycle elements synchronously with no parallelism. If the value\n",
      " |          `tf.data.AUTOTUNE` is used, then the number of parallel\n",
      " |          calls is set dynamically based on available CPU.\n",
      " |        deterministic: (Optional.) A boolean controlling whether determinism\n",
      " |          should be traded for performance by allowing elements to be produced out\n",
      " |          of order.  If `deterministic` is `None`, the\n",
      " |          `tf.data.Options.experimental_deterministic` dataset option (`True` by\n",
      " |          default) is used to decide whether to produce elements\n",
      " |          deterministically.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  map(self, map_func, num_parallel_calls=None, deterministic=None)\n",
      " |      Maps `map_func` across the elements of this dataset.\n",
      " |      \n",
      " |      This transformation applies `map_func` to each element of this dataset, and\n",
      " |      returns a new dataset containing the transformed elements, in the same\n",
      " |      order as they appeared in the input. `map_func` can be used to change both\n",
      " |      the values and the structure of a dataset's elements. For example, adding 1\n",
      " |      to each element, or projecting a subset of element components.\n",
      " |      \n",
      " |      >>> dataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
      " |      >>> dataset = dataset.map(lambda x: x + 1)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [2, 3, 4, 5, 6]\n",
      " |      \n",
      " |      The input signature of `map_func` is determined by the structure of each\n",
      " |      element in this dataset.\n",
      " |      \n",
      " |      >>> dataset = Dataset.range(5)\n",
      " |      >>> # `map_func` takes a single argument of type `tf.Tensor` with the same\n",
      " |      >>> # shape and dtype.\n",
      " |      >>> result = dataset.map(lambda x: x + 1)\n",
      " |      \n",
      " |      >>> # Each element is a tuple containing two `tf.Tensor` objects.\n",
      " |      >>> elements = [(1, \"foo\"), (2, \"bar\"), (3, \"baz\")]\n",
      " |      >>> dataset = tf.data.Dataset.from_generator(\n",
      " |      ...     lambda: elements, (tf.int32, tf.string))\n",
      " |      >>> # `map_func` takes two arguments of type `tf.Tensor`. This function\n",
      " |      >>> # projects out just the first component.\n",
      " |      >>> result = dataset.map(lambda x_int, y_str: x_int)\n",
      " |      >>> list(result.as_numpy_iterator())\n",
      " |      [1, 2, 3]\n",
      " |      \n",
      " |      >>> # Each element is a dictionary mapping strings to `tf.Tensor` objects.\n",
      " |      >>> elements =  ([{\"a\": 1, \"b\": \"foo\"},\n",
      " |      ...               {\"a\": 2, \"b\": \"bar\"},\n",
      " |      ...               {\"a\": 3, \"b\": \"baz\"}])\n",
      " |      >>> dataset = tf.data.Dataset.from_generator(\n",
      " |      ...     lambda: elements, {\"a\": tf.int32, \"b\": tf.string})\n",
      " |      >>> # `map_func` takes a single argument of type `dict` with the same keys\n",
      " |      >>> # as the elements.\n",
      " |      >>> result = dataset.map(lambda d: str(d[\"a\"]) + d[\"b\"])\n",
      " |      \n",
      " |      The value or values returned by `map_func` determine the structure of each\n",
      " |      element in the returned dataset.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(3)\n",
      " |      >>> # `map_func` returns two `tf.Tensor` objects.\n",
      " |      >>> def g(x):\n",
      " |      ...   return tf.constant(37.0), tf.constant([\"Foo\", \"Bar\", \"Baz\"])\n",
      " |      >>> result = dataset.map(g)\n",
      " |      >>> result.element_spec\n",
      " |      (TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(3,), dtype=tf.string, name=None))\n",
      " |      >>> # Python primitives, lists, and NumPy arrays are implicitly converted to\n",
      " |      >>> # `tf.Tensor`.\n",
      " |      >>> def h(x):\n",
      " |      ...   return 37.0, [\"Foo\", \"Bar\"], np.array([1.0, 2.0], dtype=np.float64)\n",
      " |      >>> result = dataset.map(h)\n",
      " |      >>> result.element_spec\n",
      " |      (TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(2,), dtype=tf.string, name=None), TensorSpec(shape=(2,), dtype=tf.float64, name=None))\n",
      " |      >>> # `map_func` can return nested structures.\n",
      " |      >>> def i(x):\n",
      " |      ...   return (37.0, [42, 16]), \"foo\"\n",
      " |      >>> result = dataset.map(i)\n",
      " |      >>> result.element_spec\n",
      " |      ((TensorSpec(shape=(), dtype=tf.float32, name=None),\n",
      " |        TensorSpec(shape=(2,), dtype=tf.int32, name=None)),\n",
      " |       TensorSpec(shape=(), dtype=tf.string, name=None))\n",
      " |      \n",
      " |      `map_func` can accept as arguments and return any type of dataset element.\n",
      " |      \n",
      " |      Note that irrespective of the context in which `map_func` is defined (eager\n",
      " |      vs. graph), tf.data traces the function and executes it as a graph. To use\n",
      " |      Python code inside of the function you have a few options:\n",
      " |      \n",
      " |      1) Rely on AutoGraph to convert Python code into an equivalent graph\n",
      " |      computation. The downside of this approach is that AutoGraph can convert\n",
      " |      some but not all Python code.\n",
      " |      \n",
      " |      2) Use `tf.py_function`, which allows you to write arbitrary Python code but\n",
      " |      will generally result in worse performance than 1). For example:\n",
      " |      \n",
      " |      >>> d = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\n",
      " |      >>> # transform a string tensor to upper case string using a Python function\n",
      " |      >>> def upper_case_fn(t: tf.Tensor):\n",
      " |      ...   return t.numpy().decode('utf-8').upper()\n",
      " |      >>> d = d.map(lambda x: tf.py_function(func=upper_case_fn,\n",
      " |      ...           inp=[x], Tout=tf.string))\n",
      " |      >>> list(d.as_numpy_iterator())\n",
      " |      [b'HELLO', b'WORLD']\n",
      " |      \n",
      " |      3) Use `tf.numpy_function`, which also allows you to write arbitrary\n",
      " |      Python code. Note that `tf.py_function` accepts `tf.Tensor` whereas\n",
      " |      `tf.numpy_function` accepts numpy arrays and returns only numpy arrays.\n",
      " |      For example:\n",
      " |      \n",
      " |      >>> d = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\n",
      " |      >>> def upper_case_fn(t: np.ndarray):\n",
      " |      ...   return t.decode('utf-8').upper()\n",
      " |      >>> d = d.map(lambda x: tf.numpy_function(func=upper_case_fn,\n",
      " |      ...           inp=[x], Tout=tf.string))\n",
      " |      >>> list(d.as_numpy_iterator())\n",
      " |      [b'HELLO', b'WORLD']\n",
      " |      \n",
      " |      Note that the use of `tf.numpy_function` and `tf.py_function`\n",
      " |      in general precludes the possibility of executing user-defined\n",
      " |      transformations in parallel (because of Python GIL).\n",
      " |      \n",
      " |      Performance can often be improved by setting `num_parallel_calls` so that\n",
      " |      `map` will use multiple threads to process elements. If deterministic order\n",
      " |      isn't required, it can also improve performance to set\n",
      " |      `deterministic=False`.\n",
      " |      \n",
      " |      >>> dataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
      " |      >>> dataset = dataset.map(lambda x: x + 1,\n",
      " |      ...     num_parallel_calls=tf.data.AUTOTUNE,\n",
      " |      ...     deterministic=False)\n",
      " |      \n",
      " |      Args:\n",
      " |        map_func: A function mapping a dataset element to another dataset element.\n",
      " |        num_parallel_calls: (Optional.) A `tf.int32` scalar `tf.Tensor`,\n",
      " |          representing the number elements to process asynchronously in parallel.\n",
      " |          If not specified, elements will be processed sequentially. If the value\n",
      " |          `tf.data.AUTOTUNE` is used, then the number of parallel\n",
      " |          calls is set dynamically based on available CPU.\n",
      " |        deterministic: (Optional.) A boolean controlling whether determinism\n",
      " |          should be traded for performance by allowing elements to be produced out\n",
      " |          of order.  If `deterministic` is `None`, the\n",
      " |          `tf.data.Options.experimental_deterministic` dataset option (`True` by\n",
      " |          default) is used to decide whether to produce elements\n",
      " |          deterministically.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  options(self)\n",
      " |      Returns the options for this dataset and its inputs.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.data.Options` object representing the dataset options.\n",
      " |  \n",
      " |  padded_batch(self, batch_size, padded_shapes=None, padding_values=None, drop_remainder=False)\n",
      " |      Combines consecutive elements of this dataset into padded batches.\n",
      " |      \n",
      " |      This transformation combines multiple consecutive elements of the input\n",
      " |      dataset into a single element.\n",
      " |      \n",
      " |      Like `tf.data.Dataset.batch`, the components of the resulting element will\n",
      " |      have an additional outer dimension, which will be `batch_size` (or\n",
      " |      `N % batch_size` for the last element if `batch_size` does not divide the\n",
      " |      number of input elements `N` evenly and `drop_remainder` is `False`). If\n",
      " |      your program depends on the batches having the same outer dimension, you\n",
      " |      should set the `drop_remainder` argument to `True` to prevent the smaller\n",
      " |      batch from being produced.\n",
      " |      \n",
      " |      Unlike `tf.data.Dataset.batch`, the input elements to be batched may have\n",
      " |      different shapes, and this transformation will pad each component to the\n",
      " |      respective shape in `padded_shapes`. The `padded_shapes` argument\n",
      " |      determines the resulting shape for each dimension of each component in an\n",
      " |      output element:\n",
      " |      \n",
      " |      * If the dimension is a constant, the component will be padded out to that\n",
      " |        length in that dimension.\n",
      " |      * If the dimension is unknown, the component will be padded out to the\n",
      " |        maximum length of all elements in that dimension.\n",
      " |      \n",
      " |      >>> A = (tf.data.Dataset\n",
      " |      ...      .range(1, 5, output_type=tf.int32)\n",
      " |      ...      .map(lambda x: tf.fill([x], x)))\n",
      " |      >>> # Pad to the smallest per-batch size that fits all elements.\n",
      " |      >>> B = A.padded_batch(2)\n",
      " |      >>> for element in B.as_numpy_iterator():\n",
      " |      ...   print(element)\n",
      " |      [[1 0]\n",
      " |       [2 2]]\n",
      " |      [[3 3 3 0]\n",
      " |       [4 4 4 4]]\n",
      " |      >>> # Pad to a fixed size.\n",
      " |      >>> C = A.padded_batch(2, padded_shapes=5)\n",
      " |      >>> for element in C.as_numpy_iterator():\n",
      " |      ...   print(element)\n",
      " |      [[1 0 0 0 0]\n",
      " |       [2 2 0 0 0]]\n",
      " |      [[3 3 3 0 0]\n",
      " |       [4 4 4 4 0]]\n",
      " |      >>> # Pad with a custom value.\n",
      " |      >>> D = A.padded_batch(2, padded_shapes=5, padding_values=-1)\n",
      " |      >>> for element in D.as_numpy_iterator():\n",
      " |      ...   print(element)\n",
      " |      [[ 1 -1 -1 -1 -1]\n",
      " |       [ 2  2 -1 -1 -1]]\n",
      " |      [[ 3  3  3 -1 -1]\n",
      " |       [ 4  4  4  4 -1]]\n",
      " |      >>> # Components of nested elements can be padded independently.\n",
      " |      >>> elements = [([1, 2, 3], [10]),\n",
      " |      ...             ([4, 5], [11, 12])]\n",
      " |      >>> dataset = tf.data.Dataset.from_generator(\n",
      " |      ...     lambda: iter(elements), (tf.int32, tf.int32))\n",
      " |      >>> # Pad the first component of the tuple to length 4, and the second\n",
      " |      >>> # component to the smallest size that fits.\n",
      " |      >>> dataset = dataset.padded_batch(2,\n",
      " |      ...     padded_shapes=([4], [None]),\n",
      " |      ...     padding_values=(-1, 100))\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [(array([[ 1,  2,  3, -1], [ 4,  5, -1, -1]], dtype=int32),\n",
      " |        array([[ 10, 100], [ 11,  12]], dtype=int32))]\n",
      " |      >>> # Pad with a single value and multiple components.\n",
      " |      >>> E = tf.data.Dataset.zip((A, A)).padded_batch(2, padding_values=-1)\n",
      " |      >>> for element in E.as_numpy_iterator():\n",
      " |      ...   print(element)\n",
      " |      (array([[ 1, -1],\n",
      " |             [ 2,  2]], dtype=int32), array([[ 1, -1],\n",
      " |             [ 2,  2]], dtype=int32))\n",
      " |      (array([[ 3,  3,  3, -1],\n",
      " |             [ 4,  4,  4,  4]], dtype=int32), array([[ 3,  3,  3, -1],\n",
      " |             [ 4,  4,  4,  4]], dtype=int32))\n",
      " |      \n",
      " |      See also `tf.data.experimental.dense_to_sparse_batch`, which combines\n",
      " |      elements that may have different shapes into a `tf.sparse.SparseTensor`.\n",
      " |      \n",
      " |      Args:\n",
      " |        batch_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          consecutive elements of this dataset to combine in a single batch.\n",
      " |        padded_shapes: (Optional.) A nested structure of `tf.TensorShape` or\n",
      " |          `tf.int64` vector tensor-like objects representing the shape to which\n",
      " |          the respective component of each input element should be padded prior\n",
      " |          to batching. Any unknown dimensions will be padded to the maximum size\n",
      " |          of that dimension in each batch. If unset, all dimensions of all\n",
      " |          components are padded to the maximum size in the batch. `padded_shapes`\n",
      " |          must be set if any component has an unknown rank.\n",
      " |        padding_values: (Optional.) A nested structure of scalar-shaped\n",
      " |          `tf.Tensor`, representing the padding values to use for the respective\n",
      " |          components. None represents that the nested structure should be padded\n",
      " |          with default values.  Defaults are `0` for numeric types and the empty\n",
      " |          string for string types. The `padding_values` should have the\n",
      " |          same structure as the input dataset. If `padding_values` is a single\n",
      " |          element and the input dataset has multiple components, then the same\n",
      " |          `padding_values` will be used to pad every component of the dataset.\n",
      " |          If `padding_values` is a scalar, then its value will be broadcasted\n",
      " |          to match the shape of each component.\n",
      " |        drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\n",
      " |          whether the last batch should be dropped in the case it has fewer than\n",
      " |          `batch_size` elements; the default behavior is not to drop the smaller\n",
      " |          batch.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If a component has an unknown rank, and  the `padded_shapes`\n",
      " |          argument is not set.\n",
      " |  \n",
      " |  prefetch(self, buffer_size)\n",
      " |      Creates a `Dataset` that prefetches elements from this dataset.\n",
      " |      \n",
      " |      Most dataset input pipelines should end with a call to `prefetch`. This\n",
      " |      allows later elements to be prepared while the current element is being\n",
      " |      processed. This often improves latency and throughput, at the cost of\n",
      " |      using additional memory to store prefetched elements.\n",
      " |      \n",
      " |      Note: Like other `Dataset` methods, prefetch operates on the\n",
      " |      elements of the input dataset. It has no concept of examples vs. batches.\n",
      " |      `examples.prefetch(2)` will prefetch two elements (2 examples),\n",
      " |      while `examples.batch(20).prefetch(2)` will prefetch 2 elements\n",
      " |      (2 batches, of 20 examples each).\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(3)\n",
      " |      >>> dataset = dataset.prefetch(2)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [0, 1, 2]\n",
      " |      \n",
      " |      Args:\n",
      " |        buffer_size: A `tf.int64` scalar `tf.Tensor`, representing the maximum\n",
      " |          number of elements that will be buffered when prefetching.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  reduce(self, initial_state, reduce_func)\n",
      " |      Reduces the input dataset to a single element.\n",
      " |      \n",
      " |      The transformation calls `reduce_func` successively on every element of\n",
      " |      the input dataset until the dataset is exhausted, aggregating information in\n",
      " |      its internal state. The `initial_state` argument is used for the initial\n",
      " |      state and the final state is returned as the result.\n",
      " |      \n",
      " |      >>> tf.data.Dataset.range(5).reduce(np.int64(0), lambda x, _: x + 1).numpy()\n",
      " |      5\n",
      " |      >>> tf.data.Dataset.range(5).reduce(np.int64(0), lambda x, y: x + y).numpy()\n",
      " |      10\n",
      " |      \n",
      " |      Args:\n",
      " |        initial_state: An element representing the initial state of the\n",
      " |          transformation.\n",
      " |        reduce_func: A function that maps `(old_state, input_element)` to\n",
      " |          `new_state`. It must take two arguments and return a new element\n",
      " |          The structure of `new_state` must match the structure of\n",
      " |          `initial_state`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A dataset element corresponding to the final state of the transformation.\n",
      " |  \n",
      " |  repeat(self, count=None)\n",
      " |      Repeats this dataset so each original value is seen `count` times.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
      " |      >>> dataset = dataset.repeat(3)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [1, 2, 3, 1, 2, 3, 1, 2, 3]\n",
      " |      \n",
      " |      Note: If this dataset is a function of global state (e.g. a random number\n",
      " |      generator), then different repetitions may produce different elements.\n",
      " |      \n",
      " |      Args:\n",
      " |        count: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the\n",
      " |          number of times the dataset should be repeated. The default behavior (if\n",
      " |          `count` is `None` or `-1`) is for the dataset be repeated indefinitely.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  shard(self, num_shards, index)\n",
      " |      Creates a `Dataset` that includes only 1/`num_shards` of this dataset.\n",
      " |      \n",
      " |      `shard` is deterministic. The Dataset produced by `A.shard(n, i)` will\n",
      " |      contain all elements of A whose index mod n = i.\n",
      " |      \n",
      " |      >>> A = tf.data.Dataset.range(10)\n",
      " |      >>> B = A.shard(num_shards=3, index=0)\n",
      " |      >>> list(B.as_numpy_iterator())\n",
      " |      [0, 3, 6, 9]\n",
      " |      >>> C = A.shard(num_shards=3, index=1)\n",
      " |      >>> list(C.as_numpy_iterator())\n",
      " |      [1, 4, 7]\n",
      " |      >>> D = A.shard(num_shards=3, index=2)\n",
      " |      >>> list(D.as_numpy_iterator())\n",
      " |      [2, 5, 8]\n",
      " |      \n",
      " |      This dataset operator is very useful when running distributed training, as\n",
      " |      it allows each worker to read a unique subset.\n",
      " |      \n",
      " |      When reading a single input file, you can shard elements as follows:\n",
      " |      \n",
      " |      ```python\n",
      " |      d = tf.data.TFRecordDataset(input_file)\n",
      " |      d = d.shard(num_workers, worker_index)\n",
      " |      d = d.repeat(num_epochs)\n",
      " |      d = d.shuffle(shuffle_buffer_size)\n",
      " |      d = d.map(parser_fn, num_parallel_calls=num_map_threads)\n",
      " |      ```\n",
      " |      \n",
      " |      Important caveats:\n",
      " |      \n",
      " |      - Be sure to shard before you use any randomizing operator (such as\n",
      " |        shuffle).\n",
      " |      - Generally it is best if the shard operator is used early in the dataset\n",
      " |        pipeline. For example, when reading from a set of TFRecord files, shard\n",
      " |        before converting the dataset to input samples. This avoids reading every\n",
      " |        file on every worker. The following is an example of an efficient\n",
      " |        sharding strategy within a complete pipeline:\n",
      " |      \n",
      " |      ```python\n",
      " |      d = Dataset.list_files(pattern)\n",
      " |      d = d.shard(num_workers, worker_index)\n",
      " |      d = d.repeat(num_epochs)\n",
      " |      d = d.shuffle(shuffle_buffer_size)\n",
      " |      d = d.interleave(tf.data.TFRecordDataset,\n",
      " |                       cycle_length=num_readers, block_length=1)\n",
      " |      d = d.map(parser_fn, num_parallel_calls=num_map_threads)\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        num_shards: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          shards operating in parallel.\n",
      " |        index: A `tf.int64` scalar `tf.Tensor`, representing the worker index.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        InvalidArgumentError: if `num_shards` or `index` are illegal values.\n",
      " |      \n",
      " |          Note: error checking is done on a best-effort basis, and errors aren't\n",
      " |          guaranteed to be caught upon dataset creation. (e.g. providing in a\n",
      " |          placeholder tensor bypasses the early checking, and will instead result\n",
      " |          in an error during a session.run call.)\n",
      " |  \n",
      " |  shuffle(self, buffer_size, seed=None, reshuffle_each_iteration=None)\n",
      " |      Randomly shuffles the elements of this dataset.\n",
      " |      \n",
      " |      This dataset fills a buffer with `buffer_size` elements, then randomly\n",
      " |      samples elements from this buffer, replacing the selected elements with new\n",
      " |      elements. For perfect shuffling, a buffer size greater than or equal to the\n",
      " |      full size of the dataset is required.\n",
      " |      \n",
      " |      For instance, if your dataset contains 10,000 elements but `buffer_size` is\n",
      " |      set to 1,000, then `shuffle` will initially select a random element from\n",
      " |      only the first 1,000 elements in the buffer. Once an element is selected,\n",
      " |      its space in the buffer is replaced by the next (i.e. 1,001-st) element,\n",
      " |      maintaining the 1,000 element buffer.\n",
      " |      \n",
      " |      `reshuffle_each_iteration` controls whether the shuffle order should be\n",
      " |      different for each epoch. In TF 1.X, the idiomatic way to create epochs\n",
      " |      was through the `repeat` transformation:\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(3)\n",
      " |      >>> dataset = dataset.shuffle(3, reshuffle_each_iteration=True)\n",
      " |      >>> dataset = dataset.repeat(2)  # doctest: +SKIP\n",
      " |      [1, 0, 2, 1, 2, 0]\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(3)\n",
      " |      >>> dataset = dataset.shuffle(3, reshuffle_each_iteration=False)\n",
      " |      >>> dataset = dataset.repeat(2)  # doctest: +SKIP\n",
      " |      [1, 0, 2, 1, 0, 2]\n",
      " |      \n",
      " |      In TF 2.0, `tf.data.Dataset` objects are Python iterables which makes it\n",
      " |      possible to also create epochs through Python iteration:\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(3)\n",
      " |      >>> dataset = dataset.shuffle(3, reshuffle_each_iteration=True)\n",
      " |      >>> list(dataset.as_numpy_iterator())  # doctest: +SKIP\n",
      " |      [1, 0, 2]\n",
      " |      >>> list(dataset.as_numpy_iterator())  # doctest: +SKIP\n",
      " |      [1, 2, 0]\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(3)\n",
      " |      >>> dataset = dataset.shuffle(3, reshuffle_each_iteration=False)\n",
      " |      >>> list(dataset.as_numpy_iterator())  # doctest: +SKIP\n",
      " |      [1, 0, 2]\n",
      " |      >>> list(dataset.as_numpy_iterator())  # doctest: +SKIP\n",
      " |      [1, 0, 2]\n",
      " |      \n",
      " |      Args:\n",
      " |        buffer_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          elements from this dataset from which the new dataset will sample.\n",
      " |        seed: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the random\n",
      " |          seed that will be used to create the distribution. See\n",
      " |          `tf.random.set_seed` for behavior.\n",
      " |        reshuffle_each_iteration: (Optional.) A boolean, which if true indicates\n",
      " |          that the dataset should be pseudorandomly reshuffled each time it is\n",
      " |          iterated over. (Defaults to `True`.)\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  skip(self, count)\n",
      " |      Creates a `Dataset` that skips `count` elements from this dataset.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(10)\n",
      " |      >>> dataset = dataset.skip(7)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [7, 8, 9]\n",
      " |      \n",
      " |      Args:\n",
      " |        count: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          elements of this dataset that should be skipped to form the new dataset.\n",
      " |          If `count` is greater than the size of this dataset, the new dataset\n",
      " |          will contain no elements.  If `count` is -1, skips the entire dataset.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  take(self, count)\n",
      " |      Creates a `Dataset` with at most `count` elements from this dataset.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(10)\n",
      " |      >>> dataset = dataset.take(3)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [0, 1, 2]\n",
      " |      \n",
      " |      Args:\n",
      " |        count: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          elements of this dataset that should be taken to form the new dataset.\n",
      " |          If `count` is -1, or if `count` is greater than the size of this\n",
      " |          dataset, the new dataset will contain all elements of this dataset.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  unbatch(self)\n",
      " |      Splits elements of a dataset into multiple elements.\n",
      " |      \n",
      " |      For example, if elements of the dataset are shaped `[B, a0, a1, ...]`,\n",
      " |      where `B` may vary for each input element, then for each element in the\n",
      " |      dataset, the unbatched dataset will contain `B` consecutive elements\n",
      " |      of shape `[a0, a1, ...]`.\n",
      " |      \n",
      " |      >>> elements = [ [1, 2, 3], [1, 2], [1, 2, 3, 4] ]\n",
      " |      >>> dataset = tf.data.Dataset.from_generator(lambda: elements, tf.int64)\n",
      " |      >>> dataset = dataset.unbatch()\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [1, 2, 3, 1, 2, 1, 2, 3, 4]\n",
      " |      \n",
      " |      Note: `unbatch` requires a data copy to slice up the batched tensor into\n",
      " |      smaller, unbatched tensors. When optimizing performance, try to avoid\n",
      " |      unnecessary usage of `unbatch`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Dataset`.\n",
      " |  \n",
      " |  window(self, size, shift=None, stride=1, drop_remainder=False)\n",
      " |      Combines (nests of) input elements into a dataset of (nests of) windows.\n",
      " |      \n",
      " |      A \"window\" is a finite dataset of flat elements of size `size` (or possibly\n",
      " |      fewer if there are not enough input elements to fill the window and\n",
      " |      `drop_remainder` evaluates to `False`).\n",
      " |      \n",
      " |      The `shift` argument determines the number of input elements by which the\n",
      " |      window moves on each iteration.  If windows and elements are both numbered\n",
      " |      starting at 0, the first element in window `k` will be element `k * shift`\n",
      " |      of the input dataset. In particular, the first element of the first window\n",
      " |      will always be the first element of the input dataset.\n",
      " |      \n",
      " |      The `stride` argument determines the stride of the input elements, and the\n",
      " |      `shift` argument determines the shift of the window.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(7).window(2)\n",
      " |      >>> for window in dataset:\n",
      " |      ...   print(list(window.as_numpy_iterator()))\n",
      " |      [0, 1]\n",
      " |      [2, 3]\n",
      " |      [4, 5]\n",
      " |      [6]\n",
      " |      >>> dataset = tf.data.Dataset.range(7).window(3, 2, 1, True)\n",
      " |      >>> for window in dataset:\n",
      " |      ...   print(list(window.as_numpy_iterator()))\n",
      " |      [0, 1, 2]\n",
      " |      [2, 3, 4]\n",
      " |      [4, 5, 6]\n",
      " |      >>> dataset = tf.data.Dataset.range(7).window(3, 1, 2, True)\n",
      " |      >>> for window in dataset:\n",
      " |      ...   print(list(window.as_numpy_iterator()))\n",
      " |      [0, 2, 4]\n",
      " |      [1, 3, 5]\n",
      " |      [2, 4, 6]\n",
      " |      \n",
      " |      Note that when the `window` transformation is applied to a dataset of\n",
      " |      nested elements, it produces a dataset of nested windows.\n",
      " |      \n",
      " |      >>> nested = ([1, 2, 3, 4], [5, 6, 7, 8])\n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices(nested).window(2)\n",
      " |      >>> for window in dataset:\n",
      " |      ...   def to_numpy(ds):\n",
      " |      ...     return list(ds.as_numpy_iterator())\n",
      " |      ...   print(tuple(to_numpy(component) for component in window))\n",
      " |      ([1, 2], [5, 6])\n",
      " |      ([3, 4], [7, 8])\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices({'a': [1, 2, 3, 4]})\n",
      " |      >>> dataset = dataset.window(2)\n",
      " |      >>> for window in dataset:\n",
      " |      ...   def to_numpy(ds):\n",
      " |      ...     return list(ds.as_numpy_iterator())\n",
      " |      ...   print({'a': to_numpy(window['a'])})\n",
      " |      {'a': [1, 2]}\n",
      " |      {'a': [3, 4]}\n",
      " |      \n",
      " |      Args:\n",
      " |        size: A `tf.int64` scalar `tf.Tensor`, representing the number of elements\n",
      " |          of the input dataset to combine into a window. Must be positive.\n",
      " |        shift: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the\n",
      " |          number of input elements by which the window moves in each iteration.\n",
      " |          Defaults to `size`. Must be positive.\n",
      " |        stride: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the\n",
      " |          stride of the input elements in the sliding window. Must be positive.\n",
      " |          The default value of 1 means \"retain every input element\".\n",
      " |        drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\n",
      " |          whether the last windows should be dropped if their size is smaller than\n",
      " |          `size`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset` of (nests of) windows -- a finite datasets of flat\n",
      " |          elements created from the (nests of) input elements.\n",
      " |  \n",
      " |  with_options(self, options)\n",
      " |      Returns a new `tf.data.Dataset` with the given options set.\n",
      " |      \n",
      " |      The options are \"global\" in the sense they apply to the entire dataset.\n",
      " |      If options are set multiple times, they are merged as long as different\n",
      " |      options do not use different non-default values.\n",
      " |      \n",
      " |      >>> ds = tf.data.Dataset.range(5)\n",
      " |      >>> ds = ds.interleave(lambda x: tf.data.Dataset.range(5),\n",
      " |      ...                    cycle_length=3,\n",
      " |      ...                    num_parallel_calls=3)\n",
      " |      >>> options = tf.data.Options()\n",
      " |      >>> # This will make the interleave order non-deterministic.\n",
      " |      >>> options.experimental_deterministic = False\n",
      " |      >>> ds = ds.with_options(options)\n",
      " |      \n",
      " |      Args:\n",
      " |        options: A `tf.data.Options` that identifies the options the use.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset` with the given options.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: when an option is set more than once to a non-default value\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from DatasetV2:\n",
      " |  \n",
      " |  from_generator(generator, output_types=None, output_shapes=None, args=None, output_signature=None)\n",
      " |      Creates a `Dataset` whose elements are generated by `generator`. (deprecated arguments)\n",
      " |      \n",
      " |      Warning: SOME ARGUMENTS ARE DEPRECATED: `(output_shapes, output_types)`. They will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Use output_signature instead\n",
      " |      \n",
      " |      The `generator` argument must be a callable object that returns\n",
      " |      an object that supports the `iter()` protocol (e.g. a generator function).\n",
      " |      \n",
      " |      The elements generated by `generator` must be compatible with either the\n",
      " |      given `output_signature` argument or with the given `output_types` and\n",
      " |      (optionally) `output_shapes` arguments, whichiver was specified.\n",
      " |      \n",
      " |      The recommended way to call `from_generator` is to use the\n",
      " |      `output_signature` argument. In this case the output will be assumed to\n",
      " |      consist of objects with the classes, shapes and types defined by\n",
      " |      `tf.TypeSpec` objects from `output_signature` argument:\n",
      " |      \n",
      " |      >>> def gen():\n",
      " |      ...   ragged_tensor = tf.ragged.constant([[1, 2], [3]])\n",
      " |      ...   yield 42, ragged_tensor\n",
      " |      >>>\n",
      " |      >>> dataset = tf.data.Dataset.from_generator(\n",
      " |      ...      gen,\n",
      " |      ...      output_signature=(\n",
      " |      ...          tf.TensorSpec(shape=(), dtype=tf.int32),\n",
      " |      ...          tf.RaggedTensorSpec(shape=(2, None), dtype=tf.int32)))\n",
      " |      >>>\n",
      " |      >>> list(dataset.take(1))\n",
      " |      [(<tf.Tensor: shape=(), dtype=int32, numpy=42>,\n",
      " |      <tf.RaggedTensor [[1, 2], [3]]>)]\n",
      " |      \n",
      " |      There is also a deprecated way to call `from_generator` by either with\n",
      " |      `output_types` argument alone or together with `output_shapes` argument.\n",
      " |      In this case the output of the function will be assumed to consist of\n",
      " |      `tf.Tensor` objects with with the types defined by `output_types` and with\n",
      " |      the shapes which are either unknown or defined by `output_shapes`.\n",
      " |      \n",
      " |      Note: The current implementation of `Dataset.from_generator()` uses\n",
      " |      `tf.numpy_function` and inherits the same constraints. In particular, it\n",
      " |      requires the dataset and iterator related operations to be placed\n",
      " |      on a device in the same process as the Python program that called\n",
      " |      `Dataset.from_generator()`. The body of `generator` will not be\n",
      " |      serialized in a `GraphDef`, and you should not use this method if you\n",
      " |      need to serialize your model and restore it in a different environment.\n",
      " |      \n",
      " |      Note: If `generator` depends on mutable global variables or other external\n",
      " |      state, be aware that the runtime may invoke `generator` multiple times\n",
      " |      (in order to support repeating the `Dataset`) and at any time\n",
      " |      between the call to `Dataset.from_generator()` and the production of the\n",
      " |      first element from the generator. Mutating global variables or external\n",
      " |      state can cause undefined behavior, and we recommend that you explicitly\n",
      " |      cache any external state in `generator` before calling\n",
      " |      `Dataset.from_generator()`.\n",
      " |      \n",
      " |      Args:\n",
      " |        generator: A callable object that returns an object that supports the\n",
      " |          `iter()` protocol. If `args` is not specified, `generator` must take no\n",
      " |          arguments; otherwise it must take as many arguments as there are values\n",
      " |          in `args`.\n",
      " |        output_types: (Optional.) A nested structure of `tf.DType` objects\n",
      " |          corresponding to each component of an element yielded by `generator`.\n",
      " |        output_shapes: (Optional.) A nested structure of `tf.TensorShape` objects\n",
      " |          corresponding to each component of an element yielded by `generator`.\n",
      " |        args: (Optional.) A tuple of `tf.Tensor` objects that will be evaluated\n",
      " |          and passed to `generator` as NumPy-array arguments.\n",
      " |        output_signature: (Optional.) A nested structure of `tf.TypeSpec` objects\n",
      " |          corresponding to each component of an element yielded by `generator`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  from_tensor_slices(tensors)\n",
      " |      Creates a `Dataset` whose elements are slices of the given tensors.\n",
      " |      \n",
      " |      The given tensors are sliced along their first dimension. This operation\n",
      " |      preserves the structure of the input tensors, removing the first dimension\n",
      " |      of each tensor and using it as the dataset dimension. All input tensors\n",
      " |      must have the same size in their first dimensions.\n",
      " |      \n",
      " |      >>> # Slicing a 1D tensor produces scalar tensor elements.\n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [1, 2, 3]\n",
      " |      \n",
      " |      >>> # Slicing a 2D tensor produces 1D tensor elements.\n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices([[1, 2], [3, 4]])\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [array([1, 2], dtype=int32), array([3, 4], dtype=int32)]\n",
      " |      \n",
      " |      >>> # Slicing a tuple of 1D tensors produces tuple elements containing\n",
      " |      >>> # scalar tensors.\n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices(([1, 2], [3, 4], [5, 6]))\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [(1, 3, 5), (2, 4, 6)]\n",
      " |      \n",
      " |      >>> # Dictionary structure is also preserved.\n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices({\"a\": [1, 2], \"b\": [3, 4]})\n",
      " |      >>> list(dataset.as_numpy_iterator()) == [{'a': 1, 'b': 3},\n",
      " |      ...                                       {'a': 2, 'b': 4}]\n",
      " |      True\n",
      " |      \n",
      " |      >>> # Two tensors can be combined into one Dataset object.\n",
      " |      >>> features = tf.constant([[1, 3], [2, 1], [3, 3]]) # ==> 3x2 tensor\n",
      " |      >>> labels = tf.constant(['A', 'B', 'A']) # ==> 3x1 tensor\n",
      " |      >>> dataset = Dataset.from_tensor_slices((features, labels))\n",
      " |      >>> # Both the features and the labels tensors can be converted\n",
      " |      >>> # to a Dataset object separately and combined after.\n",
      " |      >>> features_dataset = Dataset.from_tensor_slices(features)\n",
      " |      >>> labels_dataset = Dataset.from_tensor_slices(labels)\n",
      " |      >>> dataset = Dataset.zip((features_dataset, labels_dataset))\n",
      " |      >>> # A batched feature and label set can be converted to a Dataset\n",
      " |      >>> # in similar fashion.\n",
      " |      >>> batched_features = tf.constant([[[1, 3], [2, 3]],\n",
      " |      ...                                 [[2, 1], [1, 2]],\n",
      " |      ...                                 [[3, 3], [3, 2]]], shape=(3, 2, 2))\n",
      " |      >>> batched_labels = tf.constant([['A', 'A'],\n",
      " |      ...                               ['B', 'B'],\n",
      " |      ...                               ['A', 'B']], shape=(3, 2, 1))\n",
      " |      >>> dataset = Dataset.from_tensor_slices((batched_features, batched_labels))\n",
      " |      >>> for element in dataset.as_numpy_iterator():\n",
      " |      ...   print(element)\n",
      " |      (array([[1, 3],\n",
      " |             [2, 3]], dtype=int32), array([[b'A'],\n",
      " |             [b'A']], dtype=object))\n",
      " |      (array([[2, 1],\n",
      " |             [1, 2]], dtype=int32), array([[b'B'],\n",
      " |             [b'B']], dtype=object))\n",
      " |      (array([[3, 3],\n",
      " |             [3, 2]], dtype=int32), array([[b'A'],\n",
      " |             [b'B']], dtype=object))\n",
      " |      \n",
      " |      Note that if `tensors` contains a NumPy array, and eager execution is not\n",
      " |      enabled, the values will be embedded in the graph as one or more\n",
      " |      `tf.constant` operations. For large datasets (> 1 GB), this can waste\n",
      " |      memory and run into byte limits of graph serialization. If `tensors`\n",
      " |      contains one or more large NumPy arrays, consider the alternative described\n",
      " |      in [this guide](\n",
      " |      https://tensorflow.org/guide/data#consuming_numpy_arrays).\n",
      " |      \n",
      " |      Args:\n",
      " |        tensors: A dataset element, with each component having the same size in\n",
      " |          the first dimension.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  from_tensors(tensors)\n",
      " |      Creates a `Dataset` with a single element, comprising the given tensors.\n",
      " |      \n",
      " |      `from_tensors` produces a dataset containing only a single element. To slice\n",
      " |      the input tensor into multiple elements, use `from_tensor_slices` instead.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.from_tensors([1, 2, 3])\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [array([1, 2, 3], dtype=int32)]\n",
      " |      >>> dataset = tf.data.Dataset.from_tensors(([1, 2, 3], 'A'))\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [(array([1, 2, 3], dtype=int32), b'A')]\n",
      " |      \n",
      " |      >>> # You can use `from_tensors` to produce a dataset which repeats\n",
      " |      >>> # the same example many times.\n",
      " |      >>> example = tf.constant([1,2,3])\n",
      " |      >>> dataset = tf.data.Dataset.from_tensors(example).repeat(2)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [array([1, 2, 3], dtype=int32), array([1, 2, 3], dtype=int32)]\n",
      " |      \n",
      " |      Note that if `tensors` contains a NumPy array, and eager execution is not\n",
      " |      enabled, the values will be embedded in the graph as one or more\n",
      " |      `tf.constant` operations. For large datasets (> 1 GB), this can waste\n",
      " |      memory and run into byte limits of graph serialization. If `tensors`\n",
      " |      contains one or more large NumPy arrays, consider the alternative described\n",
      " |      in [this\n",
      " |      guide](https://tensorflow.org/guide/data#consuming_numpy_arrays).\n",
      " |      \n",
      " |      Args:\n",
      " |        tensors: A dataset element.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  list_files(file_pattern, shuffle=None, seed=None)\n",
      " |      A dataset of all files matching one or more glob patterns.\n",
      " |      \n",
      " |      The `file_pattern` argument should be a small number of glob patterns.\n",
      " |      If your filenames have already been globbed, use\n",
      " |      `Dataset.from_tensor_slices(filenames)` instead, as re-globbing every\n",
      " |      filename with `list_files` may result in poor performance with remote\n",
      " |      storage systems.\n",
      " |      \n",
      " |      Note: The default behavior of this method is to return filenames in\n",
      " |      a non-deterministic random shuffled order. Pass a `seed` or `shuffle=False`\n",
      " |      to get results in a deterministic order.\n",
      " |      \n",
      " |      Example:\n",
      " |        If we had the following files on our filesystem:\n",
      " |      \n",
      " |          - /path/to/dir/a.txt\n",
      " |          - /path/to/dir/b.py\n",
      " |          - /path/to/dir/c.py\n",
      " |      \n",
      " |        If we pass \"/path/to/dir/*.py\" as the directory, the dataset\n",
      " |        would produce:\n",
      " |      \n",
      " |          - /path/to/dir/b.py\n",
      " |          - /path/to/dir/c.py\n",
      " |      \n",
      " |      Args:\n",
      " |        file_pattern: A string, a list of strings, or a `tf.Tensor` of string type\n",
      " |          (scalar or vector), representing the filename glob (i.e. shell wildcard)\n",
      " |          pattern(s) that will be matched.\n",
      " |        shuffle: (Optional.) If `True`, the file names will be shuffled randomly.\n",
      " |          Defaults to `True`.\n",
      " |        seed: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the random\n",
      " |          seed that will be used to create the distribution. See\n",
      " |          `tf.random.set_seed` for behavior.\n",
      " |      \n",
      " |      Returns:\n",
      " |       Dataset: A `Dataset` of strings corresponding to file names.\n",
      " |  \n",
      " |  range(*args, **kwargs)\n",
      " |      Creates a `Dataset` of a step-separated range of values.\n",
      " |      \n",
      " |      >>> list(Dataset.range(5).as_numpy_iterator())\n",
      " |      [0, 1, 2, 3, 4]\n",
      " |      >>> list(Dataset.range(2, 5).as_numpy_iterator())\n",
      " |      [2, 3, 4]\n",
      " |      >>> list(Dataset.range(1, 5, 2).as_numpy_iterator())\n",
      " |      [1, 3]\n",
      " |      >>> list(Dataset.range(1, 5, -2).as_numpy_iterator())\n",
      " |      []\n",
      " |      >>> list(Dataset.range(5, 1).as_numpy_iterator())\n",
      " |      []\n",
      " |      >>> list(Dataset.range(5, 1, -2).as_numpy_iterator())\n",
      " |      [5, 3]\n",
      " |      >>> list(Dataset.range(2, 5, output_type=tf.int32).as_numpy_iterator())\n",
      " |      [2, 3, 4]\n",
      " |      >>> list(Dataset.range(1, 5, 2, output_type=tf.float32).as_numpy_iterator())\n",
      " |      [1.0, 3.0]\n",
      " |      \n",
      " |      Args:\n",
      " |        *args: follows the same semantics as python's xrange.\n",
      " |          len(args) == 1 -> start = 0, stop = args[0], step = 1.\n",
      " |          len(args) == 2 -> start = args[0], stop = args[1], step = 1.\n",
      " |          len(args) == 3 -> start = args[0], stop = args[1], step = args[2].\n",
      " |        **kwargs:\n",
      " |          - output_type: Its expected dtype. (Optional, default: `tf.int64`).\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `RangeDataset`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if len(args) == 0.\n",
      " |  \n",
      " |  zip(datasets)\n",
      " |      Creates a `Dataset` by zipping together the given datasets.\n",
      " |      \n",
      " |      This method has similar semantics to the built-in `zip()` function\n",
      " |      in Python, with the main difference being that the `datasets`\n",
      " |      argument can be an arbitrary nested structure of `Dataset` objects.\n",
      " |      \n",
      " |      >>> # The nested structure of the `datasets` argument determines the\n",
      " |      >>> # structure of elements in the resulting dataset.\n",
      " |      >>> a = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\n",
      " |      >>> b = tf.data.Dataset.range(4, 7)  # ==> [ 4, 5, 6 ]\n",
      " |      >>> ds = tf.data.Dataset.zip((a, b))\n",
      " |      >>> list(ds.as_numpy_iterator())\n",
      " |      [(1, 4), (2, 5), (3, 6)]\n",
      " |      >>> ds = tf.data.Dataset.zip((b, a))\n",
      " |      >>> list(ds.as_numpy_iterator())\n",
      " |      [(4, 1), (5, 2), (6, 3)]\n",
      " |      >>>\n",
      " |      >>> # The `datasets` argument may contain an arbitrary number of datasets.\n",
      " |      >>> c = tf.data.Dataset.range(7, 13).batch(2)  # ==> [ [7, 8],\n",
      " |      ...                                            #       [9, 10],\n",
      " |      ...                                            #       [11, 12] ]\n",
      " |      >>> ds = tf.data.Dataset.zip((a, b, c))\n",
      " |      >>> for element in ds.as_numpy_iterator():\n",
      " |      ...   print(element)\n",
      " |      (1, 4, array([7, 8]))\n",
      " |      (2, 5, array([ 9, 10]))\n",
      " |      (3, 6, array([11, 12]))\n",
      " |      >>>\n",
      " |      >>> # The number of elements in the resulting dataset is the same as\n",
      " |      >>> # the size of the smallest dataset in `datasets`.\n",
      " |      >>> d = tf.data.Dataset.range(13, 15)  # ==> [ 13, 14 ]\n",
      " |      >>> ds = tf.data.Dataset.zip((a, d))\n",
      " |      >>> list(ds.as_numpy_iterator())\n",
      " |      [(1, 13), (2, 14)]\n",
      " |      \n",
      " |      Args:\n",
      " |        datasets: A nested structure of datasets.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from DatasetV2:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from collections.abc.Iterable:\n",
      " |  \n",
      " |  __subclasshook__(C) from abc.ABCMeta\n",
      " |      Abstract classes can override this to customize issubclass().\n",
      " |      \n",
      " |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      " |      It should return True, False or NotImplemented.  If it returns\n",
      " |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      " |      overrides the normal algorithm (and the outcome is cached).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(gen.training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'albumentations'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-ad2138c2ae91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# import tensorflow_datasets as tfds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m from albumentations import (\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mCompose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomBrightness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJpegCompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHueSaturationValue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomContrast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHorizontalFlip\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mRotate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'albumentations'"
     ]
    }
   ],
   "source": [
    "# necessary imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import tensorflow_datasets as tfds\n",
    "from functools import partial\n",
    "from albumentations import (\n",
    "    Compose, RandomBrightness, JpegCompression, HueSaturationValue, RandomContrast, HorizontalFlip,\n",
    "    Rotate\n",
    ")\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100, 18)\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "# TODO test on entire test dataset\n",
    "obj = s3.Object('canopy-production-ml', \"chips/cloudfree-merge-polygons/split/test/100/100_1000_1000.tif\")\n",
    "obj_bytes = io.BytesIO(obj.get()['Body'].read())\n",
    "with rasterio.open(obj_bytes) as src:\n",
    "    img_test = np.transpose(src.read(), (1, 2, 0))\n",
    "print(img_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = tf.image.convert_image_dtype(img_test,tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = tf.image.random_flip_left_right(train_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package tensorflow._api.v2.image in tensorflow._api.v2:\n",
      "\n",
      "NAME\n",
      "    tensorflow._api.v2.image - Image ops.\n",
      "\n",
      "DESCRIPTION\n",
      "    The `tf.image` module contains various functions for image\n",
      "    processing and decoding-encoding Ops.\n",
      "    \n",
      "    Many of the encoding/decoding functions are also available in the\n",
      "    core `tf.io` module.\n",
      "    \n",
      "    ## Image processing\n",
      "    \n",
      "    ### Resizing\n",
      "    \n",
      "    The resizing Ops accept input images as tensors of several types. They always\n",
      "    output resized images as float32 tensors.\n",
      "    \n",
      "    The convenience function `tf.image.resize` supports both 4-D\n",
      "    and 3-D tensors as input and output.  4-D tensors are for batches of images,\n",
      "    3-D tensors for individual images.\n",
      "    \n",
      "    Resized images will be distorted if their original aspect ratio is not the\n",
      "    same as size. To avoid distortions see tf.image.resize_with_pad.\n",
      "    \n",
      "    *   `tf.image.resize`\n",
      "    *   `tf.image.resize_with_pad`\n",
      "    *   `tf.image.resize_with_crop_or_pad`\n",
      "    \n",
      "    The Class `tf.image.ResizeMethod` provides various resize methods like\n",
      "    `bilinear`, `nearest_neighbor`.\n",
      "    \n",
      "    ### Converting Between Colorspaces\n",
      "    \n",
      "    Image ops work either on individual images or on batches of images, depending on\n",
      "    the shape of their input Tensor.\n",
      "    \n",
      "    If 3-D, the shape is `[height, width, channels]`, and the Tensor represents one\n",
      "    image. If 4-D, the shape is `[batch_size, height, width, channels]`, and the\n",
      "    Tensor represents `batch_size` images.\n",
      "    \n",
      "    Currently, `channels` can usefully be 1, 2, 3, or 4. Single-channel images are\n",
      "    grayscale, images with 3 channels are encoded as either RGB or HSV. Images\n",
      "    with 2 or 4 channels include an alpha channel, which has to be stripped from the\n",
      "    image before passing the image to most image processing functions (and can be\n",
      "    re-attached later).\n",
      "    \n",
      "    Internally, images are either stored in as one `float32` per channel per pixel\n",
      "    (implicitly, values are assumed to lie in `[0,1)`) or one `uint8` per channel\n",
      "    per pixel (values are assumed to lie in `[0,255]`).\n",
      "    \n",
      "    TensorFlow can convert between images in RGB or HSV or YIQ.\n",
      "    \n",
      "    *   `tf.image.rgb_to_grayscale`, `tf.image.grayscale_to_rgb`\n",
      "    *   `tf.image.rgb_to_hsv`, `tf.image.hsv_to_rgb`\n",
      "    *   `tf.image.rgb_to_yiq`, `tf.image.yiq_to_rgb`\n",
      "    *   `tf.image.rgb_to_yuv`, `tf.image.yuv_to_rgb`\n",
      "    *   `tf.image.image_gradients`\n",
      "    *   `tf.image.convert_image_dtype`\n",
      "    \n",
      "    ### Image Adjustments\n",
      "    \n",
      "    TensorFlow provides functions to adjust images in various ways: brightness,\n",
      "    contrast, hue, and saturation.  Each adjustment can be done with predefined\n",
      "    parameters or with random parameters picked from predefined intervals. Random\n",
      "    adjustments are often useful to expand a training set and reduce overfitting.\n",
      "    \n",
      "    If several adjustments are chained it is advisable to minimize the number of\n",
      "    redundant conversions by first converting the images to the most natural data\n",
      "    type and representation.\n",
      "    \n",
      "    *   `tf.image.adjust_brightness`\n",
      "    *   `tf.image.adjust_contrast`\n",
      "    *   `tf.image.adjust_gamma`\n",
      "    *   `tf.image.adjust_hue`\n",
      "    *   `tf.image.adjust_jpeg_quality`\n",
      "    *   `tf.image.adjust_saturation`\n",
      "    *   `tf.image.random_brightness`\n",
      "    *   `tf.image.random_contrast`\n",
      "    *   `tf.image.random_hue`\n",
      "    *   `tf.image.random_saturation`\n",
      "    *   `tf.image.per_image_standardization`\n",
      "    \n",
      "    ### Working with Bounding Boxes\n",
      "    \n",
      "    *   `tf.image.draw_bounding_boxes`\n",
      "    *   `tf.image.combined_non_max_suppression`\n",
      "    *   `tf.image.generate_bounding_box_proposals`\n",
      "    *   `tf.image.non_max_suppression`\n",
      "    *   `tf.image.non_max_suppression_overlaps`\n",
      "    *   `tf.image.non_max_suppression_padded`\n",
      "    *   `tf.image.non_max_suppression_with_scores`\n",
      "    *   `tf.image.pad_to_bounding_box`\n",
      "    *   `tf.image.sample_distorted_bounding_box`\n",
      "    \n",
      "    ### Cropping\n",
      "    \n",
      "    *   `tf.image.central_crop`\n",
      "    *   `tf.image.crop_and_resize`\n",
      "    *   `tf.image.crop_to_bounding_box`\n",
      "    *   `tf.io.decode_and_crop_jpeg`\n",
      "    *   `tf.image.extract_glimpse`\n",
      "    *   `tf.image.random_crop`\n",
      "    *   `tf.image.resize_with_crop_or_pad`\n",
      "    \n",
      "    ### Flipping, Rotating and Transposing\n",
      "    \n",
      "    *   `tf.image.flip_left_right`\n",
      "    *   `tf.image.flip_up_down`\n",
      "    *   `tf.image.random_flip_left_right`\n",
      "    *   `tf.image.random_flip_up_down`\n",
      "    *   `tf.image.rot90`\n",
      "    *   `tf.image.transpose`\n",
      "    \n",
      "    ## Image decoding and encoding\n",
      "    \n",
      "    TensorFlow provides Ops to decode and encode JPEG and PNG formats.  Encoded\n",
      "    images are represented by scalar string Tensors, decoded images by 3-D uint8\n",
      "    tensors of shape `[height, width, channels]`. (PNG also supports uint16.)\n",
      "    \n",
      "    Note: `decode_gif` returns a 4-D array `[num_frames, height, width, 3]`\n",
      "    \n",
      "    The encode and decode Ops apply to one image at a time.  Their input and output\n",
      "    are all of variable size.  If you need fixed size images, pass the output of\n",
      "    the decode Ops to one of the cropping and resizing Ops.\n",
      "    \n",
      "    *   `tf.io.decode_bmp`\n",
      "    *   `tf.io.decode_gif`\n",
      "    *   `tf.io.decode_image`\n",
      "    *   `tf.io.decode_jpeg`\n",
      "    *   `tf.io.decode_and_crop_jpeg`\n",
      "    *   `tf.io.decode_png`\n",
      "    *   `tf.io.encode_jpeg`\n",
      "    *   `tf.io.encode_png`\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "\n",
      "\n",
      "FILE\n",
      "    /Users/purgatorid/opt/anaconda3/envs/ml-conda/lib/python3.7/site-packages/tensorflow/_api/v2/image/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## David's area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "import io\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import keras\n",
    "\n",
    "from data_loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data augmentation enabled \n",
      "Training on 7886 images \n",
      "Validation on 366 images \n",
      "Your training file is missing positive labels for classes ['3']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "gen = DataLoader(label_file_path_train=\"labels_1_4_train_v2.csv\",\n",
    "                 label_file_path_val=\"val_labels.csv\",\n",
    "                 bucket_name='canopy-production-ml',\n",
    "                 data_extension_type='.tif',\n",
    "                 training_data_shape=(100, 100, 18),\n",
    "                 augment=True,\n",
    "                 random_flip_up_down=False, #Randomly flips an image vertically (upside down). With a 1 in 2 chance, outputs the contents of `image` flipped along the first dimension, which is `height`.\n",
    "                 random_flip_left_right=False,\n",
    "                 flip_left_right=True,\n",
    "                 flip_up_down=True,\n",
    "                 rot90=False,\n",
    "                 transpose=False,\n",
    "                 enable_shuffle=True,\n",
    "                 # training_data_shuffle_buffer_size=10,\n",
    "                 training_data_batch_size=batch_size,\n",
    "                 training_data_type=tf.float32,\n",
    "                 label_data_type=tf.uint8,\n",
    "                 enable_data_prefetch=True,\n",
    "                 data_prefetch_size=tf.data.experimental.AUTOTUNE,\n",
    "                 num_parallel_calls=int(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 2.7809829059829063,\n",
       " 1: 0.6314895681707908,\n",
       " 2: 1.1188480550182678,\n",
       " 4: 0.6000461041954819,\n",
       " 5: 1.9281481481481484,\n",
       " 6: 1.6574339382362304,\n",
       " 7: 1.2629791363415817,\n",
       " 8: 0.8022807828633072,\n",
       " 9: 0.7483110536150641}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(numclasses,input_shape):\n",
    "    # parameters for CNN\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "\n",
    "    # introduce a additional layer to get from bands to 3 input channels\n",
    "    input_tensor = Conv2D(3, (1, 1))(input_tensor)\n",
    "\n",
    "    base_model_resnet50 = keras.applications.ResNet50(include_top=False,\n",
    "                              weights='imagenet',\n",
    "                              input_shape=(100, 100, 3))\n",
    "    base_model = keras.applications.ResNet50(include_top=False,\n",
    "                     weights=None,\n",
    "                     input_tensor=input_tensor)\n",
    "\n",
    "    for i, layer in enumerate(base_model_resnet50.layers):\n",
    "        # we must skip input layer, which has no weights\n",
    "        if i == 0:\n",
    "            continue\n",
    "        base_model.layers[i+1].set_weights(layer.get_weights())\n",
    "\n",
    "    # add a global spatial average pooling layer\n",
    "    top_model = base_model.output\n",
    "    top_model = GlobalAveragePooling2D()(top_model)\n",
    "\n",
    "    # let's add a fully-connected layer\n",
    "    top_model = Dense(2048, activation='relu')(top_model)\n",
    "    top_model = Dense(2048, activation='relu')(top_model)\n",
    "    # and a logistic layer\n",
    "    predictions = Dense(numclasses, activation='softmax')(top_model)\n",
    "\n",
    "    # this is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "wandb: Currently logged in as: davidanagy (use `wandb login --relogin` to force relogin)\n",
      "C:\\Anaconda3\\envs\\canopy_test_models\\lib\\site-packages\\IPython\\html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n",
      "wandb: wandb version 0.10.21 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.20<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">baseline</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/davidanagy/canopy-first-model-testing\" target=\"_blank\">https://wandb.ai/davidanagy/canopy-first-model-testing</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/davidanagy/canopy-first-model-testing/runs/2v4mxph8\" target=\"_blank\">https://wandb.ai/davidanagy/canopy-first-model-testing/runs/2v4mxph8</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\David\\canopy\\cb_feature_detection\\model-development\\wandb\\run-20210305_165733-2v4mxph8</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(2v4mxph8)</h1><iframe src=\"https://wandb.ai/davidanagy/canopy-first-model-testing/runs/2v4mxph8\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x16daf393208>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "\n",
    "wandb.init(project=\"canopy-first-model-testing\", name=\"baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function init in module wandb.sdk.wandb_init:\n",
      "\n",
      "init(job_type: Union[str, NoneType] = None, dir=None, config: Union[Dict, str, NoneType] = None, project: Union[str, NoneType] = None, entity: Union[str, NoneType] = None, reinit: bool = None, tags: Union[Sequence, NoneType] = None, group: Union[str, NoneType] = None, name: Union[str, NoneType] = None, notes: Union[str, NoneType] = None, magic: Union[dict, str, bool] = None, config_exclude_keys=None, config_include_keys=None, anonymous: Union[str, NoneType] = None, mode: Union[str, NoneType] = None, allow_val_change: Union[bool, NoneType] = None, resume: Union[bool, str, NoneType] = None, force: Union[bool, NoneType] = None, tensorboard=None, sync_tensorboard=None, monitor_gym=None, save_code=None, id=None, settings: Union[wandb.sdk.wandb_settings.Settings, Dict[str, Any], NoneType] = None) -> Union[wandb.sdk.wandb_run.Run, wandb.sdk.lib.disabled.RunDisabled, NoneType]\n",
      "    Start a new tracked run with `wandb.init()`.\n",
      "    \n",
      "    In an ML training pipeline, you could add `wandb.init()`\n",
      "    to the beginning of your training script as well as your evaluation\n",
      "    script, and each piece would be tracked as a run in W&B.\n",
      "    \n",
      "    `wandb.init()` spawns a new background process to log data to a run, and it\n",
      "    also syncs data to wandb.ai by default so you can see live visualizations.\n",
      "    Call `wandb.init()` to start a run before logging data with `wandb.log()`.\n",
      "    \n",
      "    `wandb.init()` returns a run object, and you can also access the run object\n",
      "    with wandb.run.\n",
      "    \n",
      "    Arguments:\n",
      "        project: (str, optional) The name of the project where you're sending\n",
      "            the new run. If the project is not specified, the run is put in an\n",
      "            \"Uncategorized\" project.\n",
      "        entity: (str, optional) An entity is a username or team name where\n",
      "            you're sending runs. This entity must exist before you can send runs\n",
      "            there, so make sure to create your account or team in the UI before\n",
      "            starting to log runs.\n",
      "            If you don't specify an entity, the run will be sent to your default\n",
      "            entity, which is usually your username. Change your default entity\n",
      "            in [Settings](wandb.ai/settings) under \"default location to create\n",
      "            new projects\".\n",
      "        config: (dict, argparse, absl.flags, str, optional)\n",
      "            This sets wandb.config, a dictionary-like object for saving inputs\n",
      "            to your job, like hyperparameters for a model or settings for a data\n",
      "            preprocessing job. The config will show up in a table in the UI that\n",
      "            you can use to group, filter, and sort runs. Keys should not contain\n",
      "            `.` in their names, and values should be under 10 MB.\n",
      "            If dict, argparse or absl.flags: will load the key value pairs into\n",
      "                the wandb.config object.\n",
      "            If str: will look for a yaml file by that name, and load config from\n",
      "                that file into the wandb.config object.\n",
      "        save_code: (bool, optional) Turn this on to save the main script or\n",
      "            notebook to W&B. This is valuable for improving experiment\n",
      "            reproducibility and to diff code across experiments in the UI. By\n",
      "            default this is off, but you can flip the default behavior to \"on\"\n",
      "            in [Settings](wandb.ai/settings).\n",
      "        group: (str, optional) Specify a group to organize individual runs into\n",
      "            a larger experiment. For example, you might be doing cross\n",
      "            validation, or you might have multiple jobs that train and evaluate\n",
      "            a model against different test sets. Group gives you a way to\n",
      "            organize runs together into a larger whole, and you can toggle this\n",
      "            on and off in the UI. For more details, see\n",
      "            [Grouping](docs.wandb.com/library/grouping).\n",
      "        job_type: (str, optional) Specify the type of run, which is useful when\n",
      "            you're grouping runs together into larger experiments using group.\n",
      "            For example, you might have multiple jobs in a group, with job types\n",
      "            like train and eval. Setting this makes it easy to filter and group\n",
      "            similar runs together in the UI so you can compare apples to apples.\n",
      "        tags: (list, optional) A list of strings, which will populate the list\n",
      "            of tags on this run in the UI. Tags are useful for organizing runs\n",
      "            together, or applying temporary labels like \"baseline\" or\n",
      "            \"production\". It's easy to add and remove tags in the UI, or filter\n",
      "            down to just runs with a specific tag.\n",
      "        name: (str, optional) A short display name for this run, which is how\n",
      "            you'll identify this run in the UI. By default we generate a random\n",
      "            two-word name that lets you easily cross-reference runs from the\n",
      "            table to charts. Keeping these run names short makes the chart\n",
      "            legends and tables easier to read. If you're looking for a place to\n",
      "            save your hyperparameters, we recommend saving those in config.\n",
      "        notes: (str, optional) A longer description of the run, like a -m commit\n",
      "            message in git. This helps you remember what you were doing when you\n",
      "            ran this run.\n",
      "        dir: (str, optional) An absolute path to a directory where metadata will\n",
      "            be stored. When you call download() on an artifact, this is the\n",
      "            directory where downloaded files will be saved. By default this is\n",
      "            the ./wandb directory.\n",
      "        sync_tensorboard: (bool, optional) Whether to copy all TensorBoard logs\n",
      "            to W&B (default: False).\n",
      "            [Tensorboard](https://docs.wandb.com/integrations/tensorboard)\n",
      "        resume (bool, str, optional): Sets the resuming behavior. Options:\n",
      "            \"allow\", \"must\", \"never\", \"auto\" or None. Defaults to None.\n",
      "            Cases:\n",
      "            - None (default): If the new run has the same ID as a previous run,\n",
      "                this run overwrites that data.\n",
      "            - \"auto\" (or True): if the preivous run on this machine crashed,\n",
      "                automatically resume it. Otherwise, start a new run.\n",
      "            - \"allow\": if id is set with init(id=\"UNIQUE_ID\") or\n",
      "                WANDB_RUN_ID=\"UNIQUE_ID\" and it is identical to a previous run,\n",
      "                wandb will automatically resume the run with that id. Otherwise,\n",
      "                wandb will start a new run.\n",
      "            - \"never\": if id is set with init(id=\"UNIQUE_ID\") or\n",
      "                WANDB_RUN_ID=\"UNIQUE_ID\" and it is identical to a previous run,\n",
      "                wandb will crash.\n",
      "            - \"must\": if id is set with init(id=\"UNIQUE_ID\") or\n",
      "                WANDB_RUN_ID=\"UNIQUE_ID\" and it is identical to a previous run,\n",
      "                wandb will automatically resume the run with the id. Otherwise\n",
      "                wandb will crash.\n",
      "            See https://docs.wandb.com/library/advanced/resuming for more.\n",
      "        reinit: (bool, optional) Allow multiple wandb.init() calls in the same\n",
      "            process. (default: False)\n",
      "        magic: (bool, dict, or str, optional) The bool controls whether we try to\n",
      "            auto-instrument your script, capturing basic details of your run\n",
      "            without you having to add more wandb code. (default: False)\n",
      "            You can also pass a dict, json string, or yaml filename.\n",
      "        config_exclude_keys: (list, optional) string keys to exclude from\n",
      "            `wandb.config`.\n",
      "        config_include_keys: (list, optional) string keys to include in\n",
      "            wandb.config.\n",
      "        anonymous: (str, optional) Controls anonymous data logging. Options:\n",
      "            - \"never\" (default): requires you to link your W&B account before\n",
      "                tracking the run so you don't accidentally create an anonymous\n",
      "                run.\n",
      "            - \"allow\": lets a logged-in user track runs with their account, but\n",
      "                lets someone who is running the script without a W&B account see\n",
      "                the charts in the UI.\n",
      "            - \"must\": sends the run to an anonymous account instead of to a\n",
      "                signed-up user account.\n",
      "        mode: (str, optional) Can be \"online\", \"offline\" or \"disabled\". Defaults to\n",
      "            online.\n",
      "        allow_val_change: (bool, optional) Whether to allow config values to\n",
      "            change after setting the keys once. By default we throw an exception\n",
      "            if a config value is overwritten. If you want to track something\n",
      "            like a varying learning_rate at multiple times during training, use\n",
      "            wandb.log() instead. (default: False in scripts, True in Jupyter)\n",
      "        force: (bool, optional) If True, this crashes the script if a user isn't\n",
      "            logged in to W&B. If False, this will let the script run in offline\n",
      "            mode if a user isn't logged in to W&B. (default: False)\n",
      "        sync_tensorboard: (bool, optional) Synchronize wandb logs from tensorboard or\n",
      "            tensorboardX and saves the relevant events file. Defaults to false.\n",
      "        monitor_gym: (bool, optional) automatically logs videos of environment when\n",
      "            using OpenAI Gym. (default: False)\n",
      "            See https://docs.wandb.com/library/integrations/openai-gym\n",
      "        id: (str, optional) A unique ID for this run, used for Resuming. It must\n",
      "            be unique in the project, and if you delete a run you can't reuse\n",
      "            the ID. Use the name field for a short descriptive name, or config\n",
      "            for saving hyperparameters to compare across runs. The ID cannot\n",
      "            contain special characters.\n",
      "            See https://docs.wandb.com/library/resuming\n",
      "    \n",
      "    \n",
      "    Examples:\n",
      "        Basic usage\n",
      "        ```\n",
      "        wandb.init()\n",
      "        ```\n",
      "    \n",
      "        Launch multiple runs from the same script\n",
      "        ```\n",
      "        for x in range(10):\n",
      "            with wandb.init(project=\"my-projo\") as run:\n",
      "                for y in range(100):\n",
      "                    run.log({\"metric\": x+y})\n",
      "        ```\n",
      "    \n",
      "    Raises:\n",
      "        Exception: if problem.\n",
      "    \n",
      "    Returns:\n",
      "        A `Run` object.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(wandb.init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class WandbCallback in module wandb.integration.keras.keras:\n",
      "\n",
      "class WandbCallback(tensorflow.python.keras.callbacks.Callback)\n",
      " |  WandbCallback(monitor='val_loss', verbose=0, mode='auto', save_weights_only=False, log_weights=False, log_gradients=False, save_model=True, training_data=None, validation_data=None, labels=[], data_type=None, predictions=36, generator=None, input_type=None, output_type=None, log_evaluation=False, validation_steps=None, class_colors=None, log_batch_frequency=None, log_best_prefix='best_', save_graph=True)\n",
      " |  \n",
      " |  WandbCallback automatically integrates keras with wandb.\n",
      " |  \n",
      " |  Example:\n",
      " |      ```\n",
      " |      model.fit(X_train, y_train,  validation_data=(X_test, y_test),\n",
      " |          callbacks=[WandbCallback()])\n",
      " |      ```\n",
      " |  \n",
      " |  WandbCallback will automatically log history data from any\n",
      " |      metrics collected by keras: loss and anything passed into keras_model.compile() \n",
      " |  \n",
      " |  WandbCallback will set summary metrics for the run associated with the \"best\" training\n",
      " |      step, where \"best\" is defined by the `monitor` and `mode` attribues.  This defaults\n",
      " |      to the epoch with the minimum val_loss. WandbCallback will by default save the model \n",
      " |      associated with the best epoch..\n",
      " |  \n",
      " |  WandbCallback can optionally log gradient and parameter histograms. \n",
      " |  \n",
      " |  WandbCallback can optionally save training and validation data for wandb to visualize.\n",
      " |  \n",
      " |  Arguments:\n",
      " |      monitor (str): name of metric to monitor.  Defaults to val_loss.\n",
      " |      mode (str): one of {\"auto\", \"min\", \"max\"}.\n",
      " |          \"min\" - save model when monitor is minimized\n",
      " |          \"max\" - save model when monitor is maximized\n",
      " |          \"auto\" - try to guess when to save the model (default).\n",
      " |      save_model:\n",
      " |          True - save a model when monitor beats all previous epochs\n",
      " |          False - don't save models\n",
      " |      save_graph: (boolean): if True save model graph to wandb (default: True).\n",
      " |      save_weights_only (boolean): if True, then only the model's weights will be\n",
      " |          saved (`model.save_weights(filepath)`), else the full model\n",
      " |          is saved (`model.save(filepath)`).\n",
      " |      log_weights: (boolean) if True save histograms of the model's layer's weights.\n",
      " |      log_gradients: (boolean) if True log histograms of the training gradients\n",
      " |      training_data: (tuple) Same format (X,y) as passed to model.fit.  This is needed \n",
      " |          for calculating gradients - this is mandatory if `log_gradients` is `True`.\n",
      " |      validate_data: (tuple) Same format (X,y) as passed to model.fit.  A set of data \n",
      " |          for wandb to visualize.  If this is set, every epoch, wandb will\n",
      " |          make a small number of predictions and save the results for later visualization.\n",
      " |      generator (generator): a generator that returns validation data for wandb to visualize.  This\n",
      " |          generator should return tuples (X,y).  Either validate_data or generator should\n",
      " |          be set for wandb to visualize specific data examples.\n",
      " |      validation_steps (int): if `validation_data` is a generator, how many\n",
      " |          steps to run the generator for the full validation set.\n",
      " |      labels (list): If you are visualizing your data with wandb this list of labels \n",
      " |          will convert numeric output to understandable string if you are building a\n",
      " |          multiclass classifier.  If you are making a binary classifier you can pass in\n",
      " |          a list of two labels [\"label for false\", \"label for true\"].  If validate_data\n",
      " |          and generator are both false, this won't do anything.\n",
      " |      predictions (int): the number of predictions to make for visualization each epoch, max \n",
      " |          is 100.\n",
      " |      input_type (string): type of the model input to help visualization. can be one of:\n",
      " |          (\"image\", \"images\", \"segmentation_mask\").\n",
      " |      output_type (string): type of the model output to help visualziation. can be one of:\n",
      " |          (\"image\", \"images\", \"segmentation_mask\").  \n",
      " |      log_evaluation (boolean): if True save a dataframe containing the full\n",
      " |          validation results at the end of training.\n",
      " |      class_colors ([float, float, float]): if the input or output is a segmentation mask, \n",
      " |          an array containing an rgb tuple (range 0-1) for each class.\n",
      " |      log_batch_frequency (integer): if None, callback will log every epoch.\n",
      " |          If set to integer, callback will log training metrics every log_batch_frequency \n",
      " |          batches.\n",
      " |      log_best_prefix (string): if None, no extra summary metrics will be saved.\n",
      " |          If set to a string, the monitored metric and epoch will be prepended with this value\n",
      " |          and stored as summary metrics.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      WandbCallback\n",
      " |      tensorflow.python.keras.callbacks.Callback\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, monitor='val_loss', verbose=0, mode='auto', save_weights_only=False, log_weights=False, log_gradients=False, save_model=True, training_data=None, validation_data=None, labels=[], data_type=None, predictions=36, generator=None, input_type=None, output_type=None, log_evaluation=False, validation_steps=None, class_colors=None, log_batch_frequency=None, log_best_prefix='best_', save_graph=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  on_batch_begin(self, batch, logs=None)\n",
      " |      A backwards compatibility alias for `on_train_batch_begin`.\n",
      " |  \n",
      " |  on_batch_end(self, batch, logs=None)\n",
      " |      A backwards compatibility alias for `on_train_batch_end`.\n",
      " |  \n",
      " |  on_epoch_end(self, epoch, logs={})\n",
      " |      Called at the end of an epoch.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run. This function should only\n",
      " |      be called during TRAIN mode.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          epoch: Integer, index of epoch.\n",
      " |          logs: Dict, metric results for this training epoch, and for the\n",
      " |            validation epoch if validation is performed. Validation result keys\n",
      " |            are prefixed with `val_`.\n",
      " |  \n",
      " |  on_predict_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a batch in `predict` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict, contains the return value of `model.predict_step`,\n",
      " |            it typically returns a dict with a key 'outputs' containing\n",
      " |            the model's outputs.\n",
      " |  \n",
      " |  on_predict_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a batch in `predict` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Aggregated metric results up until this batch.\n",
      " |  \n",
      " |  on_predict_begin(self, logs=None)\n",
      " |      Called at the beginning of prediction.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_predict_end(self, logs=None)\n",
      " |      Called at the end of prediction.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_test_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a batch in `evaluate` methods.\n",
      " |      \n",
      " |      Also called at the beginning of a validation batch in the `fit`\n",
      " |      methods, if validation data is provided.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict, contains the return value of `model.test_step`. Typically,\n",
      " |            the values of the `Model`'s metrics are returned.  Example:\n",
      " |            `{'loss': 0.2, 'accuracy': 0.7}`.\n",
      " |  \n",
      " |  on_test_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a batch in `evaluate` methods.\n",
      " |      \n",
      " |      Also called at the end of a validation batch in the `fit`\n",
      " |      methods, if validation data is provided.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Aggregated metric results up until this batch.\n",
      " |  \n",
      " |  on_test_begin(self, logs=None)\n",
      " |      Called at the beginning of evaluation or validation.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_test_end(self, logs=None)\n",
      " |      Called at the end of evaluation or validation.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently the output of the last call to\n",
      " |            `on_test_batch_end()` is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_train_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a training batch in `fit` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict, contains the return value of `model.train_step`. Typically,\n",
      " |            the values of the `Model`'s metrics are returned.  Example:\n",
      " |            `{'loss': 0.2, 'accuracy': 0.7}`.\n",
      " |  \n",
      " |  on_train_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a training batch in `fit` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Aggregated metric results up until this batch.\n",
      " |  \n",
      " |  on_train_begin(self, logs=None)\n",
      " |      Called at the beginning of training.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_train_end(self, logs=None)\n",
      " |      Called at the end of training.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently the output of the last call to `on_epoch_end()`\n",
      " |            is passed to this argument for this method but that may change in\n",
      " |            the future.\n",
      " |  \n",
      " |  set_model(self, model)\n",
      " |  \n",
      " |  set_params(self, params)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.keras.callbacks.Callback:\n",
      " |  \n",
      " |  on_epoch_begin(self, epoch, logs=None)\n",
      " |      Called at the start of an epoch.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run. This function should only\n",
      " |      be called during TRAIN mode.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          epoch: Integer, index of epoch.\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.keras.callbacks.Callback:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(WandbCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 100, 100, 18 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 100, 100, 3)  57          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 106, 106, 3)  0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 50, 50, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 50, 50, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 50, 50, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 52, 52, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 25, 25, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 25, 25, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 25, 25, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 25, 25, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 25, 25, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 25, 25, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 25, 25, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 25, 25, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 25, 25, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 25, 25, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 25, 25, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 25, 25, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 25, 25, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 25, 25, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 25, 25, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 25, 25, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 25, 25, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 25, 25, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 25, 25, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 25, 25, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 25, 25, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 25, 25, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 25, 25, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 25, 25, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 25, 25, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 25, 25, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 25, 25, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 13, 13, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 13, 13, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 13, 13, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 13, 13, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 13, 13, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 13, 13, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 13, 13, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 13, 13, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 13, 13, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 13, 13, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 13, 13, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 13, 13, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 13, 13, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 13, 13, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 13, 13, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 13, 13, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 13, 13, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 13, 13, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 13, 13, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 13, 13, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 13, 13, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 13, 13, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 13, 13, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 13, 13, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 13, 13, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 13, 13, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 13, 13, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 13, 13, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 13, 13, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 7, 7, 256)    131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 7, 7, 256)    0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 7, 7, 256)    0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 7, 7, 1024)   525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 7, 7, 1024)   0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 7, 7, 1024)   0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 7, 7, 256)    0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 7, 7, 256)    0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 7, 7, 1024)   0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 7, 7, 1024)   0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 7, 7, 256)    0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 7, 7, 256)    0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 7, 7, 1024)   0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 7, 7, 1024)   0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 7, 7, 256)    0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 7, 7, 256)    0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 7, 7, 1024)   0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 7, 7, 1024)   0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 7, 7, 256)    0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 7, 7, 256)    0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 7, 7, 1024)   0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 7, 7, 1024)   0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 7, 7, 256)    0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 7, 7, 256)    0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 7, 7, 1024)   0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 7, 7, 1024)   0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 4, 4, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 4, 4, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 4, 4, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 4, 4, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 4, 4, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 4, 4, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 4, 4, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 4, 4, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 4, 4, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 4, 4, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 4, 4, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 4, 4, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 4, 4, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 4, 4, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 4, 4, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 4, 4, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2048)         4196352     global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2048)         4196352     dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           20490       dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 32,000,963\n",
      "Trainable params: 31,947,843\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "random_id = 5555 #TODO\n",
    "checkpoint_file = 'checkpoint_{}.h5'.format(random_id)\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "  filepath= checkpoint_file,\n",
    "  format='h5',\n",
    "  verbose=1,\n",
    "  save_weights_only=True,\n",
    "  monitor='val_loss',\n",
    "  mode='min',\n",
    "  save_best_only=True)\n",
    "\n",
    "reducelronplateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "  monitor='val_loss', factor=0.1, patience=10, verbose=1,\n",
    "  mode='min', min_lr=1e-10)\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',mode='min', patience=20, verbose=1)\n",
    "\n",
    "#labels = [\"Habitation\", \"ISL\", \"Industrial_agriculture\", \"Mining\", \"Rainforest\",\n",
    "#          \"River\", \"Roads\", \"Savannah\", \"Shifting_cultivation\", \"Water\"] # Cut out mining b/c not in training data\n",
    "\n",
    "callbacks_list = [model_checkpoint_callback, reducelronplateau, early_stop]\n",
    "                  #WandbCallback(monitor='accuracy', data_type=\"image\", labels=labels)]\n",
    "\n",
    "model = define_model(10, (100,100,18))\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                          optimizer=keras.optimizers.Adam(),\n",
    "                          metrics=[tf.metrics.BinaryAccuracy(name='accuracy')]) #TODO add callbacks to save checkpoints and maybe lr reducer, earlystop,etc \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 2.7809829059829063,\n",
       " 1: 0.6314895681707908,\n",
       " 2: 1.1188480550182678,\n",
       " 3: 1,\n",
       " 4: 0.6000461041954819,\n",
       " 5: 1.9281481481481484,\n",
       " 6: 1.6574339382362304,\n",
       " 7: 1.2629791363415817,\n",
       " 8: 0.8022807828633072,\n",
       " 9: 0.7483110536150641}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = gen.class_weight\n",
    "\n",
    "corrected_weights = {}\n",
    "\n",
    "for i in range(10):\n",
    "    if i == 3:\n",
    "        corrected_weights[i] = 1\n",
    "    else:\n",
    "        corrected_weights[i] = weights[i]\n",
    "        \n",
    "corrected_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-ae583a577cb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                     class_weight=corrected_weights)\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\envs\\canopy_test_models\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\canopy_test_models\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1061\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[0;32m   1064\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m       \u001b[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\canopy_test_models\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_adapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m       \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_make_class_weight_map_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inferred_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_infer_steps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\canopy_test_models\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[0;32m   1693\u001b[0m     \"\"\"\n\u001b[0;32m   1694\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1695\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1696\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1697\u001b[0m       return ParallelMapDataset(\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\canopy_test_models\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[0;32m   4043\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4044\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4045\u001b[1;33m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[0;32m   4046\u001b[0m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[0;32m   4047\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\canopy_test_models\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   3369\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3370\u001b[0m         \u001b[1;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3371\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3372\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3373\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\canopy_test_models\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2937\u001b[0m     \"\"\"\n\u001b[0;32m   2938\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[1;32m-> 2939\u001b[1;33m         *args, **kwargs)\n\u001b[0m\u001b[0;32m   2940\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2941\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\canopy_test_models\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2904\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2905\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2906\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2907\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2908\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\canopy_test_models\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\canopy_test_models\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3075\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\canopy_test_models\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\canopy_test_models\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3362\u001b[0m           attributes=defun_kwargs)\n\u001b[0;32m   3363\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=missing-docstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3364\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3365\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3366\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\canopy_test_models\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3297\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3299\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3300\u001b[0m       \u001b[1;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3301\u001b[0m       \u001b[1;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\canopy_test_models\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    253\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\canopy_test_models\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_whitelisted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m   \u001b[1;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\canopy_test_models\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\canopy_test_models\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_class_weights_map_fn\u001b[1;34m(*data)\u001b[0m\n\u001b[0;32m   1312\u001b[0m           \"`class_weight` is only supported for Models with a single output.\")\n\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1314\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1315\u001b[0m       raise ValueError(\"`class_weight` not supported for \"\n\u001b[0;32m   1316\u001b[0m                        \"3+ dimensional targets.\")\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "history = model.fit(gen.training_dataset, validation_data=gen.validation_dataset, \n",
    "                    epochs=epochs, \n",
    "                    callbacks=callbacks_list,\n",
    "                    class_weight=corrected_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'rank'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-d0989e84fcd3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrected_weights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'rank'"
     ]
    }
   ],
   "source": [
    "np.array(list(corrected_weights.values())).shape.rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 100, 100, 18 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 100, 100, 3)  57          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 106, 106, 3)  0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 50, 50, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 50, 50, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 50, 50, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 52, 52, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 25, 25, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 25, 25, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 25, 25, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 25, 25, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 25, 25, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 25, 25, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 25, 25, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 25, 25, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 25, 25, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 25, 25, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 25, 25, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 25, 25, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 25, 25, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 25, 25, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 25, 25, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 25, 25, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 25, 25, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 25, 25, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 25, 25, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 25, 25, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 25, 25, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 25, 25, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 25, 25, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 25, 25, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 25, 25, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 25, 25, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 25, 25, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 13, 13, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 13, 13, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 13, 13, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 13, 13, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 13, 13, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 13, 13, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 13, 13, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 13, 13, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 13, 13, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 13, 13, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 13, 13, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 13, 13, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 13, 13, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 13, 13, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 13, 13, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 13, 13, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 13, 13, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 13, 13, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 13, 13, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 13, 13, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 13, 13, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 13, 13, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 13, 13, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 13, 13, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 13, 13, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 13, 13, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 13, 13, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 13, 13, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 13, 13, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 7, 7, 256)    131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 7, 7, 256)    0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 7, 7, 256)    0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 7, 7, 1024)   525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 7, 7, 1024)   0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 7, 7, 1024)   0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 7, 7, 256)    0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 7, 7, 256)    0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 7, 7, 1024)   0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 7, 7, 1024)   0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 7, 7, 256)    0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 7, 7, 256)    0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 7, 7, 1024)   0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 7, 7, 1024)   0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 7, 7, 256)    0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 7, 7, 256)    0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 7, 7, 1024)   0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 7, 7, 1024)   0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 7, 7, 256)    0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 7, 7, 256)    0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 7, 7, 1024)   0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 7, 7, 1024)   0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 7, 7, 256)    0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 7, 7, 256)    0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 7, 7, 1024)   0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 7, 7, 1024)   0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 4, 4, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 4, 4, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 4, 4, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 4, 4, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 4, 4, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 4, 4, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 4, 4, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 4, 4, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 4, 4, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 4, 4, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 4, 4, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 4, 4, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 4, 4, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 4, 4, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 4, 4, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 4, 4, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2048)         4196352     global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2048)         4196352     dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           20490       dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 32,000,963\n",
      "Trainable params: 31,947,843\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = define_model(10, (100,100,18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x2a3d6aa8ac8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d6ac6fc8>,\n",
       " <tensorflow.python.keras.layers.convolutional.ZeroPadding2D at 0x2a3db44ce88>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3db435f48>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3db44fb08>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3db45f288>,\n",
       " <tensorflow.python.keras.layers.convolutional.ZeroPadding2D at 0x2a3db454308>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x2a3d84bc8c8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d84c0d48>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8502e88>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d8505248>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d8508748>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8541b08>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d8548c88>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d84bc488>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d854f0c8>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d84c4f88>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8583848>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x2a3d858fcc8>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d858f3c8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d85c7948>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d85ca748>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d85c54c8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d85c5988>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d860ee48>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d864c4c8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d860f3c8>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d86570c8>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x2a3d8650188>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d86505c8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d8695b48>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8697588>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d86a1508>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d86a31c8>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d86df2c8>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d86dca88>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d86dc408>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d87202c8>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x2a3d8729b48>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d8729208>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d876dec8>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d87a6f08>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d87b7908>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d87b3448>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d87e9d48>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d8920e08>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d87597c8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d8926248>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8763188>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8960308>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x2a3d8968b08>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d89681c8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d89a16c8>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d89a8088>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d89a2888>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d89a2308>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d89e6f08>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d89e7e48>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d89e72c8>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8a2d048>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x2a3d8a35708>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d8a359c8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d8a6de08>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8a73608>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d8a76ec8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d8a765c8>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8ab9608>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d8ac22c8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d8ac21c8>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8afbb08>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x2a3d8b03dc8>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d8aff5c8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d8b3db48>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8b41588>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d8b4a548>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d8b46c88>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8b850c8>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d8b8dac8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d8b8d988>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8bcc388>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x2a3d8bcdec8>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d8bcd548>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d8c183c8>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8c59e48>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d8c527c8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d8c528c8>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8c95a08>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d8c9d948>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d8bffa48>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d8c99048>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8c15608>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8cdc2c8>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x2a3d8ce4288>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d8ce4408>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d8d1b688>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8d23048>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d8d1e848>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d8d1e308>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8d62ec8>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d8d66dc8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d8d668c8>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8da40c8>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x2a3d8db06c8>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d8db0988>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d8ee9f08>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8eef5c8>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d8eeb0c8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d8eeb588>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8f322c8>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d8f3b988>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d8f3b408>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8f76ac8>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x2a3d8f7cd88>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d8f7b0c8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d8fb7b08>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8fbb488>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d8fc2508>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d8fc5c48>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8ffd048>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d9006c48>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d9006588>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d9045248>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x2a3d9048f08>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d90485c8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d907cb48>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d908f5c8>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d90928c8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d9092508>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d90ccbc8>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d90d2c48>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d90d8088>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d9112248>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x2a3d911db48>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d911d208>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d9a83708>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d9a8a048>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d9a868c8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d9a86388>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d9e29f08>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d9e2cb48>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d9e2c5c8>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d9e6c048>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x2a3d9e77708>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d9e779c8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d9ebc748>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d9ef6fc8>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3d9f08b08>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d9f03448>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3db03d2c8>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3db03f388>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3d9eb2e08>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3db03f148>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d9eb6608>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3db07ca48>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x2a3db085e48>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3db08c088>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3db0c2788>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3db0c3948>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3db0cb708>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3db0cfcc8>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3db104648>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3db110c48>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3db110f88>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3db14cf08>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x2a3db153ec8>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3db1535c8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3dbc99d48>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3dbc9e688>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3dbc9b088>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3dbc9b908>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3dc4a22c8>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3dc4ab348>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x2a3dc4ab248>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3dc4e1b88>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x2a3dc4e9e48>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x2a3dc4ef188>,\n",
       " <tensorflow.python.keras.layers.pooling.GlobalAveragePooling2D at 0x2a3dc55df48>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x2a3dc55de08>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x2a3dc560648>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x2a3dc546ac8>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3db44fb08>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on BatchNormalization in module tensorflow.python.keras.layers.normalization_v2 object:\n",
      "\n",
      "class BatchNormalization(tensorflow.python.keras.layers.normalization.BatchNormalizationBase)\n",
      " |  BatchNormalization(*args, **kwargs)\n",
      " |  \n",
      " |  Normalize and scale inputs or activations.\n",
      " |  \n",
      " |  Normalize the activations of the previous layer at each batch,\n",
      " |  i.e. applies a transformation that maintains the mean activation\n",
      " |  close to 0 and the activation standard deviation close to 1.\n",
      " |  \n",
      " |  Batch normalization differs from other layers in several key aspects:\n",
      " |  \n",
      " |  1) Adding BatchNormalization with `training=True` to a model causes the\n",
      " |  result of one example to depend on the contents of all other examples in a\n",
      " |  minibatch. Be careful when padding batches or masking examples, as these can\n",
      " |  change the minibatch statistics and affect other examples.\n",
      " |  \n",
      " |  2) Updates to the weights (moving statistics) are based on the forward pass\n",
      " |  of a model rather than the result of gradient computations.\n",
      " |  \n",
      " |  3) When performing inference using a model containing batch normalization, it\n",
      " |  is generally (though not always) desirable to use accumulated statistics\n",
      " |  rather than mini-batch statistics. This is accomplished by passing\n",
      " |  `training=False` when calling the model, or using `model.predict`.\n",
      " |  \n",
      " |  Arguments:\n",
      " |    axis: Integer, the axis that should be normalized (typically the features\n",
      " |      axis). For instance, after a `Conv2D` layer with\n",
      " |      `data_format=\"channels_first\"`, set `axis=1` in `BatchNormalization`.\n",
      " |    momentum: Momentum for the moving average.\n",
      " |    epsilon: Small float added to variance to avoid dividing by zero.\n",
      " |    center: If True, add offset of `beta` to normalized tensor. If False, `beta`\n",
      " |      is ignored.\n",
      " |    scale: If True, multiply by `gamma`. If False, `gamma` is not used. When the\n",
      " |      next layer is linear (also e.g. `nn.relu`), this can be disabled since the\n",
      " |      scaling will be done by the next layer.\n",
      " |    beta_initializer: Initializer for the beta weight.\n",
      " |    gamma_initializer: Initializer for the gamma weight.\n",
      " |    moving_mean_initializer: Initializer for the moving mean.\n",
      " |    moving_variance_initializer: Initializer for the moving variance.\n",
      " |    beta_regularizer: Optional regularizer for the beta weight.\n",
      " |    gamma_regularizer: Optional regularizer for the gamma weight.\n",
      " |    beta_constraint: Optional constraint for the beta weight.\n",
      " |    gamma_constraint: Optional constraint for the gamma weight.\n",
      " |    renorm: Whether to use [Batch Renormalization](\n",
      " |      https://arxiv.org/abs/1702.03275). This adds extra variables during\n",
      " |        training. The inference is the same for either value of this parameter.\n",
      " |    renorm_clipping: A dictionary that may map keys 'rmax', 'rmin', 'dmax' to\n",
      " |      scalar `Tensors` used to clip the renorm correction. The correction `(r,\n",
      " |      d)` is used as `corrected_value = normalized_value * r + d`, with `r`\n",
      " |      clipped to [rmin, rmax], and `d` to [-dmax, dmax]. Missing rmax, rmin,\n",
      " |      dmax are set to inf, 0, inf, respectively.\n",
      " |    renorm_momentum: Momentum used to update the moving means and standard\n",
      " |      deviations with renorm. Unlike `momentum`, this affects training and\n",
      " |      should be neither too small (which would add noise) nor too large (which\n",
      " |      would give stale estimates). Note that `momentum` is still applied to get\n",
      " |      the means and variances for inference.\n",
      " |    fused: if `True`, use a faster, fused implementation, or raise a ValueError\n",
      " |      if the fused implementation cannot be used. If `None`, use the faster\n",
      " |      implementation if possible. If False, do not used the fused\n",
      " |      implementation.\n",
      " |    trainable: Boolean, if `True` the variables will be marked as trainable.\n",
      " |    virtual_batch_size: An `int`. By default, `virtual_batch_size` is `None`,\n",
      " |      which means batch normalization is performed across the whole batch. When\n",
      " |      `virtual_batch_size` is not `None`, instead perform \"Ghost Batch\n",
      " |      Normalization\", which creates virtual sub-batches which are each\n",
      " |      normalized separately (with shared gamma, beta, and moving statistics).\n",
      " |      Must divide the actual batch size during execution.\n",
      " |    adjustment: A function taking the `Tensor` containing the (dynamic) shape of\n",
      " |      the input tensor and returning a pair (scale, bias) to apply to the\n",
      " |      normalized values (before gamma and beta), only during training. For\n",
      " |      example, if axis==-1,\n",
      " |        `adjustment = lambda shape: (\n",
      " |          tf.random.uniform(shape[-1:], 0.93, 1.07),\n",
      " |          tf.random.uniform(shape[-1:], -0.1, 0.1))` will scale the normalized\n",
      " |            value by up to 7% up or down, then shift the result by up to 0.1\n",
      " |            (with independent scaling and bias for each feature but shared\n",
      " |            across all examples), and finally apply gamma and/or beta. If\n",
      " |            `None`, no adjustment is applied. Cannot be specified if\n",
      " |            virtual_batch_size is specified.\n",
      " |  Call arguments:\n",
      " |    inputs: Input tensor (of any rank).\n",
      " |    training: Python boolean indicating whether the layer should behave in\n",
      " |      training mode or in inference mode.\n",
      " |      - `training=True`: The layer will normalize its inputs using the mean and\n",
      " |        variance of the current batch of inputs.\n",
      " |      - `training=False`: The layer will normalize its inputs using the mean and\n",
      " |        variance of its moving statistics, learned during training.\n",
      " |  Input shape: Arbitrary. Use the keyword argument `input_shape` (tuple of\n",
      " |    integers, does not include the samples axis) when using this layer as the\n",
      " |    first layer in a model.\n",
      " |  Output shape: Same shape as input.  \n",
      " |  **About setting `layer.trainable = False` on a `BatchNormalization layer:**\n",
      " |  \n",
      " |  The meaning of setting `layer.trainable = False` is to freeze the layer,\n",
      " |  i.e. its internal state will not change during training:\n",
      " |  its trainable weights will not be updated\n",
      " |  during `fit()` or `train_on_batch()`, and its state updates will not be run.\n",
      " |  \n",
      " |  Usually, this does not necessarily mean that the layer is run in inference\n",
      " |  mode (which is normally controlled by the `training` argument that can\n",
      " |  be passed when calling a layer). \"Frozen state\" and \"inference mode\"\n",
      " |  are two separate concepts.\n",
      " |  \n",
      " |  However, in the case of the `BatchNormalization` layer, **setting\n",
      " |  `trainable = False` on the layer means that the layer will be\n",
      " |  subsequently run in inference mode** (meaning that it will use\n",
      " |  the moving mean and the moving variance to normalize the current batch,\n",
      " |  rather than using the mean and variance of the current batch).\n",
      " |  \n",
      " |  This behavior has been introduced in TensorFlow 2.0, in order\n",
      " |  to enable `layer.trainable = False` to produce the most commonly\n",
      " |  expected behavior in the convnet fine-tuning use case.\n",
      " |  \n",
      " |  Note that:\n",
      " |    - This behavior only occurs as of TensorFlow 2.0. In 1.*,\n",
      " |      setting `layer.trainable = False` would freeze the layer but would\n",
      " |      not switch it to inference mode.\n",
      " |    - Setting `trainable` on an model containing other layers will\n",
      " |      recursively set the `trainable` value of all inner layers.\n",
      " |    - If the value of the `trainable`\n",
      " |      attribute is changed after calling `compile()` on a model,\n",
      " |      the new value doesn't take effect for this model\n",
      " |      until `compile()` is called again.\n",
      " |      \n",
      " |  Normalization equations: Consider the intermediate activations \\(x\\) of a\n",
      " |    mini-batch of size\n",
      " |    \\\\(m\\\\):  We can compute the mean and variance of the batch  \\\\({\\mu_B} =\n",
      " |      \\frac{1}{m} \\sum_{i=1}^{m} {x_i}\\\\)  \\\\({\\sigma_B^2} = \\frac{1}{m}\n",
      " |      \\sum_{i=1}^{m} ({x_i} - {\\mu_B})^2\\\\)  and then compute a normalized\n",
      " |      \\\\(x\\\\), including a small factor \\\\({\\epsilon}\\\\) for numerical\n",
      " |      stability.  \\\\(\\hat{x_i} = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 +\n",
      " |      \\epsilon}}\\\\)  And finally \\\\(\\hat{x}\\) is linearly transformed by\n",
      " |      \\({\\gamma}\\\\)\n",
      " |    and \\\\({\\beta}\\\\), which are learned parameters:  \\\\({y_i} = {\\gamma *\n",
      " |      \\hat{x_i} + \\beta}\\\\)\n",
      " |  Reference:\n",
      " |    - [Ioffe and Szegedy, 2015](https://arxiv.org/abs/1502.03167).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BatchNormalization\n",
      " |      tensorflow.python.keras.layers.normalization.BatchNormalizationBase\n",
      " |      tensorflow.python.keras.engine.base_layer.Layer\n",
      " |      tensorflow.python.module.module.Module\n",
      " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      tensorflow.python.keras.utils.version_utils.LayerVersionSelector\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods inherited from tensorflow.python.keras.layers.normalization.BatchNormalizationBase:\n",
      " |  \n",
      " |  __init__(self, axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None, renorm=False, renorm_clipping=None, renorm_momentum=0.99, fused=None, trainable=True, virtual_batch_size=None, adjustment=None, name=None, **kwargs)\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the variables of the layer (optional, for subclass implementers).\n",
      " |      \n",
      " |      This is a method that implementers of subclasses of `Layer` or `Model`\n",
      " |      can override if they need a state-creation step in-between\n",
      " |      layer instantiation and layer call.\n",
      " |      \n",
      " |      This is typically used to create the weights of `Layer` subclasses.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        input_shape: Instance of `TensorShape`, or list of instances of\n",
      " |          `TensorShape` if the layer expects a list of inputs\n",
      " |          (one instance per input).\n",
      " |  \n",
      " |  call(self, inputs, training=None)\n",
      " |      This is where the layer's logic lives.\n",
      " |      \n",
      " |      Note here that `call()` method in `tf.keras` is little bit different\n",
      " |      from `keras` API. In `keras` API, you can pass support masking for\n",
      " |      layers as additional arguments. Whereas `tf.keras` has `compute_mask()`\n",
      " |      method to support masking.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Input tensor, or list/tuple of input tensors.\n",
      " |          **kwargs: Additional keyword arguments. Currently unused.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor or list/tuple of tensors.\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      If the layer has not been built, this method will call `build` on the\n",
      " |      layer. This assumes that the layer will later be used with inputs that\n",
      " |      match the input shape provided here.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.keras.layers.normalization.BatchNormalizationBase:\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Wraps `call`, applying pre- and post-processing steps.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        *args: Positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: Keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |      \n",
      " |      Note:\n",
      " |        - The following optional keyword arguments are reserved for specific uses:\n",
      " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          * `mask`: Boolean input mask.\n",
      " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
      " |          layers do), its default value will be set to the mask generated\n",
      " |          for `inputs` by the previous layer (if `input` did come from\n",
      " |          a layer that generated a corresponding mask, i.e. if it came from\n",
      " |          a Keras layer with masking support.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
      " |        RuntimeError: if `super().__init__()` was not called in the constructor.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_loss(self, losses, **kwargs)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be dependent\n",
      " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
      " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
      " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This method can be used inside a subclassed layer or model's `call`\n",
      " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(tf.keras.layers.Layer):\n",
      " |        def call(self, inputs):\n",
      " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any loss Tensors passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      losses become part of the model's topology and are tracked in `get_config`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Activity regularization.\n",
      " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      ```\n",
      " |      \n",
      " |      If this is not the case for your loss (if, for example, your loss references\n",
      " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
      " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
      " |      topology since they can't be serialized.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      d = tf.keras.layers.Dense(10)\n",
      " |      x = d(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Weight regularization.\n",
      " |      model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      ```\n",
      " |      \n",
      " |      Arguments:\n",
      " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
      " |          may also be zero-argument callables which create a loss tensor.\n",
      " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
      " |          Accepted values:\n",
      " |            inputs - Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_metric(self, value, name=None, **kwargs)\n",
      " |      Adds metric tensor to the layer.\n",
      " |      \n",
      " |      This method can be used inside the `call()` method of a subclassed layer\n",
      " |      or model.\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyMetricLayer(tf.keras.layers.Layer):\n",
      " |        def __init__(self):\n",
      " |          super(MyMetricLayer, self).__init__(name='my_metric_layer')\n",
      " |          self.mean = metrics_module.Mean(name='metric_1')\n",
      " |      \n",
      " |        def call(self, inputs):\n",
      " |          self.add_metric(self.mean(x))\n",
      " |          self.add_metric(math_ops.reduce_sum(x), name='metric_2')\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any tensor passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      metrics become part of the model's topology and are tracked when you\n",
      " |      save the model via `save()`.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(math_ops.reduce_sum(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Note: Calling `add_metric()` with the result of a metric object on a\n",
      " |      Functional Model, as shown in the example below, is not supported. This is\n",
      " |      because we cannot trace the metric result tensor back to the model's inputs.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        value: Metric tensor.\n",
      " |        name: String metric name.\n",
      " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
      " |          Accepted values:\n",
      " |          `aggregation` - When the `value` tensor provided is not the result of\n",
      " |          calling a `keras.Metric` instance, it will be aggregated by default\n",
      " |          using a `keras.Metric.Mean`.\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Add update op(s), potentially dependent on layer inputs. (deprecated arguments)\n",
      " |      \n",
      " |      Warning: SOME ARGUMENTS ARE DEPRECATED: `(inputs)`. They will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      `inputs` is now automatically inferred\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and variance\n",
      " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
      " |      when calling a layer. Hence, when reusing the same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This call is ignored when eager execution is enabled (in that case, variable\n",
      " |      updates are run on the fly and thus do not need to be tracked for later\n",
      " |      execution).\n",
      " |      \n",
      " |      Arguments:\n",
      " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
      " |          that returns an update op. A zero-arg callable should be passed in\n",
      " |          order to disable running the updates by setting `trainable=False`\n",
      " |          on this Layer, when executing in Eager mode.\n",
      " |        inputs: Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_variable(self, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! Alias for `add_weight`. (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.add_weight` method instead.\n",
      " |  \n",
      " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, partitioner=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>, **kwargs)\n",
      " |      Adds a new variable to the layer.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        name: Variable name.\n",
      " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype` or `float32`.\n",
      " |        initializer: Initializer instance (callable).\n",
      " |        regularizer: Regularizer instance (callable).\n",
      " |        trainable: Boolean, whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
      " |          Note that `trainable` cannot be `True` if `synchronization`\n",
      " |          is set to `ON_READ`.\n",
      " |        constraint: Constraint instance (callable).\n",
      " |        partitioner: Partitioner to be passed to the `Trackable` API.\n",
      " |        use_resource: Whether to use `ResourceVariable`.\n",
      " |        synchronization: Indicates when a distributed a variable will be\n",
      " |          aggregated. Accepted values are constants defined in the class\n",
      " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
      " |          `AUTO` and the current `DistributionStrategy` chooses\n",
      " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
      " |          `trainable` must not be set to `True`.\n",
      " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
      " |          Accepted values are constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
      " |          `collections`, `experimental_autocast` and `caching_device`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The created variable. Usually either a `Variable` or `ResourceVariable`\n",
      " |        instance. If `partitioner` is not `None`, a `PartitionedVariable`\n",
      " |        instance is returned.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called with partitioned variable regularization and\n",
      " |          eager execution is enabled.\n",
      " |        ValueError: When giving unsupported dtype and no initializer or when\n",
      " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
      " |  \n",
      " |  apply(self, inputs, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.__call__` method instead.\n",
      " |      \n",
      " |      This is an alias of `self.__call__`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask=None)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_signature(self, input_signature)\n",
      " |      Compute the output tensor signature of the layer based on the inputs.\n",
      " |      \n",
      " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
      " |      and dtype information for a tensor. This method allows layers to provide\n",
      " |      output dtype information if it is different from the input dtype.\n",
      " |      For any layer that doesn't implement this function,\n",
      " |      the framework will fall back to use `compute_output_shape`, and will\n",
      " |      assume that the output dtype matches the input dtype.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
      " |          objects, describing a candidate input for the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
      " |          how the layer would transform the provided input.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |      Deprecated, do NOT use! (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.losses` instead.\n",
      " |      \n",
      " |      Retrieves losses relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of loss tensors of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |      Deprecated, do NOT use! (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.updates` instead.\n",
      " |      \n",
      " |      Retrieves updates relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of update ops of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      returns both trainable and non-trainable weight values associated with this\n",
      " |      layer as a list of Numpy arrays, which can in turn be used to load state\n",
      " |      into similarly parameterized layers.\n",
      " |      \n",
      " |      For example, a Dense layer returns a list of two values-- per-output\n",
      " |      weights and the bias value. These can be used to set the weights of another\n",
      " |      Dense layer:\n",
      " |      \n",
      " |      >>> a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b.set_weights(a.get_weights())\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Returns:\n",
      " |          Weights values as a list of numpy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from Numpy arrays.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      sets the weight values from numpy arrays. The weight values should be\n",
      " |      passed in the order they are created by the layer. Note that the layer's\n",
      " |      weights must be instantiated before calling this function by calling\n",
      " |      the layer.\n",
      " |      \n",
      " |      For example, a Dense layer returns a list of two values-- per-output\n",
      " |      weights and the bias value. These can be used to set the weights of another\n",
      " |      Dense layer:\n",
      " |      \n",
      " |      >>> a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b.set_weights(a.get_weights())\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Arguments:\n",
      " |          weights: a list of Numpy arrays. The number\n",
      " |              of arrays and their shape must match\n",
      " |              number of the dimensions of the weights\n",
      " |              of the layer (i.e. it should match the\n",
      " |              output of `get_weights`).\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the provided weights list does not match the\n",
      " |              layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  dtype\n",
      " |      Dtype used by the weights of the layer, set in the constructor.\n",
      " |  \n",
      " |  dynamic\n",
      " |      Whether the layer is dynamic (eager-only); set in the constructor.\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  input_spec\n",
      " |      `InputSpec` instance(s) describing the input format for this layer.\n",
      " |      \n",
      " |      When you create a layer subclass, you can set `self.input_spec` to enable\n",
      " |      the layer to run input compatibility checks when it is called.\n",
      " |      Consider a `Conv2D` layer: it can only be called on a single input tensor\n",
      " |      of rank 4. As such, you can set, in `__init__()`:\n",
      " |      \n",
      " |      ```python\n",
      " |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
      " |      ```\n",
      " |      \n",
      " |      Now, if you try to call the layer on an input that isn't rank 4\n",
      " |      (for instance, an input of shape `(2,)`, it will raise a nicely-formatted\n",
      " |      error:\n",
      " |      \n",
      " |      ```\n",
      " |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
      " |      expected ndim=4, found ndim=1. Full shape received: [2]\n",
      " |      ```\n",
      " |      \n",
      " |      Input checks that can be specified via `input_spec` include:\n",
      " |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
      " |      - Shape\n",
      " |      - Rank (ndim)\n",
      " |      - Dtype\n",
      " |      \n",
      " |      For more information, see `tf.keras.layers.InputSpec`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
      " |  \n",
      " |  losses\n",
      " |      List of losses added using the `add_loss()` API.\n",
      " |      \n",
      " |      Variable regularization tensors are created when this property is accessed,\n",
      " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
      " |      propagate gradients back to the corresponding variables.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      >>> class MyLayer(tf.keras.layers.Layer):\n",
      " |      ...   def call(self, inputs):\n",
      " |      ...     self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |      ...     return inputs\n",
      " |      >>> l = MyLayer()\n",
      " |      >>> l(np.ones((10, 1)))\n",
      " |      >>> l.losses\n",
      " |      [1.0]\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Activity regularization.\n",
      " |      >>> model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      >>> model.losses\n",
      " |      [<tf.Tensor 'Abs:0' shape=() dtype=float32>]\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones')\n",
      " |      >>> x = d(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Weight regularization.\n",
      " |      >>> model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      >>> model.losses\n",
      " |      [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  metrics\n",
      " |      List of metrics added using the `add_metric()` API.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      >>> input = tf.keras.layers.Input(shape=(3,))\n",
      " |      >>> d = tf.keras.layers.Dense(2)\n",
      " |      >>> output = d(input)\n",
      " |      >>> d.add_metric(tf.reduce_max(output), name='max')\n",
      " |      >>> d.add_metric(tf.reduce_min(output), name='min')\n",
      " |      >>> [m.name for m in d.metrics]\n",
      " |      ['max', 'min']\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  name\n",
      " |      Name of the layer (string), set in the constructor.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |      List of all non-trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Non-trainable weights are *not* updated during training. They are expected\n",
      " |      to be updated manually in `call()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of non-trainable variables.\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  stateful\n",
      " |  \n",
      " |  supports_masking\n",
      " |      Whether this layer supports computing a mask using `compute_mask`.\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      Sequence of trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  trainable_weights\n",
      " |      List of all trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Trainable weights are updated via gradient descent during training.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of trainable variables.\n",
      " |  \n",
      " |  updates\n",
      " |      DEPRECATED FUNCTION\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Alias of `self.weights`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  with_name_scope(method) from builtins.type\n",
      " |      Decorator to automatically enter the module name scope.\n",
      " |      \n",
      " |      >>> class MyModule(tf.Module):\n",
      " |      ...   @tf.Module.with_name_scope\n",
      " |      ...   def __call__(self, x):\n",
      " |      ...     if not hasattr(self, 'w'):\n",
      " |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
      " |      ...     return tf.matmul(x, self.w)\n",
      " |      \n",
      " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
      " |      names included the module name:\n",
      " |      \n",
      " |      >>> mod = MyModule()\n",
      " |      >>> mod(tf.ones([1, 2]))\n",
      " |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
      " |      >>> mod.w\n",
      " |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
      " |      numpy=..., dtype=float32)>\n",
      " |      \n",
      " |      Args:\n",
      " |        method: The method to wrap.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The original method wrapped such that it enters the module's name scope.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  name_scope\n",
      " |      Returns a `tf.name_scope` instance for this class.\n",
      " |  \n",
      " |  submodules\n",
      " |      Sequence of all sub-modules.\n",
      " |      \n",
      " |      Submodules are modules which are properties of this module, or found as\n",
      " |      properties of modules which are properties of this module (and so on).\n",
      " |      \n",
      " |      >>> a = tf.Module()\n",
      " |      >>> b = tf.Module()\n",
      " |      >>> c = tf.Module()\n",
      " |      >>> a.b = b\n",
      " |      >>> b.c = c\n",
      " |      >>> list(a.submodules) == [b, c]\n",
      " |      True\n",
      " |      >>> list(b.submodules) == [c]\n",
      " |      True\n",
      " |      >>> list(c.submodules) == []\n",
      " |      True\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of all submodules.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from tensorflow.python.keras.utils.version_utils.LayerVersionSelector:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bn = model.layers[4]\n",
    "\n",
    "help(bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conv1_bn/'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn.name_scope()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conv1_bn'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conv2_block1_1_bn'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[9].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conv1_pad'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[2].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bns = []\n",
    "\n",
    "for layer in model.layers:\n",
    "    if 'bn' in layer.name:\n",
    "        bns.append(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3db44fb08>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8502e88>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8541b08>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d84c4f88>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8583848>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d85ca748>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d860ee48>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d86570c8>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8697588>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d86df2c8>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d87202c8>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d87a6f08>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d87e9d48>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8763188>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8960308>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d89a8088>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d89e6f08>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8a2d048>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8a73608>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8ab9608>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8afbb08>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8b41588>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8b850c8>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8bcc388>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8c59e48>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8c95a08>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8c15608>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8cdc2c8>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8d23048>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8d62ec8>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8da40c8>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8eef5c8>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8f322c8>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8f76ac8>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8fbb488>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d8ffd048>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d9045248>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d908f5c8>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d90ccbc8>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d9112248>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d9a8a048>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d9e29f08>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d9e6c048>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d9ef6fc8>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3db03d2c8>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3d9eb6608>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3db07ca48>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3db0c3948>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3db104648>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3db14cf08>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3dbc9e688>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3dc4a22c8>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2a3dc4e1b88>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(numclasses, input_shape, freeze_bns=True):\n",
    "    # parameters for CNN\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "\n",
    "    # introduce a additional layer to get from bands to 3 input channels\n",
    "    input_tensor = Conv2D(3, (1, 1))(input_tensor)\n",
    "\n",
    "    base_model_resnet50 = keras.applications.ResNet50(include_top=False,\n",
    "                              weights='imagenet',\n",
    "                              input_shape=(100, 100, 3))\n",
    "    base_model = keras.applications.ResNet50(include_top=False,\n",
    "                     weights=None,\n",
    "                     input_tensor=input_tensor)\n",
    "\n",
    "    for i, layer in enumerate(base_model_resnet50.layers):\n",
    "        # we must skip input layer, which has no weights\n",
    "        if i == 0:\n",
    "            continue\n",
    "        base_model.layers[i+1].set_weights(layer.get_weights())\n",
    "        \n",
    "        if freeze_bns:\n",
    "            if 'bn' in layer.name:\n",
    "                base_model.layers[i+1].trainable = False\n",
    "\n",
    "    # add a global spatial average pooling layer\n",
    "    top_model = base_model.output\n",
    "    top_model = GlobalAveragePooling2D()(top_model)\n",
    "\n",
    "    # let's add a fully-connected layer\n",
    "    top_model = Dense(2048, activation='relu')(top_model)\n",
    "    top_model = Dense(2048, activation='relu')(top_model)\n",
    "    # and a logistic layer\n",
    "    predictions = Dense(numclasses, activation='softmax')(top_model)\n",
    "\n",
    "    # this is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 100, 100, 18 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 100, 100, 3)  57          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 106, 106, 3)  0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 50, 50, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 50, 50, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 50, 50, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 52, 52, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 25, 25, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 25, 25, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 25, 25, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 25, 25, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 25, 25, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 25, 25, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 25, 25, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 25, 25, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 25, 25, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 25, 25, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 25, 25, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 25, 25, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 25, 25, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 25, 25, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 25, 25, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 25, 25, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 25, 25, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 25, 25, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 25, 25, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 25, 25, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 25, 25, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 25, 25, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 25, 25, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 25, 25, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 25, 25, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 25, 25, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 25, 25, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 13, 13, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 13, 13, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 13, 13, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 13, 13, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 13, 13, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 13, 13, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 13, 13, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 13, 13, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 13, 13, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 13, 13, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 13, 13, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 13, 13, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 13, 13, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 13, 13, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 13, 13, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 13, 13, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 13, 13, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 13, 13, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 13, 13, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 13, 13, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 13, 13, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 13, 13, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 13, 13, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 13, 13, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 13, 13, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 13, 13, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 13, 13, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 13, 13, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 13, 13, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 7, 7, 256)    131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 7, 7, 256)    0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 7, 7, 256)    0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 7, 7, 1024)   525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 7, 7, 1024)   0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 7, 7, 1024)   0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 7, 7, 256)    0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 7, 7, 256)    0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 7, 7, 1024)   0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 7, 7, 1024)   0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 7, 7, 256)    0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 7, 7, 256)    0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 7, 7, 1024)   0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 7, 7, 1024)   0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 7, 7, 256)    0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 7, 7, 256)    0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 7, 7, 1024)   0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 7, 7, 1024)   0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 7, 7, 256)    0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 7, 7, 256)    0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 7, 7, 1024)   0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 7, 7, 1024)   0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 7, 7, 256)    0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 7, 7, 256)    0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 7, 7, 1024)   0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 7, 7, 1024)   0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 4, 4, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 4, 4, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 4, 4, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 4, 4, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 4, 4, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 4, 4, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 4, 4, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 4, 4, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 4, 4, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 4, 4, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 4, 4, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 4, 4, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 4, 4, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 4, 4, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 4, 4, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 4, 4, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2048)         4196352     global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 2048)         4196352     dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 10)           20490       dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 32,000,963\n",
      "Trainable params: 31,894,723\n",
      "Non-trainable params: 106,240\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = define_model(10, (100,100,18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[4].trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[3].trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canopy_ml",
   "language": "python",
   "name": "canopy_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

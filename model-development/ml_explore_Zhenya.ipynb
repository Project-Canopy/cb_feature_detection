{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "import rasterio\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import layers\n",
    "import keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with Simple CNN and Data Loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation on 55 images \n",
      "Training on 100 images \n"
     ]
    }
   ],
   "source": [
    "gen = DataLoader(label_file_path_train=\"labels_test_v1.csv\", #or labels.csv\n",
    "                        label_file_path_val=\"val_labels.csv\",\n",
    "                        bucket_name='canopy-production-ml',\n",
    "                        data_extension_type='.tif',\n",
    "                        training_data_shape=(100, 100, 18),\n",
    "                        shuffle_and_repeat=False,\n",
    "                        enable_just_shuffle=True,\n",
    "                        enable_just_repeat=False,\n",
    "                        training_data_shuffle_buffer_size=10,\n",
    "                        data_repeat_count=None,\n",
    "                        training_data_batch_size=20,\n",
    "                        normalization_value=255.0,  #normalization TODO double check other channels than RGB \n",
    "                        training_data_type=tf.float32,\n",
    "                        label_data_type=tf.uint8,\n",
    "                        enable_data_prefetch=False,\n",
    "                        data_prefetch_size=tf.data.experimental.AUTOTUNE,\n",
    "                        num_parallel_calls=int(2))\n",
    "# TODO add data augmentation in DataLoader \n",
    "\n",
    "no_of_val_imgs = len(gen.validation_filenames)\n",
    "no_of_train_imgs = len(gen.training_filenames)\n",
    "print(\"Validation on {} images \".format(str(no_of_val_imgs)))\n",
    "print(\"Training on {} images \".format(str(no_of_train_imgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<data_loader.DataLoader at 0x13e653310>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 86s 18s/step - loss: 8.7035 - val_loss: 1.5012\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 81s 17s/step - loss: 1.4694 - val_loss: 0.8578\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 82s 17s/step - loss: 0.6244 - val_loss: 0.2065\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 86s 17s/step - loss: 0.1674 - val_loss: 0.2439\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 84s 17s/step - loss: 0.1836 - val_loss: 0.2107\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 87s 18s/step - loss: 0.1562 - val_loss: 0.1956\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 82s 17s/step - loss: 0.1483 - val_loss: 0.1829\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 83s 17s/step - loss: 0.1326 - val_loss: 0.1831\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 87s 17s/step - loss: 0.1373 - val_loss: 0.1843\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 83s 17s/step - loss: 0.1324 - val_loss: 0.1902\n"
     ]
    }
   ],
   "source": [
    "def Simple_CNN(numclasses, input_shape): #TODO use a more complex CNN\n",
    "        model = Sequential([\n",
    "            layers.Input(input_shape),\n",
    "            layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "            layers.MaxPooling2D(),\n",
    "            layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "            layers.MaxPooling2D(),\n",
    "            layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "            layers.MaxPooling2D(),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(numclasses)\n",
    "        ])\n",
    "        return model\n",
    "\n",
    "model_simpleCNN = Simple_CNN(10, input_shape=(100, 100, 18))\n",
    "callbacks_list = []\n",
    "\n",
    "model_simpleCNN.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "          optimizer=keras.optimizers.Adam()) #TODO add callbacks to save checkpoints and maybe lr reducer,etc \n",
    "\n",
    "epochs = 10\n",
    "history = model_simpleCNN.fit(gen.training_dataset, validation_data=gen.validation_dataset, epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100, 18)\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "obj = s3.Object('canopy-production-ml', \"chips/cloudfree-merge-polygons/split/test/100/100_1000_1000.tif\")\n",
    "obj_bytes = io.BytesIO(obj.get()['Body'].read())\n",
    "with rasterio.open(obj_bytes) as src:\n",
    "    img_test = np.transpose(src.read(), (1, 2, 0))\n",
    "print(img_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['Habitation', 'ISL', 'Industrial_agriculture', 'Mining',\n",
    "    'Rainforest', 'River', 'Roads', 'Savannah', 'Shifting_cultivation',\n",
    "    'Water'\n",
    "]\n",
    "# TODO Need to weight labels since they are pretty unbalanced (Rainforest is largely represented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 = boto3.resource('s3')\n",
    "# obj = s3.Object('canopy-production-ml', \"chips/cloudfree-merge-polygons/split/train/58/58_1300_1000.tif\")\n",
    "# obj_bytes = io.BytesIO(obj.get()['Body'].read())\n",
    "# with rasterio.open(obj_bytes) as src:\n",
    "#     img_test = np.transpose(src.read(), (1, 2, 0)) / 255\n",
    "# print(img_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This chip was predicted to belong to class Rainforest\n"
     ]
    }
   ],
   "source": [
    "predictions = model_simpleCNN.predict(np.array([img_test]))\n",
    "highest_score_predictions = np.argmax(predictions) # TODO: read more about multi classes PER IMAGE classification, what is the threshold?\n",
    "\n",
    "print(\"This chip was predicted to belong to class {}\".format(label_list[highest_score_predictions]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2020.7277        4.4248075 -1614.4033    -3687.283       157.33759\n",
      "  -1660.8308     -644.8952    -2039.6393    -1582.1956    -1557.5543   ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3, 7, 0, 5, 2, 8, 9, 6, 1, 4]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(predictions)\n",
    "predictions.argsort() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 35s 10s/step - loss: 0.1902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.19022126495838165"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_simpleCNN.evaluate(gen.validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Resnet50(numclasses, input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(keras.applications.ResNet50(include_top=False, pooling='avg', weights=None, input_shape=input_shape))\n",
    "    model.add(Flatten())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(2048, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(numclasses, activation='softmax'))\n",
    "    model.layers[0].trainable = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/applications/imagenet_utils.py:331: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 18 input channels.\n",
      "  warnings.warn('This model usually expects 1 or 3 input channels. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 100s 19s/step - loss: 0.7197 - val_loss: 0.7552\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 92s 18s/step - loss: 0.7108 - val_loss: 0.7552\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 89s 18s/step - loss: 0.6910 - val_loss: 0.7552\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 92s 18s/step - loss: 0.6992 - val_loss: 0.7552\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 88s 18s/step - loss: 0.6945 - val_loss: 0.7552\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 90s 17s/step - loss: 0.6954 - val_loss: 0.7552\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 91s 18s/step - loss: 0.6927 - val_loss: 0.7552\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 90s 17s/step - loss: 0.6903 - val_loss: 0.7552\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 91s 19s/step - loss: 0.6820 - val_loss: 0.7552\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 94s 18s/step - loss: 0.6781 - val_loss: 0.7552\n"
     ]
    }
   ],
   "source": [
    "model_resnet50 = Resnet50(10, input_shape=(100, 100, 18))\n",
    "callbacks_list = []\n",
    "\n",
    "model_resnet50.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "          optimizer=keras.optimizers.Adam()) #TODO add callbacks to save checkpoints and maybe lr reducer, earlystop,etc \n",
    "\n",
    "epochs = 10\n",
    "history = model_resnet50.fit(gen.training_dataset, validation_data=gen.validation_dataset, epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This chip was predicted to belong to class Industrial_agriculture\n"
     ]
    }
   ],
   "source": [
    "predictions = model_resnet50.predict(np.array([img_test]))\n",
    "highest_score_predictions = np.argmax(predictions) # TODO: read more about multi classes PER IMAGE classification, what is the threshold?\n",
    "\n",
    "print(\"This chip was predicted to belong to class {}\".format(label_list[highest_score_predictions]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 29s 9s/step - loss: 0.7552\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7551586031913757"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_resnet50.evaluate(gen.validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Industrial_agriculture\n",
      "Water\n",
      "Shifting_cultivation\n"
     ]
    }
   ],
   "source": [
    "# https://kgptalkie.com/multi-label-image-classification-on-movies-poster-using-cnn/\n",
    "top3 = np.argsort(predictions[0])[:-4:-1]\n",
    "for i in range(3):\n",
    "  print(label_list[top3[i]]) # We need to define a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_learningCurve(history, epoch):\n",
    "  # Plot training & validation accuracy values\n",
    "#   plt.plot(history.history['accuracy'])\n",
    "#   plt.plot(history.history['val_accuracy'])\n",
    "#   plt.title('Model accuracy')\n",
    "#   plt.ylabel('Accuracy')\n",
    "#   plt.xlabel('Epoch')\n",
    "#   plt.legend(['Train', 'Val'], loc='upper left')\n",
    "#   plt.show()\n",
    "\n",
    "  # Plot training & validation loss values\n",
    "  plt.plot(history.history['loss'])\n",
    "  plt.plot(history.history['val_loss'])\n",
    "  plt.title('Model loss')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Val'], loc='upper left')\n",
    "  plt.show()\n",
    "\n",
    "plot_learningCurve(history, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medium_CNN(numclasses, input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, (3,3), activation='relu', input_shape = input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(2,2))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(32, (3,3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(2,2))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(2,2))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(128, (3,3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(2,2))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "    model.add(Dense(numclasses, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 86s 17s/step - loss: 0.9270 - accuracy: 0.1209 - val_loss: 0.8956 - val_accuracy: 0.1200\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 91s 19s/step - loss: 0.9144 - accuracy: 0.1223 - val_loss: 0.9091 - val_accuracy: 0.1200\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 85s 17s/step - loss: 0.9159 - accuracy: 0.1198 - val_loss: 0.9205 - val_accuracy: 0.1200\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 86s 18s/step - loss: 0.9162 - accuracy: 0.1193 - val_loss: 0.9494 - val_accuracy: 0.1200\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 84s 17s/step - loss: 0.9036 - accuracy: 0.1200 - val_loss: 0.9662 - val_accuracy: 0.1200\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 85s 17s/step - loss: 0.9077 - accuracy: 0.1201 - val_loss: 0.9763 - val_accuracy: 0.1200\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 89s 17s/step - loss: 0.9053 - accuracy: 0.1200 - val_loss: 0.9755 - val_accuracy: 0.1200\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 84s 17s/step - loss: 0.8920 - accuracy: 0.1197 - val_loss: 0.9652 - val_accuracy: 0.1200\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 85s 17s/step - loss: 0.8925 - accuracy: 0.1195 - val_loss: 0.9509 - val_accuracy: 0.1200\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 84s 17s/step - loss: 0.8863 - accuracy: 0.1211 - val_loss: 0.9204 - val_accuracy: 0.1200\n"
     ]
    }
   ],
   "source": [
    "model_medium_CNN = medium_CNN(10, input_shape=(100, 100, 18))\n",
    "callbacks_list = []\n",
    "\n",
    "model_medium_CNN.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                          optimizer=keras.optimizers.Adam(),\n",
    "                          metrics=[tf.metrics.BinaryAccuracy(threshold=0.0, name='accuracy')]) #TODO add callbacks to save checkpoints and maybe lr reducer, earlystop,etc \n",
    "\n",
    "epochs = 10\n",
    "history = model_medium_CNN.fit(gen.training_dataset, validation_data=gen.validation_dataset, epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This chip was predicted to belong to top 3 classes:\n",
      "Water\n",
      "Shifting_cultivation\n",
      "Savannah\n"
     ]
    }
   ],
   "source": [
    "predictions = model_medium_CNN.predict(np.array([img_test]))\n",
    "highest_score_predictions = np.argmax(predictions) # TODO: read more about multi classes PER IMAGE classification, what is the threshold?\n",
    "\n",
    "print(\"This chip was predicted to belong to top 3 classes:\")\n",
    "\n",
    "top3 = np.argsort(predictions[0])[:-4:-1]\n",
    "for i in range(3):\n",
    "  print(label_list[top3[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 32s 11s/step - loss: 0.9204 - accuracy: 0.1200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9204237461090088, 0.12000000476837158]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_medium_CNN.evaluate(gen.validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_learningCurve(history, epoch):\n",
    "  # Plot training & validation accuracy values\n",
    "  plt.plot(history.history['accuracy'])\n",
    "  plt.plot(history.history['val_accuracy'])\n",
    "  plt.title('Model accuracy')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Val'], loc='upper left')\n",
    "  plt.show()\n",
    "\n",
    "  # Plot training & validation loss values\n",
    "  plt.plot(history.history['loss'])\n",
    "  plt.plot(history.history['val_loss'])\n",
    "  plt.title('Model loss')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Val'], loc='upper left')\n",
    "  plt.show()\n",
    "\n",
    "plot_learningCurve(history, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST with bigearthnet-resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow_hub/keras_layer.py:228 call  *\n        result = f()\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1669 __call__  **\n        return self._call_impl(args, kwargs)\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py:246 _call_impl\n        return super(WrappedFunction, self)._call_impl(\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1687 _call_impl\n        return self._call_with_flat_signature(args, kwargs, cancellation_manager)\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1736 _call_with_flat_signature\n        return self._call_flat(args, self.captured_inputs, cancellation_manager)\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1900 _call_flat\n        raise ValueError(\n\n    ValueError: The argument 'images' (value Tensor(\"Placeholder:0\", shape=(None, 100, 100, 4), dtype=float32)) is not compatible with the shape this function was traced with. Expected shape (None, None, None, 3), but got shape (None, 100, 100, 4).\n    \n    If you called get_concrete_function, you may need to pass a tf.TensorSpec(..., shape=...) with a less specific shape, having None on axes which can vary.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-9359f750e35b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://tfhub.dev/google/remote_sensing/bigearthnet-resnet50/1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m model_bigearthnet = tf.keras.Sequential([\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInputLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMAGE_SIZE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# reshape?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layers, name)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    221\u001b[0m       \u001b[0;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m       \u001b[0;31m# refresh its output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m       \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSINGLE_LAYER_OUTPUT_ERROR_MSG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;31m# >> model = tf.keras.Model(inputs, outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0m\u001b[1;32m    952\u001b[0m                                                 input_list)\n\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1088\u001b[0m           layer=self, inputs=inputs, build_graph=True, training=training_value):\n\u001b[1;32m   1089\u001b[0m         \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m         outputs = self._keras_tensor_symbolic_call(\n\u001b[0m\u001b[1;32m   1091\u001b[0m             inputs, input_masks, args, kwargs)\n\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    861\u001b[0m           \u001b[0;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow_hub/keras_layer.py:228 call  *\n        result = f()\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1669 __call__  **\n        return self._call_impl(args, kwargs)\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py:246 _call_impl\n        return super(WrappedFunction, self)._call_impl(\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1687 _call_impl\n        return self._call_with_flat_signature(args, kwargs, cancellation_manager)\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1736 _call_with_flat_signature\n        return self._call_flat(args, self.captured_inputs, cancellation_manager)\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1900 _call_flat\n        raise ValueError(\n\n    ValueError: The argument 'images' (value Tensor(\"Placeholder:0\", shape=(None, 100, 100, 4), dtype=float32)) is not compatible with the shape this function was traced with. Expected shape (None, None, None, 3), but got shape (None, 100, 100, 4).\n    \n    If you called get_concrete_function, you may need to pass a tf.TensorSpec(..., shape=...) with a less specific shape, having None on axes which can vary.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "IMAGE_SIZE = (100,100)\n",
    "num_classes = 10\n",
    "model_handle = \"https://tfhub.dev/google/remote_sensing/bigearthnet-resnet50/1\"\n",
    "model_bigearthnet = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
    "    # reshape? \n",
    "    hub.KerasLayer(model_handle, trainable=False, input_shape=IMAGE_SIZE + (3,)),\n",
    "#     (model.layers[-1].output)\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(num_classes,\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
    "])\n",
    "model_bigearthnet.build((None,)+IMAGE_SIZE+(4,))\n",
    "model_bigearthnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_bigearthnet.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#           optimizer=keras.optimizers.Adam()) #TODO add callbacks to save checkpoints and maybe lr reducer, earlystop,etc \n",
    "\n",
    "# epochs = 10\n",
    "# history = model_bigearthnet.fit(gen.training_dataset, validation_data=gen.validation_dataset, epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = model_resnet50.predict(np.array([img_test]))\n",
    "# highest_score_predictions = np.argmax(predictions) # TODO: read more about multi classes PER IMAGE classification, what is the threshold?\n",
    "\n",
    "# print(\"This chip was predicted to belong to class {}\".format(label_list[highest_score_predictions]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preproduction Candidate: ResNet50 pretrained on ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation on 55 images \n",
      "Training on 100 images \n"
     ]
    }
   ],
   "source": [
    "gen = DataLoader(label_file_path_train=\"labels_test_v1.csv\", #or labels.csv\n",
    "                        label_file_path_val=\"val_labels.csv\",\n",
    "                        bucket_name='canopy-production-ml',\n",
    "                        data_extension_type='.tif',\n",
    "                        training_data_shape=(100, 100, 18),\n",
    "                        shuffle_and_repeat=False,\n",
    "                        enable_just_shuffle=False,\n",
    "                        enable_just_repeat=False,\n",
    "                        training_data_shuffle_buffer_size=10,\n",
    "                        data_repeat_count=None,\n",
    "                        training_data_batch_size=20,\n",
    "                        normalization_value=255.0,  #normalization TODO double check other channels than RGB \n",
    "                        training_data_type=tf.float32,\n",
    "                        label_data_type=tf.uint8,\n",
    "                        enable_data_prefetch=False,\n",
    "                        data_prefetch_size=tf.data.experimental.AUTOTUNE,\n",
    "                        num_parallel_calls=int(2))\n",
    "# TODO add data augmentation in DataLoader \n",
    "\n",
    "no_of_val_imgs = len(gen.validation_filenames)\n",
    "no_of_train_imgs = len(gen.training_filenames)\n",
    "print(\"Validation on {} images \".format(str(no_of_val_imgs)))\n",
    "print(\"Training on {} images \".format(str(no_of_train_imgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(numclasses,input_shape):\n",
    "    # parameters for CNN\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "\n",
    "    # introduce a additional layer to get from bands to 3 input channels\n",
    "    input_tensor = Conv2D(3, (1, 1))(input_tensor)\n",
    "\n",
    "    base_model_resnet50 = keras.applications.ResNet50(include_top=False,\n",
    "                              weights='imagenet',\n",
    "                              input_shape=(100, 100, 3))\n",
    "    base_model = keras.applications.ResNet50(include_top=False,\n",
    "                     weights=None,\n",
    "                     input_tensor=input_tensor)\n",
    "\n",
    "    for i, layer in enumerate(base_model_resnet50.layers):\n",
    "        # we must skip input layer, which has no weights\n",
    "        if i == 0:\n",
    "            continue\n",
    "        base_model.layers[i+1].set_weights(layer.get_weights())\n",
    "\n",
    "    # add a global spatial average pooling layer\n",
    "    top_model = base_model.output\n",
    "    top_model = GlobalAveragePooling2D()(top_model)\n",
    "\n",
    "    # let's add a fully-connected layer\n",
    "    top_model = Dense(2048, activation='relu')(top_model)\n",
    "    top_model = Dense(2048, activation='relu')(top_model)\n",
    "    # and a logistic layer\n",
    "    predictions = Dense(numclasses, activation='softmax')(top_model)\n",
    "\n",
    "    # this is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 100, 100, 18 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 100, 100, 3)  57          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 106, 106, 3)  0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 50, 50, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 50, 50, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 50, 50, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 52, 52, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 25, 25, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 25, 25, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 25, 25, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 25, 25, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 25, 25, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 25, 25, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 25, 25, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 25, 25, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 25, 25, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 25, 25, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 25, 25, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 25, 25, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 25, 25, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 25, 25, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 25, 25, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 25, 25, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 25, 25, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 25, 25, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 25, 25, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 25, 25, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 25, 25, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 25, 25, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 25, 25, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 25, 25, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 25, 25, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 25, 25, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 25, 25, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 13, 13, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 13, 13, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 13, 13, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 13, 13, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 13, 13, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 13, 13, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 13, 13, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 13, 13, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 13, 13, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 13, 13, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 13, 13, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 13, 13, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 13, 13, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 13, 13, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 13, 13, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 13, 13, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 13, 13, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 13, 13, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 13, 13, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 13, 13, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 13, 13, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 13, 13, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 13, 13, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 13, 13, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 13, 13, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 13, 13, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 13, 13, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 13, 13, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 13, 13, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 7, 7, 256)    131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 7, 7, 256)    0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 7, 7, 256)    0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 7, 7, 1024)   525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 7, 7, 1024)   0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 7, 7, 1024)   0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 7, 7, 256)    0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 7, 7, 256)    0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 7, 7, 1024)   0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 7, 7, 1024)   0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 7, 7, 256)    0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 7, 7, 256)    0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 7, 7, 1024)   0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 7, 7, 1024)   0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 7, 7, 256)    0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 7, 7, 256)    0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 7, 7, 1024)   0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 7, 7, 1024)   0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 7, 7, 256)    0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 7, 7, 256)    0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 7, 7, 1024)   0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 7, 7, 1024)   0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 7, 7, 256)    0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 7, 7, 256)    0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 7, 7, 1024)   0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 7, 7, 1024)   0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 4, 4, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 4, 4, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 4, 4, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 4, 4, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 4, 4, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 4, 4, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 4, 4, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 4, 4, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 4, 4, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 4, 4, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 4, 4, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 4, 4, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 4, 4, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 4, 4, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 4, 4, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 4, 4, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2048)         4196352     global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 2048)         4196352     dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 10)           20490       dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 32,000,963\n",
      "Trainable params: 31,947,843\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "random_id = 1234 #TODO\n",
    "checkpoint_file = 'checkpoint_{}.h5'.format(random_id)\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "  filepath= checkpoint_file,\n",
    "  format='h5',\n",
    "  verbose=1,\n",
    "  save_weights_only=True,\n",
    "  monitor='val_loss',\n",
    "  mode='min',\n",
    "  save_best_only=True)\n",
    "\n",
    "reducelronplateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "  monitor='val_loss', factor=0.1, patience=10, verbose=1,\n",
    "  mode='min', min_lr=1e-10)\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',mode='min', patience=20, verbose=1)\n",
    "\n",
    "callbacks_list = [model_checkpoint_callback, reducelronplateau, early_stop]\n",
    "\n",
    "model = define_model(10, (100,100,18))\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                          optimizer=keras.optimizers.Adam(),\n",
    "                          metrics=[tf.metrics.BinaryAccuracy(name='accuracy')]) #TODO add callbacks to save checkpoints and maybe lr reducer, earlystop,etc \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5/5 [==============================] - 39s 6s/step - loss: 0.7017 - accuracy: 0.9058 - val_loss: 0.6661 - val_accuracy: 0.9473\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.66607, saving model to checkpoint_1234.h5\n",
      "Epoch 2/20\n",
      "5/5 [==============================] - 27s 5s/step - loss: 0.6775 - accuracy: 0.9348 - val_loss: 0.6661 - val_accuracy: 0.9473\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.66607\n",
      "Epoch 3/20\n",
      "5/5 [==============================] - 26s 5s/step - loss: 0.6775 - accuracy: 0.9348 - val_loss: 0.6661 - val_accuracy: 0.9473\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.66607\n",
      "Epoch 4/20\n",
      "5/5 [==============================] - 27s 5s/step - loss: 0.6775 - accuracy: 0.9348 - val_loss: 0.6661 - val_accuracy: 0.9473\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.66607\n",
      "Epoch 5/20\n",
      "5/5 [==============================] - 27s 6s/step - loss: 0.6775 - accuracy: 0.9348 - val_loss: 0.6661 - val_accuracy: 0.9473\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.66607\n",
      "Epoch 6/20\n",
      "5/5 [==============================] - 27s 5s/step - loss: 0.6775 - accuracy: 0.9348 - val_loss: 0.6661 - val_accuracy: 0.9473\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.66607\n",
      "Epoch 7/20\n",
      "5/5 [==============================] - 26s 5s/step - loss: 0.6775 - accuracy: 0.9348 - val_loss: 0.6661 - val_accuracy: 0.9473\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.66607\n",
      "Epoch 8/20\n",
      "5/5 [==============================] - 27s 5s/step - loss: 0.6775 - accuracy: 0.9348 - val_loss: 0.6661 - val_accuracy: 0.9473\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.66607\n",
      "Epoch 9/20\n",
      "5/5 [==============================] - 26s 5s/step - loss: 0.6775 - accuracy: 0.9348 - val_loss: 0.6661 - val_accuracy: 0.9473\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.66607\n",
      "Epoch 10/20\n",
      "5/5 [==============================] - 26s 5s/step - loss: 0.6775 - accuracy: 0.9348 - val_loss: 0.6661 - val_accuracy: 0.9473\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.66607\n",
      "Epoch 11/20\n",
      "5/5 [==============================] - 26s 5s/step - loss: 0.6775 - accuracy: 0.9348 - val_loss: 0.6661 - val_accuracy: 0.9473\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.66607\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 12/20\n",
      "5/5 [==============================] - 26s 5s/step - loss: 0.6775 - accuracy: 0.9348 - val_loss: 0.6661 - val_accuracy: 0.9473\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.66607\n",
      "Epoch 13/20\n",
      "5/5 [==============================] - 27s 5s/step - loss: 0.6775 - accuracy: 0.9348 - val_loss: 0.6661 - val_accuracy: 0.9473\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.66607\n",
      "Epoch 14/20\n",
      "5/5 [==============================] - 30s 6s/step - loss: 0.6775 - accuracy: 0.9348 - val_loss: 0.6661 - val_accuracy: 0.9473\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.66607\n",
      "Epoch 15/20\n",
      "5/5 [==============================] - 27s 5s/step - loss: 0.6775 - accuracy: 0.9348 - val_loss: 0.6661 - val_accuracy: 0.9473\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.66607\n",
      "Epoch 16/20\n",
      "5/5 [==============================] - 28s 6s/step - loss: 0.6775 - accuracy: 0.9348 - val_loss: 0.6661 - val_accuracy: 0.9473\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.66607\n",
      "Epoch 17/20\n",
      "5/5 [==============================] - 26s 5s/step - loss: 0.6775 - accuracy: 0.9348 - val_loss: 0.6661 - val_accuracy: 0.9473\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.66607\n",
      "Epoch 18/20\n",
      "5/5 [==============================] - 27s 5s/step - loss: 0.6775 - accuracy: 0.9348 - val_loss: 0.6661 - val_accuracy: 0.9473\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.66607\n",
      "Epoch 19/20\n",
      "5/5 [==============================] - 26s 5s/step - loss: 0.6775 - accuracy: 0.9348 - val_loss: 0.6661 - val_accuracy: 0.9473\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.66607\n",
      "Epoch 20/20\n",
      "5/5 [==============================] - 27s 6s/step - loss: 0.6775 - accuracy: 0.9348 - val_loss: 0.6661 - val_accuracy: 0.9473\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.66607\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "history = model.fit(gen.training_dataset, validation_data=gen.validation_dataset, \n",
    "                    epochs=epochs, \n",
    "                    callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 9s 3s/step - loss: 0.6661 - accuracy: 0.9473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6660677194595337, 0.9472727179527283]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(gen.validation_dataset) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on BatchDataset in module tensorflow.python.data.ops.dataset_ops object:\n",
      "\n",
      "class BatchDataset(UnaryDataset)\n",
      " |  BatchDataset(input_dataset, batch_size, drop_remainder)\n",
      " |  \n",
      " |  A `Dataset` that batches contiguous elements from its input.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BatchDataset\n",
      " |      UnaryDataset\n",
      " |      DatasetV2\n",
      " |      collections.abc.Iterable\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      tensorflow.python.framework.composite_tensor.CompositeTensor\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, input_dataset, batch_size, drop_remainder)\n",
      " |      See `Dataset.batch()` for details.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  element_spec\n",
      " |      The type specification of an element of this dataset.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
      " |      >>> dataset.element_spec\n",
      " |      TensorSpec(shape=(), dtype=tf.int32, name=None)\n",
      " |      \n",
      " |      Returns:\n",
      " |        A nested structure of `tf.TypeSpec` objects matching the structure of an\n",
      " |        element of this dataset and specifying the type of individual components.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from DatasetV2:\n",
      " |  \n",
      " |  __bool__(self)\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Creates an iterator for elements of this dataset.\n",
      " |      \n",
      " |      The returned iterator implements the Python Iterator protocol.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An `tf.data.Iterator` for the elements of this dataset.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If not inside of tf.function and not executing eagerly.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Returns the length of the dataset if it is known and finite.\n",
      " |      \n",
      " |      This method requires that you are running in eager mode, and that the\n",
      " |      length of the dataset is known and non-infinite. When the length may be\n",
      " |      unknown or infinite, or if you are running in graph mode, use\n",
      " |      `tf.data.Dataset.cardinality` instead.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An integer representing the length of the dataset.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If the dataset length is unknown or infinite, or if eager\n",
      " |          execution is not enabled.\n",
      " |  \n",
      " |  __nonzero__ = __bool__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  apply(self, transformation_func)\n",
      " |      Applies a transformation function to this dataset.\n",
      " |      \n",
      " |      `apply` enables chaining of custom `Dataset` transformations, which are\n",
      " |      represented as functions that take one `Dataset` argument and return a\n",
      " |      transformed `Dataset`.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(100)\n",
      " |      >>> def dataset_fn(ds):\n",
      " |      ...   return ds.filter(lambda x: x < 5)\n",
      " |      >>> dataset = dataset.apply(dataset_fn)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [0, 1, 2, 3, 4]\n",
      " |      \n",
      " |      Args:\n",
      " |        transformation_func: A function that takes one `Dataset` argument and\n",
      " |          returns a `Dataset`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: The `Dataset` returned by applying `transformation_func` to this\n",
      " |            dataset.\n",
      " |  \n",
      " |  as_numpy_iterator(self)\n",
      " |      Returns an iterator which converts all elements of the dataset to numpy.\n",
      " |      \n",
      " |      Use `as_numpy_iterator` to inspect the content of your dataset. To see\n",
      " |      element shapes and types, print dataset elements directly instead of using\n",
      " |      `as_numpy_iterator`.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
      " |      >>> for element in dataset:\n",
      " |      ...   print(element)\n",
      " |      tf.Tensor(1, shape=(), dtype=int32)\n",
      " |      tf.Tensor(2, shape=(), dtype=int32)\n",
      " |      tf.Tensor(3, shape=(), dtype=int32)\n",
      " |      \n",
      " |      This method requires that you are running in eager mode and the dataset's\n",
      " |      element_spec contains only `TensorSpec` components.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
      " |      >>> for element in dataset.as_numpy_iterator():\n",
      " |      ...   print(element)\n",
      " |      1\n",
      " |      2\n",
      " |      3\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
      " |      >>> print(list(dataset.as_numpy_iterator()))\n",
      " |      [1, 2, 3]\n",
      " |      \n",
      " |      `as_numpy_iterator()` will preserve the nested structure of dataset\n",
      " |      elements.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices({'a': ([1, 2], [3, 4]),\n",
      " |      ...                                               'b': [5, 6]})\n",
      " |      >>> list(dataset.as_numpy_iterator()) == [{'a': (1, 3), 'b': 5},\n",
      " |      ...                                       {'a': (2, 4), 'b': 6}]\n",
      " |      True\n",
      " |      \n",
      " |      Returns:\n",
      " |        An iterable over the elements of the dataset, with their tensors converted\n",
      " |        to numpy arrays.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if an element contains a non-`Tensor` value.\n",
      " |        RuntimeError: if eager execution is not enabled.\n",
      " |  \n",
      " |  batch(self, batch_size, drop_remainder=False)\n",
      " |      Combines consecutive elements of this dataset into batches.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(8)\n",
      " |      >>> dataset = dataset.batch(3)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [array([0, 1, 2]), array([3, 4, 5]), array([6, 7])]\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(8)\n",
      " |      >>> dataset = dataset.batch(3, drop_remainder=True)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [array([0, 1, 2]), array([3, 4, 5])]\n",
      " |      \n",
      " |      The components of the resulting element will have an additional outer\n",
      " |      dimension, which will be `batch_size` (or `N % batch_size` for the last\n",
      " |      element if `batch_size` does not divide the number of input elements `N`\n",
      " |      evenly and `drop_remainder` is `False`). If your program depends on the\n",
      " |      batches having the same outer dimension, you should set the `drop_remainder`\n",
      " |      argument to `True` to prevent the smaller batch from being produced.\n",
      " |      \n",
      " |      Args:\n",
      " |        batch_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          consecutive elements of this dataset to combine in a single batch.\n",
      " |        drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\n",
      " |          whether the last batch should be dropped in the case it has fewer than\n",
      " |          `batch_size` elements; the default behavior is not to drop the smaller\n",
      " |          batch.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  cache(self, filename='')\n",
      " |      Caches the elements in this dataset.\n",
      " |      \n",
      " |      The first time the dataset is iterated over, its elements will be cached\n",
      " |      either in the specified file or in memory. Subsequent iterations will\n",
      " |      use the cached data.\n",
      " |      \n",
      " |      Note: For the cache to be finalized, the input dataset must be iterated\n",
      " |      through in its entirety. Otherwise, subsequent iterations will not use\n",
      " |      cached data.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(5)\n",
      " |      >>> dataset = dataset.map(lambda x: x**2)\n",
      " |      >>> dataset = dataset.cache()\n",
      " |      >>> # The first time reading through the data will generate the data using\n",
      " |      >>> # `range` and `map`.\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [0, 1, 4, 9, 16]\n",
      " |      >>> # Subsequent iterations read from the cache.\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [0, 1, 4, 9, 16]\n",
      " |      \n",
      " |      When caching to a file, the cached data will persist across runs. Even the\n",
      " |      first iteration through the data will read from the cache file. Changing\n",
      " |      the input pipeline before the call to `.cache()` will have no effect until\n",
      " |      the cache file is removed or the filename is changed.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(5)\n",
      " |      >>> dataset = dataset.cache(\"/path/to/file\")  # doctest: +SKIP\n",
      " |      >>> list(dataset.as_numpy_iterator())  # doctest: +SKIP\n",
      " |      [0, 1, 2, 3, 4]\n",
      " |      >>> dataset = tf.data.Dataset.range(10)\n",
      " |      >>> dataset = dataset.cache(\"/path/to/file\")  # Same file! # doctest: +SKIP\n",
      " |      >>> list(dataset.as_numpy_iterator())  # doctest: +SKIP\n",
      " |      [0, 1, 2, 3, 4]\n",
      " |      \n",
      " |      Note: `cache` will produce exactly the same elements during each iteration\n",
      " |      through the dataset. If you wish to randomize the iteration order, make sure\n",
      " |      to call `shuffle` *after* calling `cache`.\n",
      " |      \n",
      " |      Args:\n",
      " |        filename: A `tf.string` scalar `tf.Tensor`, representing the name of a\n",
      " |          directory on the filesystem to use for caching elements in this Dataset.\n",
      " |          If a filename is not provided, the dataset will be cached in memory.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  cardinality(self)\n",
      " |      Returns the cardinality of the dataset, if known.\n",
      " |      \n",
      " |      `cardinality` may return `tf.data.INFINITE_CARDINALITY` if the dataset\n",
      " |      contains an infinite number of elements or `tf.data.UNKNOWN_CARDINALITY` if\n",
      " |      the analysis fails to determine the number of elements in the dataset\n",
      " |      (e.g. when the dataset source is a file).\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(42)\n",
      " |      >>> print(dataset.cardinality().numpy())\n",
      " |      42\n",
      " |      >>> dataset = dataset.repeat()\n",
      " |      >>> cardinality = dataset.cardinality()\n",
      " |      >>> print((cardinality == tf.data.INFINITE_CARDINALITY).numpy())\n",
      " |      True\n",
      " |      >>> dataset = dataset.filter(lambda x: True)\n",
      " |      >>> cardinality = dataset.cardinality()\n",
      " |      >>> print((cardinality == tf.data.UNKNOWN_CARDINALITY).numpy())\n",
      " |      True\n",
      " |      \n",
      " |      Returns:\n",
      " |        A scalar `tf.int64` `Tensor` representing the cardinality of the dataset.\n",
      " |        If the cardinality is infinite or unknown, `cardinality` returns the\n",
      " |        named constants `tf.data.INFINITE_CARDINALITY` and\n",
      " |        `tf.data.UNKNOWN_CARDINALITY` respectively.\n",
      " |  \n",
      " |  concatenate(self, dataset)\n",
      " |      Creates a `Dataset` by concatenating the given dataset with this dataset.\n",
      " |      \n",
      " |      >>> a = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\n",
      " |      >>> b = tf.data.Dataset.range(4, 8)  # ==> [ 4, 5, 6, 7 ]\n",
      " |      >>> ds = a.concatenate(b)\n",
      " |      >>> list(ds.as_numpy_iterator())\n",
      " |      [1, 2, 3, 4, 5, 6, 7]\n",
      " |      >>> # The input dataset and dataset to be concatenated should have the same\n",
      " |      >>> # nested structures and output types.\n",
      " |      >>> c = tf.data.Dataset.zip((a, b))\n",
      " |      >>> a.concatenate(c)\n",
      " |      Traceback (most recent call last):\n",
      " |      TypeError: Two datasets to concatenate have different types\n",
      " |      <dtype: 'int64'> and (tf.int64, tf.int64)\n",
      " |      >>> d = tf.data.Dataset.from_tensor_slices([\"a\", \"b\", \"c\"])\n",
      " |      >>> a.concatenate(d)\n",
      " |      Traceback (most recent call last):\n",
      " |      TypeError: Two datasets to concatenate have different types\n",
      " |      <dtype: 'int64'> and <dtype: 'string'>\n",
      " |      \n",
      " |      Args:\n",
      " |        dataset: `Dataset` to be concatenated.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  enumerate(self, start=0)\n",
      " |      Enumerates the elements of this dataset.\n",
      " |      \n",
      " |      It is similar to python's `enumerate`.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
      " |      >>> dataset = dataset.enumerate(start=5)\n",
      " |      >>> for element in dataset.as_numpy_iterator():\n",
      " |      ...   print(element)\n",
      " |      (5, 1)\n",
      " |      (6, 2)\n",
      " |      (7, 3)\n",
      " |      \n",
      " |      >>> # The nested structure of the input dataset determines the structure of\n",
      " |      >>> # elements in the resulting dataset.\n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices([(7, 8), (9, 10)])\n",
      " |      >>> dataset = dataset.enumerate()\n",
      " |      >>> for element in dataset.as_numpy_iterator():\n",
      " |      ...   print(element)\n",
      " |      (0, array([7, 8], dtype=int32))\n",
      " |      (1, array([ 9, 10], dtype=int32))\n",
      " |      \n",
      " |      Args:\n",
      " |        start: A `tf.int64` scalar `tf.Tensor`, representing the start value for\n",
      " |          enumeration.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  filter(self, predicate)\n",
      " |      Filters this dataset according to `predicate`.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
      " |      >>> dataset = dataset.filter(lambda x: x < 3)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [1, 2]\n",
      " |      >>> # `tf.math.equal(x, y)` is required for equality comparison\n",
      " |      >>> def filter_fn(x):\n",
      " |      ...   return tf.math.equal(x, 1)\n",
      " |      >>> dataset = dataset.filter(filter_fn)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [1]\n",
      " |      \n",
      " |      Args:\n",
      " |        predicate: A function mapping a dataset element to a boolean.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: The `Dataset` containing the elements of this dataset for which\n",
      " |            `predicate` is `True`.\n",
      " |  \n",
      " |  flat_map(self, map_func)\n",
      " |      Maps `map_func` across this dataset and flattens the result.\n",
      " |      \n",
      " |      Use `flat_map` if you want to make sure that the order of your dataset\n",
      " |      stays the same. For example, to flatten a dataset of batches into a\n",
      " |      dataset of their elements:\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices(\n",
      " |      ...                [[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
      " |      >>> dataset = dataset.flat_map(lambda x: Dataset.from_tensor_slices(x))\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      " |      \n",
      " |      `tf.data.Dataset.interleave()` is a generalization of `flat_map`, since\n",
      " |      `flat_map` produces the same output as\n",
      " |      `tf.data.Dataset.interleave(cycle_length=1)`\n",
      " |      \n",
      " |      Args:\n",
      " |        map_func: A function mapping a dataset element to a dataset.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  interleave(self, map_func, cycle_length=None, block_length=None, num_parallel_calls=None, deterministic=None)\n",
      " |      Maps `map_func` across this dataset, and interleaves the results.\n",
      " |      \n",
      " |      For example, you can use `Dataset.interleave()` to process many input files\n",
      " |      concurrently:\n",
      " |      \n",
      " |      >>> # Preprocess 4 files concurrently, and interleave blocks of 16 records\n",
      " |      >>> # from each file.\n",
      " |      >>> filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n",
      " |      ...              \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
      " |      >>> def parse_fn(filename):\n",
      " |      ...   return tf.data.Dataset.range(10)\n",
      " |      >>> dataset = dataset.interleave(lambda x:\n",
      " |      ...     tf.data.TextLineDataset(x).map(parse_fn, num_parallel_calls=1),\n",
      " |      ...     cycle_length=4, block_length=16)\n",
      " |      \n",
      " |      The `cycle_length` and `block_length` arguments control the order in which\n",
      " |      elements are produced. `cycle_length` controls the number of input elements\n",
      " |      that are processed concurrently. If you set `cycle_length` to 1, this\n",
      " |      transformation will handle one input element at a time, and will produce\n",
      " |      identical results to `tf.data.Dataset.flat_map`. In general,\n",
      " |      this transformation will apply `map_func` to `cycle_length` input elements,\n",
      " |      open iterators on the returned `Dataset` objects, and cycle through them\n",
      " |      producing `block_length` consecutive elements from each iterator, and\n",
      " |      consuming the next input element each time it reaches the end of an\n",
      " |      iterator.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      >>> dataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
      " |      >>> # NOTE: New lines indicate \"block\" boundaries.\n",
      " |      >>> dataset = dataset.interleave(\n",
      " |      ...     lambda x: Dataset.from_tensors(x).repeat(6),\n",
      " |      ...     cycle_length=2, block_length=4)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [1, 1, 1, 1,\n",
      " |       2, 2, 2, 2,\n",
      " |       1, 1,\n",
      " |       2, 2,\n",
      " |       3, 3, 3, 3,\n",
      " |       4, 4, 4, 4,\n",
      " |       3, 3,\n",
      " |       4, 4,\n",
      " |       5, 5, 5, 5,\n",
      " |       5, 5]\n",
      " |      \n",
      " |      Note: The order of elements yielded by this transformation is\n",
      " |      deterministic, as long as `map_func` is a pure function and\n",
      " |      `deterministic=True`. If `map_func` contains any stateful operations, the\n",
      " |      order in which that state is accessed is undefined.\n",
      " |      \n",
      " |      Performance can often be improved by setting `num_parallel_calls` so that\n",
      " |      `interleave` will use multiple threads to fetch elements. If determinism\n",
      " |      isn't required, it can also improve performance to set\n",
      " |      `deterministic=False`.\n",
      " |      \n",
      " |      >>> filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n",
      " |      ...              \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
      " |      >>> dataset = dataset.interleave(lambda x: tf.data.TFRecordDataset(x),\n",
      " |      ...     cycle_length=4, num_parallel_calls=tf.data.AUTOTUNE,\n",
      " |      ...     deterministic=False)\n",
      " |      \n",
      " |      Args:\n",
      " |        map_func: A function mapping a dataset element to a dataset.\n",
      " |        cycle_length: (Optional.) The number of input elements that will be\n",
      " |          processed concurrently. If not set, the tf.data runtime decides what it\n",
      " |          should be based on available CPU. If `num_parallel_calls` is set to\n",
      " |          `tf.data.AUTOTUNE`, the `cycle_length` argument identifies\n",
      " |          the maximum degree of parallelism.\n",
      " |        block_length: (Optional.) The number of consecutive elements to produce\n",
      " |          from each input element before cycling to another input element. If not\n",
      " |          set, defaults to 1.\n",
      " |        num_parallel_calls: (Optional.) If specified, the implementation creates a\n",
      " |          threadpool, which is used to fetch inputs from cycle elements\n",
      " |          asynchronously and in parallel. The default behavior is to fetch inputs\n",
      " |          from cycle elements synchronously with no parallelism. If the value\n",
      " |          `tf.data.AUTOTUNE` is used, then the number of parallel\n",
      " |          calls is set dynamically based on available CPU.\n",
      " |        deterministic: (Optional.) A boolean controlling whether determinism\n",
      " |          should be traded for performance by allowing elements to be produced out\n",
      " |          of order.  If `deterministic` is `None`, the\n",
      " |          `tf.data.Options.experimental_deterministic` dataset option (`True` by\n",
      " |          default) is used to decide whether to produce elements\n",
      " |          deterministically.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  map(self, map_func, num_parallel_calls=None, deterministic=None)\n",
      " |      Maps `map_func` across the elements of this dataset.\n",
      " |      \n",
      " |      This transformation applies `map_func` to each element of this dataset, and\n",
      " |      returns a new dataset containing the transformed elements, in the same\n",
      " |      order as they appeared in the input. `map_func` can be used to change both\n",
      " |      the values and the structure of a dataset's elements. For example, adding 1\n",
      " |      to each element, or projecting a subset of element components.\n",
      " |      \n",
      " |      >>> dataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
      " |      >>> dataset = dataset.map(lambda x: x + 1)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [2, 3, 4, 5, 6]\n",
      " |      \n",
      " |      The input signature of `map_func` is determined by the structure of each\n",
      " |      element in this dataset.\n",
      " |      \n",
      " |      >>> dataset = Dataset.range(5)\n",
      " |      >>> # `map_func` takes a single argument of type `tf.Tensor` with the same\n",
      " |      >>> # shape and dtype.\n",
      " |      >>> result = dataset.map(lambda x: x + 1)\n",
      " |      \n",
      " |      >>> # Each element is a tuple containing two `tf.Tensor` objects.\n",
      " |      >>> elements = [(1, \"foo\"), (2, \"bar\"), (3, \"baz\")]\n",
      " |      >>> dataset = tf.data.Dataset.from_generator(\n",
      " |      ...     lambda: elements, (tf.int32, tf.string))\n",
      " |      >>> # `map_func` takes two arguments of type `tf.Tensor`. This function\n",
      " |      >>> # projects out just the first component.\n",
      " |      >>> result = dataset.map(lambda x_int, y_str: x_int)\n",
      " |      >>> list(result.as_numpy_iterator())\n",
      " |      [1, 2, 3]\n",
      " |      \n",
      " |      >>> # Each element is a dictionary mapping strings to `tf.Tensor` objects.\n",
      " |      >>> elements =  ([{\"a\": 1, \"b\": \"foo\"},\n",
      " |      ...               {\"a\": 2, \"b\": \"bar\"},\n",
      " |      ...               {\"a\": 3, \"b\": \"baz\"}])\n",
      " |      >>> dataset = tf.data.Dataset.from_generator(\n",
      " |      ...     lambda: elements, {\"a\": tf.int32, \"b\": tf.string})\n",
      " |      >>> # `map_func` takes a single argument of type `dict` with the same keys\n",
      " |      >>> # as the elements.\n",
      " |      >>> result = dataset.map(lambda d: str(d[\"a\"]) + d[\"b\"])\n",
      " |      \n",
      " |      The value or values returned by `map_func` determine the structure of each\n",
      " |      element in the returned dataset.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(3)\n",
      " |      >>> # `map_func` returns two `tf.Tensor` objects.\n",
      " |      >>> def g(x):\n",
      " |      ...   return tf.constant(37.0), tf.constant([\"Foo\", \"Bar\", \"Baz\"])\n",
      " |      >>> result = dataset.map(g)\n",
      " |      >>> result.element_spec\n",
      " |      (TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(3,), dtype=tf.string, name=None))\n",
      " |      >>> # Python primitives, lists, and NumPy arrays are implicitly converted to\n",
      " |      >>> # `tf.Tensor`.\n",
      " |      >>> def h(x):\n",
      " |      ...   return 37.0, [\"Foo\", \"Bar\"], np.array([1.0, 2.0], dtype=np.float64)\n",
      " |      >>> result = dataset.map(h)\n",
      " |      >>> result.element_spec\n",
      " |      (TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(2,), dtype=tf.string, name=None), TensorSpec(shape=(2,), dtype=tf.float64, name=None))\n",
      " |      >>> # `map_func` can return nested structures.\n",
      " |      >>> def i(x):\n",
      " |      ...   return (37.0, [42, 16]), \"foo\"\n",
      " |      >>> result = dataset.map(i)\n",
      " |      >>> result.element_spec\n",
      " |      ((TensorSpec(shape=(), dtype=tf.float32, name=None),\n",
      " |        TensorSpec(shape=(2,), dtype=tf.int32, name=None)),\n",
      " |       TensorSpec(shape=(), dtype=tf.string, name=None))\n",
      " |      \n",
      " |      `map_func` can accept as arguments and return any type of dataset element.\n",
      " |      \n",
      " |      Note that irrespective of the context in which `map_func` is defined (eager\n",
      " |      vs. graph), tf.data traces the function and executes it as a graph. To use\n",
      " |      Python code inside of the function you have a few options:\n",
      " |      \n",
      " |      1) Rely on AutoGraph to convert Python code into an equivalent graph\n",
      " |      computation. The downside of this approach is that AutoGraph can convert\n",
      " |      some but not all Python code.\n",
      " |      \n",
      " |      2) Use `tf.py_function`, which allows you to write arbitrary Python code but\n",
      " |      will generally result in worse performance than 1). For example:\n",
      " |      \n",
      " |      >>> d = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\n",
      " |      >>> # transform a string tensor to upper case string using a Python function\n",
      " |      >>> def upper_case_fn(t: tf.Tensor):\n",
      " |      ...   return t.numpy().decode('utf-8').upper()\n",
      " |      >>> d = d.map(lambda x: tf.py_function(func=upper_case_fn,\n",
      " |      ...           inp=[x], Tout=tf.string))\n",
      " |      >>> list(d.as_numpy_iterator())\n",
      " |      [b'HELLO', b'WORLD']\n",
      " |      \n",
      " |      3) Use `tf.numpy_function`, which also allows you to write arbitrary\n",
      " |      Python code. Note that `tf.py_function` accepts `tf.Tensor` whereas\n",
      " |      `tf.numpy_function` accepts numpy arrays and returns only numpy arrays.\n",
      " |      For example:\n",
      " |      \n",
      " |      >>> d = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\n",
      " |      >>> def upper_case_fn(t: np.ndarray):\n",
      " |      ...   return t.decode('utf-8').upper()\n",
      " |      >>> d = d.map(lambda x: tf.numpy_function(func=upper_case_fn,\n",
      " |      ...           inp=[x], Tout=tf.string))\n",
      " |      >>> list(d.as_numpy_iterator())\n",
      " |      [b'HELLO', b'WORLD']\n",
      " |      \n",
      " |      Note that the use of `tf.numpy_function` and `tf.py_function`\n",
      " |      in general precludes the possibility of executing user-defined\n",
      " |      transformations in parallel (because of Python GIL).\n",
      " |      \n",
      " |      Performance can often be improved by setting `num_parallel_calls` so that\n",
      " |      `map` will use multiple threads to process elements. If deterministic order\n",
      " |      isn't required, it can also improve performance to set\n",
      " |      `deterministic=False`.\n",
      " |      \n",
      " |      >>> dataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
      " |      >>> dataset = dataset.map(lambda x: x + 1,\n",
      " |      ...     num_parallel_calls=tf.data.AUTOTUNE,\n",
      " |      ...     deterministic=False)\n",
      " |      \n",
      " |      Args:\n",
      " |        map_func: A function mapping a dataset element to another dataset element.\n",
      " |        num_parallel_calls: (Optional.) A `tf.int32` scalar `tf.Tensor`,\n",
      " |          representing the number elements to process asynchronously in parallel.\n",
      " |          If not specified, elements will be processed sequentially. If the value\n",
      " |          `tf.data.AUTOTUNE` is used, then the number of parallel\n",
      " |          calls is set dynamically based on available CPU.\n",
      " |        deterministic: (Optional.) A boolean controlling whether determinism\n",
      " |          should be traded for performance by allowing elements to be produced out\n",
      " |          of order.  If `deterministic` is `None`, the\n",
      " |          `tf.data.Options.experimental_deterministic` dataset option (`True` by\n",
      " |          default) is used to decide whether to produce elements\n",
      " |          deterministically.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  options(self)\n",
      " |      Returns the options for this dataset and its inputs.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.data.Options` object representing the dataset options.\n",
      " |  \n",
      " |  padded_batch(self, batch_size, padded_shapes=None, padding_values=None, drop_remainder=False)\n",
      " |      Combines consecutive elements of this dataset into padded batches.\n",
      " |      \n",
      " |      This transformation combines multiple consecutive elements of the input\n",
      " |      dataset into a single element.\n",
      " |      \n",
      " |      Like `tf.data.Dataset.batch`, the components of the resulting element will\n",
      " |      have an additional outer dimension, which will be `batch_size` (or\n",
      " |      `N % batch_size` for the last element if `batch_size` does not divide the\n",
      " |      number of input elements `N` evenly and `drop_remainder` is `False`). If\n",
      " |      your program depends on the batches having the same outer dimension, you\n",
      " |      should set the `drop_remainder` argument to `True` to prevent the smaller\n",
      " |      batch from being produced.\n",
      " |      \n",
      " |      Unlike `tf.data.Dataset.batch`, the input elements to be batched may have\n",
      " |      different shapes, and this transformation will pad each component to the\n",
      " |      respective shape in `padded_shapes`. The `padded_shapes` argument\n",
      " |      determines the resulting shape for each dimension of each component in an\n",
      " |      output element:\n",
      " |      \n",
      " |      * If the dimension is a constant, the component will be padded out to that\n",
      " |        length in that dimension.\n",
      " |      * If the dimension is unknown, the component will be padded out to the\n",
      " |        maximum length of all elements in that dimension.\n",
      " |      \n",
      " |      >>> A = (tf.data.Dataset\n",
      " |      ...      .range(1, 5, output_type=tf.int32)\n",
      " |      ...      .map(lambda x: tf.fill([x], x)))\n",
      " |      >>> # Pad to the smallest per-batch size that fits all elements.\n",
      " |      >>> B = A.padded_batch(2)\n",
      " |      >>> for element in B.as_numpy_iterator():\n",
      " |      ...   print(element)\n",
      " |      [[1 0]\n",
      " |       [2 2]]\n",
      " |      [[3 3 3 0]\n",
      " |       [4 4 4 4]]\n",
      " |      >>> # Pad to a fixed size.\n",
      " |      >>> C = A.padded_batch(2, padded_shapes=5)\n",
      " |      >>> for element in C.as_numpy_iterator():\n",
      " |      ...   print(element)\n",
      " |      [[1 0 0 0 0]\n",
      " |       [2 2 0 0 0]]\n",
      " |      [[3 3 3 0 0]\n",
      " |       [4 4 4 4 0]]\n",
      " |      >>> # Pad with a custom value.\n",
      " |      >>> D = A.padded_batch(2, padded_shapes=5, padding_values=-1)\n",
      " |      >>> for element in D.as_numpy_iterator():\n",
      " |      ...   print(element)\n",
      " |      [[ 1 -1 -1 -1 -1]\n",
      " |       [ 2  2 -1 -1 -1]]\n",
      " |      [[ 3  3  3 -1 -1]\n",
      " |       [ 4  4  4  4 -1]]\n",
      " |      >>> # Components of nested elements can be padded independently.\n",
      " |      >>> elements = [([1, 2, 3], [10]),\n",
      " |      ...             ([4, 5], [11, 12])]\n",
      " |      >>> dataset = tf.data.Dataset.from_generator(\n",
      " |      ...     lambda: iter(elements), (tf.int32, tf.int32))\n",
      " |      >>> # Pad the first component of the tuple to length 4, and the second\n",
      " |      >>> # component to the smallest size that fits.\n",
      " |      >>> dataset = dataset.padded_batch(2,\n",
      " |      ...     padded_shapes=([4], [None]),\n",
      " |      ...     padding_values=(-1, 100))\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [(array([[ 1,  2,  3, -1], [ 4,  5, -1, -1]], dtype=int32),\n",
      " |        array([[ 10, 100], [ 11,  12]], dtype=int32))]\n",
      " |      >>> # Pad with a single value and multiple components.\n",
      " |      >>> E = tf.data.Dataset.zip((A, A)).padded_batch(2, padding_values=-1)\n",
      " |      >>> for element in E.as_numpy_iterator():\n",
      " |      ...   print(element)\n",
      " |      (array([[ 1, -1],\n",
      " |             [ 2,  2]], dtype=int32), array([[ 1, -1],\n",
      " |             [ 2,  2]], dtype=int32))\n",
      " |      (array([[ 3,  3,  3, -1],\n",
      " |             [ 4,  4,  4,  4]], dtype=int32), array([[ 3,  3,  3, -1],\n",
      " |             [ 4,  4,  4,  4]], dtype=int32))\n",
      " |      \n",
      " |      See also `tf.data.experimental.dense_to_sparse_batch`, which combines\n",
      " |      elements that may have different shapes into a `tf.sparse.SparseTensor`.\n",
      " |      \n",
      " |      Args:\n",
      " |        batch_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          consecutive elements of this dataset to combine in a single batch.\n",
      " |        padded_shapes: (Optional.) A nested structure of `tf.TensorShape` or\n",
      " |          `tf.int64` vector tensor-like objects representing the shape to which\n",
      " |          the respective component of each input element should be padded prior\n",
      " |          to batching. Any unknown dimensions will be padded to the maximum size\n",
      " |          of that dimension in each batch. If unset, all dimensions of all\n",
      " |          components are padded to the maximum size in the batch. `padded_shapes`\n",
      " |          must be set if any component has an unknown rank.\n",
      " |        padding_values: (Optional.) A nested structure of scalar-shaped\n",
      " |          `tf.Tensor`, representing the padding values to use for the respective\n",
      " |          components. None represents that the nested structure should be padded\n",
      " |          with default values.  Defaults are `0` for numeric types and the empty\n",
      " |          string for string types. The `padding_values` should have the\n",
      " |          same structure as the input dataset. If `padding_values` is a single\n",
      " |          element and the input dataset has multiple components, then the same\n",
      " |          `padding_values` will be used to pad every component of the dataset.\n",
      " |          If `padding_values` is a scalar, then its value will be broadcasted\n",
      " |          to match the shape of each component.\n",
      " |        drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\n",
      " |          whether the last batch should be dropped in the case it has fewer than\n",
      " |          `batch_size` elements; the default behavior is not to drop the smaller\n",
      " |          batch.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If a component has an unknown rank, and  the `padded_shapes`\n",
      " |          argument is not set.\n",
      " |  \n",
      " |  prefetch(self, buffer_size)\n",
      " |      Creates a `Dataset` that prefetches elements from this dataset.\n",
      " |      \n",
      " |      Most dataset input pipelines should end with a call to `prefetch`. This\n",
      " |      allows later elements to be prepared while the current element is being\n",
      " |      processed. This often improves latency and throughput, at the cost of\n",
      " |      using additional memory to store prefetched elements.\n",
      " |      \n",
      " |      Note: Like other `Dataset` methods, prefetch operates on the\n",
      " |      elements of the input dataset. It has no concept of examples vs. batches.\n",
      " |      `examples.prefetch(2)` will prefetch two elements (2 examples),\n",
      " |      while `examples.batch(20).prefetch(2)` will prefetch 2 elements\n",
      " |      (2 batches, of 20 examples each).\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(3)\n",
      " |      >>> dataset = dataset.prefetch(2)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [0, 1, 2]\n",
      " |      \n",
      " |      Args:\n",
      " |        buffer_size: A `tf.int64` scalar `tf.Tensor`, representing the maximum\n",
      " |          number of elements that will be buffered when prefetching.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  reduce(self, initial_state, reduce_func)\n",
      " |      Reduces the input dataset to a single element.\n",
      " |      \n",
      " |      The transformation calls `reduce_func` successively on every element of\n",
      " |      the input dataset until the dataset is exhausted, aggregating information in\n",
      " |      its internal state. The `initial_state` argument is used for the initial\n",
      " |      state and the final state is returned as the result.\n",
      " |      \n",
      " |      >>> tf.data.Dataset.range(5).reduce(np.int64(0), lambda x, _: x + 1).numpy()\n",
      " |      5\n",
      " |      >>> tf.data.Dataset.range(5).reduce(np.int64(0), lambda x, y: x + y).numpy()\n",
      " |      10\n",
      " |      \n",
      " |      Args:\n",
      " |        initial_state: An element representing the initial state of the\n",
      " |          transformation.\n",
      " |        reduce_func: A function that maps `(old_state, input_element)` to\n",
      " |          `new_state`. It must take two arguments and return a new element\n",
      " |          The structure of `new_state` must match the structure of\n",
      " |          `initial_state`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A dataset element corresponding to the final state of the transformation.\n",
      " |  \n",
      " |  repeat(self, count=None)\n",
      " |      Repeats this dataset so each original value is seen `count` times.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
      " |      >>> dataset = dataset.repeat(3)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [1, 2, 3, 1, 2, 3, 1, 2, 3]\n",
      " |      \n",
      " |      Note: If this dataset is a function of global state (e.g. a random number\n",
      " |      generator), then different repetitions may produce different elements.\n",
      " |      \n",
      " |      Args:\n",
      " |        count: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the\n",
      " |          number of times the dataset should be repeated. The default behavior (if\n",
      " |          `count` is `None` or `-1`) is for the dataset be repeated indefinitely.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  shard(self, num_shards, index)\n",
      " |      Creates a `Dataset` that includes only 1/`num_shards` of this dataset.\n",
      " |      \n",
      " |      `shard` is deterministic. The Dataset produced by `A.shard(n, i)` will\n",
      " |      contain all elements of A whose index mod n = i.\n",
      " |      \n",
      " |      >>> A = tf.data.Dataset.range(10)\n",
      " |      >>> B = A.shard(num_shards=3, index=0)\n",
      " |      >>> list(B.as_numpy_iterator())\n",
      " |      [0, 3, 6, 9]\n",
      " |      >>> C = A.shard(num_shards=3, index=1)\n",
      " |      >>> list(C.as_numpy_iterator())\n",
      " |      [1, 4, 7]\n",
      " |      >>> D = A.shard(num_shards=3, index=2)\n",
      " |      >>> list(D.as_numpy_iterator())\n",
      " |      [2, 5, 8]\n",
      " |      \n",
      " |      This dataset operator is very useful when running distributed training, as\n",
      " |      it allows each worker to read a unique subset.\n",
      " |      \n",
      " |      When reading a single input file, you can shard elements as follows:\n",
      " |      \n",
      " |      ```python\n",
      " |      d = tf.data.TFRecordDataset(input_file)\n",
      " |      d = d.shard(num_workers, worker_index)\n",
      " |      d = d.repeat(num_epochs)\n",
      " |      d = d.shuffle(shuffle_buffer_size)\n",
      " |      d = d.map(parser_fn, num_parallel_calls=num_map_threads)\n",
      " |      ```\n",
      " |      \n",
      " |      Important caveats:\n",
      " |      \n",
      " |      - Be sure to shard before you use any randomizing operator (such as\n",
      " |        shuffle).\n",
      " |      - Generally it is best if the shard operator is used early in the dataset\n",
      " |        pipeline. For example, when reading from a set of TFRecord files, shard\n",
      " |        before converting the dataset to input samples. This avoids reading every\n",
      " |        file on every worker. The following is an example of an efficient\n",
      " |        sharding strategy within a complete pipeline:\n",
      " |      \n",
      " |      ```python\n",
      " |      d = Dataset.list_files(pattern)\n",
      " |      d = d.shard(num_workers, worker_index)\n",
      " |      d = d.repeat(num_epochs)\n",
      " |      d = d.shuffle(shuffle_buffer_size)\n",
      " |      d = d.interleave(tf.data.TFRecordDataset,\n",
      " |                       cycle_length=num_readers, block_length=1)\n",
      " |      d = d.map(parser_fn, num_parallel_calls=num_map_threads)\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        num_shards: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          shards operating in parallel.\n",
      " |        index: A `tf.int64` scalar `tf.Tensor`, representing the worker index.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        InvalidArgumentError: if `num_shards` or `index` are illegal values.\n",
      " |      \n",
      " |          Note: error checking is done on a best-effort basis, and errors aren't\n",
      " |          guaranteed to be caught upon dataset creation. (e.g. providing in a\n",
      " |          placeholder tensor bypasses the early checking, and will instead result\n",
      " |          in an error during a session.run call.)\n",
      " |  \n",
      " |  shuffle(self, buffer_size, seed=None, reshuffle_each_iteration=None)\n",
      " |      Randomly shuffles the elements of this dataset.\n",
      " |      \n",
      " |      This dataset fills a buffer with `buffer_size` elements, then randomly\n",
      " |      samples elements from this buffer, replacing the selected elements with new\n",
      " |      elements. For perfect shuffling, a buffer size greater than or equal to the\n",
      " |      full size of the dataset is required.\n",
      " |      \n",
      " |      For instance, if your dataset contains 10,000 elements but `buffer_size` is\n",
      " |      set to 1,000, then `shuffle` will initially select a random element from\n",
      " |      only the first 1,000 elements in the buffer. Once an element is selected,\n",
      " |      its space in the buffer is replaced by the next (i.e. 1,001-st) element,\n",
      " |      maintaining the 1,000 element buffer.\n",
      " |      \n",
      " |      `reshuffle_each_iteration` controls whether the shuffle order should be\n",
      " |      different for each epoch. In TF 1.X, the idiomatic way to create epochs\n",
      " |      was through the `repeat` transformation:\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(3)\n",
      " |      >>> dataset = dataset.shuffle(3, reshuffle_each_iteration=True)\n",
      " |      >>> dataset = dataset.repeat(2)  # doctest: +SKIP\n",
      " |      [1, 0, 2, 1, 2, 0]\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(3)\n",
      " |      >>> dataset = dataset.shuffle(3, reshuffle_each_iteration=False)\n",
      " |      >>> dataset = dataset.repeat(2)  # doctest: +SKIP\n",
      " |      [1, 0, 2, 1, 0, 2]\n",
      " |      \n",
      " |      In TF 2.0, `tf.data.Dataset` objects are Python iterables which makes it\n",
      " |      possible to also create epochs through Python iteration:\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(3)\n",
      " |      >>> dataset = dataset.shuffle(3, reshuffle_each_iteration=True)\n",
      " |      >>> list(dataset.as_numpy_iterator())  # doctest: +SKIP\n",
      " |      [1, 0, 2]\n",
      " |      >>> list(dataset.as_numpy_iterator())  # doctest: +SKIP\n",
      " |      [1, 2, 0]\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(3)\n",
      " |      >>> dataset = dataset.shuffle(3, reshuffle_each_iteration=False)\n",
      " |      >>> list(dataset.as_numpy_iterator())  # doctest: +SKIP\n",
      " |      [1, 0, 2]\n",
      " |      >>> list(dataset.as_numpy_iterator())  # doctest: +SKIP\n",
      " |      [1, 0, 2]\n",
      " |      \n",
      " |      Args:\n",
      " |        buffer_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          elements from this dataset from which the new dataset will sample.\n",
      " |        seed: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the random\n",
      " |          seed that will be used to create the distribution. See\n",
      " |          `tf.random.set_seed` for behavior.\n",
      " |        reshuffle_each_iteration: (Optional.) A boolean, which if true indicates\n",
      " |          that the dataset should be pseudorandomly reshuffled each time it is\n",
      " |          iterated over. (Defaults to `True`.)\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  skip(self, count)\n",
      " |      Creates a `Dataset` that skips `count` elements from this dataset.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(10)\n",
      " |      >>> dataset = dataset.skip(7)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [7, 8, 9]\n",
      " |      \n",
      " |      Args:\n",
      " |        count: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          elements of this dataset that should be skipped to form the new dataset.\n",
      " |          If `count` is greater than the size of this dataset, the new dataset\n",
      " |          will contain no elements.  If `count` is -1, skips the entire dataset.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  take(self, count)\n",
      " |      Creates a `Dataset` with at most `count` elements from this dataset.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(10)\n",
      " |      >>> dataset = dataset.take(3)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [0, 1, 2]\n",
      " |      \n",
      " |      Args:\n",
      " |        count: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          elements of this dataset that should be taken to form the new dataset.\n",
      " |          If `count` is -1, or if `count` is greater than the size of this\n",
      " |          dataset, the new dataset will contain all elements of this dataset.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  unbatch(self)\n",
      " |      Splits elements of a dataset into multiple elements.\n",
      " |      \n",
      " |      For example, if elements of the dataset are shaped `[B, a0, a1, ...]`,\n",
      " |      where `B` may vary for each input element, then for each element in the\n",
      " |      dataset, the unbatched dataset will contain `B` consecutive elements\n",
      " |      of shape `[a0, a1, ...]`.\n",
      " |      \n",
      " |      >>> elements = [ [1, 2, 3], [1, 2], [1, 2, 3, 4] ]\n",
      " |      >>> dataset = tf.data.Dataset.from_generator(lambda: elements, tf.int64)\n",
      " |      >>> dataset = dataset.unbatch()\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [1, 2, 3, 1, 2, 1, 2, 3, 4]\n",
      " |      \n",
      " |      Note: `unbatch` requires a data copy to slice up the batched tensor into\n",
      " |      smaller, unbatched tensors. When optimizing performance, try to avoid\n",
      " |      unnecessary usage of `unbatch`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Dataset`.\n",
      " |  \n",
      " |  window(self, size, shift=None, stride=1, drop_remainder=False)\n",
      " |      Combines (nests of) input elements into a dataset of (nests of) windows.\n",
      " |      \n",
      " |      A \"window\" is a finite dataset of flat elements of size `size` (or possibly\n",
      " |      fewer if there are not enough input elements to fill the window and\n",
      " |      `drop_remainder` evaluates to `False`).\n",
      " |      \n",
      " |      The `shift` argument determines the number of input elements by which the\n",
      " |      window moves on each iteration.  If windows and elements are both numbered\n",
      " |      starting at 0, the first element in window `k` will be element `k * shift`\n",
      " |      of the input dataset. In particular, the first element of the first window\n",
      " |      will always be the first element of the input dataset.\n",
      " |      \n",
      " |      The `stride` argument determines the stride of the input elements, and the\n",
      " |      `shift` argument determines the shift of the window.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(7).window(2)\n",
      " |      >>> for window in dataset:\n",
      " |      ...   print(list(window.as_numpy_iterator()))\n",
      " |      [0, 1]\n",
      " |      [2, 3]\n",
      " |      [4, 5]\n",
      " |      [6]\n",
      " |      >>> dataset = tf.data.Dataset.range(7).window(3, 2, 1, True)\n",
      " |      >>> for window in dataset:\n",
      " |      ...   print(list(window.as_numpy_iterator()))\n",
      " |      [0, 1, 2]\n",
      " |      [2, 3, 4]\n",
      " |      [4, 5, 6]\n",
      " |      >>> dataset = tf.data.Dataset.range(7).window(3, 1, 2, True)\n",
      " |      >>> for window in dataset:\n",
      " |      ...   print(list(window.as_numpy_iterator()))\n",
      " |      [0, 2, 4]\n",
      " |      [1, 3, 5]\n",
      " |      [2, 4, 6]\n",
      " |      \n",
      " |      Note that when the `window` transformation is applied to a dataset of\n",
      " |      nested elements, it produces a dataset of nested windows.\n",
      " |      \n",
      " |      >>> nested = ([1, 2, 3, 4], [5, 6, 7, 8])\n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices(nested).window(2)\n",
      " |      >>> for window in dataset:\n",
      " |      ...   def to_numpy(ds):\n",
      " |      ...     return list(ds.as_numpy_iterator())\n",
      " |      ...   print(tuple(to_numpy(component) for component in window))\n",
      " |      ([1, 2], [5, 6])\n",
      " |      ([3, 4], [7, 8])\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices({'a': [1, 2, 3, 4]})\n",
      " |      >>> dataset = dataset.window(2)\n",
      " |      >>> for window in dataset:\n",
      " |      ...   def to_numpy(ds):\n",
      " |      ...     return list(ds.as_numpy_iterator())\n",
      " |      ...   print({'a': to_numpy(window['a'])})\n",
      " |      {'a': [1, 2]}\n",
      " |      {'a': [3, 4]}\n",
      " |      \n",
      " |      Args:\n",
      " |        size: A `tf.int64` scalar `tf.Tensor`, representing the number of elements\n",
      " |          of the input dataset to combine into a window. Must be positive.\n",
      " |        shift: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the\n",
      " |          number of input elements by which the window moves in each iteration.\n",
      " |          Defaults to `size`. Must be positive.\n",
      " |        stride: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the\n",
      " |          stride of the input elements in the sliding window. Must be positive.\n",
      " |          The default value of 1 means \"retain every input element\".\n",
      " |        drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\n",
      " |          whether the last windows should be dropped if their size is smaller than\n",
      " |          `size`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset` of (nests of) windows -- a finite datasets of flat\n",
      " |          elements created from the (nests of) input elements.\n",
      " |  \n",
      " |  with_options(self, options)\n",
      " |      Returns a new `tf.data.Dataset` with the given options set.\n",
      " |      \n",
      " |      The options are \"global\" in the sense they apply to the entire dataset.\n",
      " |      If options are set multiple times, they are merged as long as different\n",
      " |      options do not use different non-default values.\n",
      " |      \n",
      " |      >>> ds = tf.data.Dataset.range(5)\n",
      " |      >>> ds = ds.interleave(lambda x: tf.data.Dataset.range(5),\n",
      " |      ...                    cycle_length=3,\n",
      " |      ...                    num_parallel_calls=3)\n",
      " |      >>> options = tf.data.Options()\n",
      " |      >>> # This will make the interleave order non-deterministic.\n",
      " |      >>> options.experimental_deterministic = False\n",
      " |      >>> ds = ds.with_options(options)\n",
      " |      \n",
      " |      Args:\n",
      " |        options: A `tf.data.Options` that identifies the options the use.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset` with the given options.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: when an option is set more than once to a non-default value\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from DatasetV2:\n",
      " |  \n",
      " |  from_generator(generator, output_types=None, output_shapes=None, args=None, output_signature=None)\n",
      " |      Creates a `Dataset` whose elements are generated by `generator`. (deprecated arguments)\n",
      " |      \n",
      " |      Warning: SOME ARGUMENTS ARE DEPRECATED: `(output_shapes, output_types)`. They will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Use output_signature instead\n",
      " |      \n",
      " |      The `generator` argument must be a callable object that returns\n",
      " |      an object that supports the `iter()` protocol (e.g. a generator function).\n",
      " |      \n",
      " |      The elements generated by `generator` must be compatible with either the\n",
      " |      given `output_signature` argument or with the given `output_types` and\n",
      " |      (optionally) `output_shapes` arguments, whichiver was specified.\n",
      " |      \n",
      " |      The recommended way to call `from_generator` is to use the\n",
      " |      `output_signature` argument. In this case the output will be assumed to\n",
      " |      consist of objects with the classes, shapes and types defined by\n",
      " |      `tf.TypeSpec` objects from `output_signature` argument:\n",
      " |      \n",
      " |      >>> def gen():\n",
      " |      ...   ragged_tensor = tf.ragged.constant([[1, 2], [3]])\n",
      " |      ...   yield 42, ragged_tensor\n",
      " |      >>>\n",
      " |      >>> dataset = tf.data.Dataset.from_generator(\n",
      " |      ...      gen,\n",
      " |      ...      output_signature=(\n",
      " |      ...          tf.TensorSpec(shape=(), dtype=tf.int32),\n",
      " |      ...          tf.RaggedTensorSpec(shape=(2, None), dtype=tf.int32)))\n",
      " |      >>>\n",
      " |      >>> list(dataset.take(1))\n",
      " |      [(<tf.Tensor: shape=(), dtype=int32, numpy=42>,\n",
      " |      <tf.RaggedTensor [[1, 2], [3]]>)]\n",
      " |      \n",
      " |      There is also a deprecated way to call `from_generator` by either with\n",
      " |      `output_types` argument alone or together with `output_shapes` argument.\n",
      " |      In this case the output of the function will be assumed to consist of\n",
      " |      `tf.Tensor` objects with with the types defined by `output_types` and with\n",
      " |      the shapes which are either unknown or defined by `output_shapes`.\n",
      " |      \n",
      " |      Note: The current implementation of `Dataset.from_generator()` uses\n",
      " |      `tf.numpy_function` and inherits the same constraints. In particular, it\n",
      " |      requires the dataset and iterator related operations to be placed\n",
      " |      on a device in the same process as the Python program that called\n",
      " |      `Dataset.from_generator()`. The body of `generator` will not be\n",
      " |      serialized in a `GraphDef`, and you should not use this method if you\n",
      " |      need to serialize your model and restore it in a different environment.\n",
      " |      \n",
      " |      Note: If `generator` depends on mutable global variables or other external\n",
      " |      state, be aware that the runtime may invoke `generator` multiple times\n",
      " |      (in order to support repeating the `Dataset`) and at any time\n",
      " |      between the call to `Dataset.from_generator()` and the production of the\n",
      " |      first element from the generator. Mutating global variables or external\n",
      " |      state can cause undefined behavior, and we recommend that you explicitly\n",
      " |      cache any external state in `generator` before calling\n",
      " |      `Dataset.from_generator()`.\n",
      " |      \n",
      " |      Args:\n",
      " |        generator: A callable object that returns an object that supports the\n",
      " |          `iter()` protocol. If `args` is not specified, `generator` must take no\n",
      " |          arguments; otherwise it must take as many arguments as there are values\n",
      " |          in `args`.\n",
      " |        output_types: (Optional.) A nested structure of `tf.DType` objects\n",
      " |          corresponding to each component of an element yielded by `generator`.\n",
      " |        output_shapes: (Optional.) A nested structure of `tf.TensorShape` objects\n",
      " |          corresponding to each component of an element yielded by `generator`.\n",
      " |        args: (Optional.) A tuple of `tf.Tensor` objects that will be evaluated\n",
      " |          and passed to `generator` as NumPy-array arguments.\n",
      " |        output_signature: (Optional.) A nested structure of `tf.TypeSpec` objects\n",
      " |          corresponding to each component of an element yielded by `generator`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  from_tensor_slices(tensors)\n",
      " |      Creates a `Dataset` whose elements are slices of the given tensors.\n",
      " |      \n",
      " |      The given tensors are sliced along their first dimension. This operation\n",
      " |      preserves the structure of the input tensors, removing the first dimension\n",
      " |      of each tensor and using it as the dataset dimension. All input tensors\n",
      " |      must have the same size in their first dimensions.\n",
      " |      \n",
      " |      >>> # Slicing a 1D tensor produces scalar tensor elements.\n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [1, 2, 3]\n",
      " |      \n",
      " |      >>> # Slicing a 2D tensor produces 1D tensor elements.\n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices([[1, 2], [3, 4]])\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [array([1, 2], dtype=int32), array([3, 4], dtype=int32)]\n",
      " |      \n",
      " |      >>> # Slicing a tuple of 1D tensors produces tuple elements containing\n",
      " |      >>> # scalar tensors.\n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices(([1, 2], [3, 4], [5, 6]))\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [(1, 3, 5), (2, 4, 6)]\n",
      " |      \n",
      " |      >>> # Dictionary structure is also preserved.\n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices({\"a\": [1, 2], \"b\": [3, 4]})\n",
      " |      >>> list(dataset.as_numpy_iterator()) == [{'a': 1, 'b': 3},\n",
      " |      ...                                       {'a': 2, 'b': 4}]\n",
      " |      True\n",
      " |      \n",
      " |      >>> # Two tensors can be combined into one Dataset object.\n",
      " |      >>> features = tf.constant([[1, 3], [2, 1], [3, 3]]) # ==> 3x2 tensor\n",
      " |      >>> labels = tf.constant(['A', 'B', 'A']) # ==> 3x1 tensor\n",
      " |      >>> dataset = Dataset.from_tensor_slices((features, labels))\n",
      " |      >>> # Both the features and the labels tensors can be converted\n",
      " |      >>> # to a Dataset object separately and combined after.\n",
      " |      >>> features_dataset = Dataset.from_tensor_slices(features)\n",
      " |      >>> labels_dataset = Dataset.from_tensor_slices(labels)\n",
      " |      >>> dataset = Dataset.zip((features_dataset, labels_dataset))\n",
      " |      >>> # A batched feature and label set can be converted to a Dataset\n",
      " |      >>> # in similar fashion.\n",
      " |      >>> batched_features = tf.constant([[[1, 3], [2, 3]],\n",
      " |      ...                                 [[2, 1], [1, 2]],\n",
      " |      ...                                 [[3, 3], [3, 2]]], shape=(3, 2, 2))\n",
      " |      >>> batched_labels = tf.constant([['A', 'A'],\n",
      " |      ...                               ['B', 'B'],\n",
      " |      ...                               ['A', 'B']], shape=(3, 2, 1))\n",
      " |      >>> dataset = Dataset.from_tensor_slices((batched_features, batched_labels))\n",
      " |      >>> for element in dataset.as_numpy_iterator():\n",
      " |      ...   print(element)\n",
      " |      (array([[1, 3],\n",
      " |             [2, 3]], dtype=int32), array([[b'A'],\n",
      " |             [b'A']], dtype=object))\n",
      " |      (array([[2, 1],\n",
      " |             [1, 2]], dtype=int32), array([[b'B'],\n",
      " |             [b'B']], dtype=object))\n",
      " |      (array([[3, 3],\n",
      " |             [3, 2]], dtype=int32), array([[b'A'],\n",
      " |             [b'B']], dtype=object))\n",
      " |      \n",
      " |      Note that if `tensors` contains a NumPy array, and eager execution is not\n",
      " |      enabled, the values will be embedded in the graph as one or more\n",
      " |      `tf.constant` operations. For large datasets (> 1 GB), this can waste\n",
      " |      memory and run into byte limits of graph serialization. If `tensors`\n",
      " |      contains one or more large NumPy arrays, consider the alternative described\n",
      " |      in [this guide](\n",
      " |      https://tensorflow.org/guide/data#consuming_numpy_arrays).\n",
      " |      \n",
      " |      Args:\n",
      " |        tensors: A dataset element, with each component having the same size in\n",
      " |          the first dimension.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  from_tensors(tensors)\n",
      " |      Creates a `Dataset` with a single element, comprising the given tensors.\n",
      " |      \n",
      " |      `from_tensors` produces a dataset containing only a single element. To slice\n",
      " |      the input tensor into multiple elements, use `from_tensor_slices` instead.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.from_tensors([1, 2, 3])\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [array([1, 2, 3], dtype=int32)]\n",
      " |      >>> dataset = tf.data.Dataset.from_tensors(([1, 2, 3], 'A'))\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [(array([1, 2, 3], dtype=int32), b'A')]\n",
      " |      \n",
      " |      >>> # You can use `from_tensors` to produce a dataset which repeats\n",
      " |      >>> # the same example many times.\n",
      " |      >>> example = tf.constant([1,2,3])\n",
      " |      >>> dataset = tf.data.Dataset.from_tensors(example).repeat(2)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [array([1, 2, 3], dtype=int32), array([1, 2, 3], dtype=int32)]\n",
      " |      \n",
      " |      Note that if `tensors` contains a NumPy array, and eager execution is not\n",
      " |      enabled, the values will be embedded in the graph as one or more\n",
      " |      `tf.constant` operations. For large datasets (> 1 GB), this can waste\n",
      " |      memory and run into byte limits of graph serialization. If `tensors`\n",
      " |      contains one or more large NumPy arrays, consider the alternative described\n",
      " |      in [this\n",
      " |      guide](https://tensorflow.org/guide/data#consuming_numpy_arrays).\n",
      " |      \n",
      " |      Args:\n",
      " |        tensors: A dataset element.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  list_files(file_pattern, shuffle=None, seed=None)\n",
      " |      A dataset of all files matching one or more glob patterns.\n",
      " |      \n",
      " |      The `file_pattern` argument should be a small number of glob patterns.\n",
      " |      If your filenames have already been globbed, use\n",
      " |      `Dataset.from_tensor_slices(filenames)` instead, as re-globbing every\n",
      " |      filename with `list_files` may result in poor performance with remote\n",
      " |      storage systems.\n",
      " |      \n",
      " |      Note: The default behavior of this method is to return filenames in\n",
      " |      a non-deterministic random shuffled order. Pass a `seed` or `shuffle=False`\n",
      " |      to get results in a deterministic order.\n",
      " |      \n",
      " |      Example:\n",
      " |        If we had the following files on our filesystem:\n",
      " |      \n",
      " |          - /path/to/dir/a.txt\n",
      " |          - /path/to/dir/b.py\n",
      " |          - /path/to/dir/c.py\n",
      " |      \n",
      " |        If we pass \"/path/to/dir/*.py\" as the directory, the dataset\n",
      " |        would produce:\n",
      " |      \n",
      " |          - /path/to/dir/b.py\n",
      " |          - /path/to/dir/c.py\n",
      " |      \n",
      " |      Args:\n",
      " |        file_pattern: A string, a list of strings, or a `tf.Tensor` of string type\n",
      " |          (scalar or vector), representing the filename glob (i.e. shell wildcard)\n",
      " |          pattern(s) that will be matched.\n",
      " |        shuffle: (Optional.) If `True`, the file names will be shuffled randomly.\n",
      " |          Defaults to `True`.\n",
      " |        seed: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the random\n",
      " |          seed that will be used to create the distribution. See\n",
      " |          `tf.random.set_seed` for behavior.\n",
      " |      \n",
      " |      Returns:\n",
      " |       Dataset: A `Dataset` of strings corresponding to file names.\n",
      " |  \n",
      " |  range(*args, **kwargs)\n",
      " |      Creates a `Dataset` of a step-separated range of values.\n",
      " |      \n",
      " |      >>> list(Dataset.range(5).as_numpy_iterator())\n",
      " |      [0, 1, 2, 3, 4]\n",
      " |      >>> list(Dataset.range(2, 5).as_numpy_iterator())\n",
      " |      [2, 3, 4]\n",
      " |      >>> list(Dataset.range(1, 5, 2).as_numpy_iterator())\n",
      " |      [1, 3]\n",
      " |      >>> list(Dataset.range(1, 5, -2).as_numpy_iterator())\n",
      " |      []\n",
      " |      >>> list(Dataset.range(5, 1).as_numpy_iterator())\n",
      " |      []\n",
      " |      >>> list(Dataset.range(5, 1, -2).as_numpy_iterator())\n",
      " |      [5, 3]\n",
      " |      >>> list(Dataset.range(2, 5, output_type=tf.int32).as_numpy_iterator())\n",
      " |      [2, 3, 4]\n",
      " |      >>> list(Dataset.range(1, 5, 2, output_type=tf.float32).as_numpy_iterator())\n",
      " |      [1.0, 3.0]\n",
      " |      \n",
      " |      Args:\n",
      " |        *args: follows the same semantics as python's xrange.\n",
      " |          len(args) == 1 -> start = 0, stop = args[0], step = 1.\n",
      " |          len(args) == 2 -> start = args[0], stop = args[1], step = 1.\n",
      " |          len(args) == 3 -> start = args[0], stop = args[1], step = args[2].\n",
      " |        **kwargs:\n",
      " |          - output_type: Its expected dtype. (Optional, default: `tf.int64`).\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `RangeDataset`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if len(args) == 0.\n",
      " |  \n",
      " |  zip(datasets)\n",
      " |      Creates a `Dataset` by zipping together the given datasets.\n",
      " |      \n",
      " |      This method has similar semantics to the built-in `zip()` function\n",
      " |      in Python, with the main difference being that the `datasets`\n",
      " |      argument can be an arbitrary nested structure of `Dataset` objects.\n",
      " |      \n",
      " |      >>> # The nested structure of the `datasets` argument determines the\n",
      " |      >>> # structure of elements in the resulting dataset.\n",
      " |      >>> a = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\n",
      " |      >>> b = tf.data.Dataset.range(4, 7)  # ==> [ 4, 5, 6 ]\n",
      " |      >>> ds = tf.data.Dataset.zip((a, b))\n",
      " |      >>> list(ds.as_numpy_iterator())\n",
      " |      [(1, 4), (2, 5), (3, 6)]\n",
      " |      >>> ds = tf.data.Dataset.zip((b, a))\n",
      " |      >>> list(ds.as_numpy_iterator())\n",
      " |      [(4, 1), (5, 2), (6, 3)]\n",
      " |      >>>\n",
      " |      >>> # The `datasets` argument may contain an arbitrary number of datasets.\n",
      " |      >>> c = tf.data.Dataset.range(7, 13).batch(2)  # ==> [ [7, 8],\n",
      " |      ...                                            #       [9, 10],\n",
      " |      ...                                            #       [11, 12] ]\n",
      " |      >>> ds = tf.data.Dataset.zip((a, b, c))\n",
      " |      >>> for element in ds.as_numpy_iterator():\n",
      " |      ...   print(element)\n",
      " |      (1, 4, array([7, 8]))\n",
      " |      (2, 5, array([ 9, 10]))\n",
      " |      (3, 6, array([11, 12]))\n",
      " |      >>>\n",
      " |      >>> # The number of elements in the resulting dataset is the same as\n",
      " |      >>> # the size of the smallest dataset in `datasets`.\n",
      " |      >>> d = tf.data.Dataset.range(13, 15)  # ==> [ 13, 14 ]\n",
      " |      >>> ds = tf.data.Dataset.zip((a, d))\n",
      " |      >>> list(ds.as_numpy_iterator())\n",
      " |      [(1, 13), (2, 14)]\n",
      " |      \n",
      " |      Args:\n",
      " |        datasets: A nested structure of datasets.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from DatasetV2:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from collections.abc.Iterable:\n",
      " |  \n",
      " |  __subclasshook__(C) from abc.ABCMeta\n",
      " |      Abstract classes can override this to customize issubclass().\n",
      " |      \n",
      " |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      " |      It should return True, False or NotImplemented.  If it returns\n",
      " |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      " |      overrides the normal algorithm (and the outcome is cached).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(gen.training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'albumentations'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-ad2138c2ae91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# import tensorflow_datasets as tfds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m from albumentations import (\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mCompose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomBrightness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJpegCompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHueSaturationValue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomContrast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHorizontalFlip\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mRotate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'albumentations'"
     ]
    }
   ],
   "source": [
    "# necessary imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import tensorflow_datasets as tfds\n",
    "from functools import partial\n",
    "from albumentations import (\n",
    "    Compose, RandomBrightness, JpegCompression, HueSaturationValue, RandomContrast, HorizontalFlip,\n",
    "    Rotate\n",
    ")\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100, 18)\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "# TODO test on entire test dataset\n",
    "obj = s3.Object('canopy-production-ml', \"chips/cloudfree-merge-polygons/split/test/100/100_1000_1000.tif\")\n",
    "obj_bytes = io.BytesIO(obj.get()['Body'].read())\n",
    "with rasterio.open(obj_bytes) as src:\n",
    "    img_test = np.transpose(src.read(), (1, 2, 0))\n",
    "print(img_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = tf.image.convert_image_dtype(img_test,tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = tf.image.random_flip_left_right(train_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package tensorflow._api.v2.image in tensorflow._api.v2:\n",
      "\n",
      "NAME\n",
      "    tensorflow._api.v2.image - Image ops.\n",
      "\n",
      "DESCRIPTION\n",
      "    The `tf.image` module contains various functions for image\n",
      "    processing and decoding-encoding Ops.\n",
      "    \n",
      "    Many of the encoding/decoding functions are also available in the\n",
      "    core `tf.io` module.\n",
      "    \n",
      "    ## Image processing\n",
      "    \n",
      "    ### Resizing\n",
      "    \n",
      "    The resizing Ops accept input images as tensors of several types. They always\n",
      "    output resized images as float32 tensors.\n",
      "    \n",
      "    The convenience function `tf.image.resize` supports both 4-D\n",
      "    and 3-D tensors as input and output.  4-D tensors are for batches of images,\n",
      "    3-D tensors for individual images.\n",
      "    \n",
      "    Resized images will be distorted if their original aspect ratio is not the\n",
      "    same as size. To avoid distortions see tf.image.resize_with_pad.\n",
      "    \n",
      "    *   `tf.image.resize`\n",
      "    *   `tf.image.resize_with_pad`\n",
      "    *   `tf.image.resize_with_crop_or_pad`\n",
      "    \n",
      "    The Class `tf.image.ResizeMethod` provides various resize methods like\n",
      "    `bilinear`, `nearest_neighbor`.\n",
      "    \n",
      "    ### Converting Between Colorspaces\n",
      "    \n",
      "    Image ops work either on individual images or on batches of images, depending on\n",
      "    the shape of their input Tensor.\n",
      "    \n",
      "    If 3-D, the shape is `[height, width, channels]`, and the Tensor represents one\n",
      "    image. If 4-D, the shape is `[batch_size, height, width, channels]`, and the\n",
      "    Tensor represents `batch_size` images.\n",
      "    \n",
      "    Currently, `channels` can usefully be 1, 2, 3, or 4. Single-channel images are\n",
      "    grayscale, images with 3 channels are encoded as either RGB or HSV. Images\n",
      "    with 2 or 4 channels include an alpha channel, which has to be stripped from the\n",
      "    image before passing the image to most image processing functions (and can be\n",
      "    re-attached later).\n",
      "    \n",
      "    Internally, images are either stored in as one `float32` per channel per pixel\n",
      "    (implicitly, values are assumed to lie in `[0,1)`) or one `uint8` per channel\n",
      "    per pixel (values are assumed to lie in `[0,255]`).\n",
      "    \n",
      "    TensorFlow can convert between images in RGB or HSV or YIQ.\n",
      "    \n",
      "    *   `tf.image.rgb_to_grayscale`, `tf.image.grayscale_to_rgb`\n",
      "    *   `tf.image.rgb_to_hsv`, `tf.image.hsv_to_rgb`\n",
      "    *   `tf.image.rgb_to_yiq`, `tf.image.yiq_to_rgb`\n",
      "    *   `tf.image.rgb_to_yuv`, `tf.image.yuv_to_rgb`\n",
      "    *   `tf.image.image_gradients`\n",
      "    *   `tf.image.convert_image_dtype`\n",
      "    \n",
      "    ### Image Adjustments\n",
      "    \n",
      "    TensorFlow provides functions to adjust images in various ways: brightness,\n",
      "    contrast, hue, and saturation.  Each adjustment can be done with predefined\n",
      "    parameters or with random parameters picked from predefined intervals. Random\n",
      "    adjustments are often useful to expand a training set and reduce overfitting.\n",
      "    \n",
      "    If several adjustments are chained it is advisable to minimize the number of\n",
      "    redundant conversions by first converting the images to the most natural data\n",
      "    type and representation.\n",
      "    \n",
      "    *   `tf.image.adjust_brightness`\n",
      "    *   `tf.image.adjust_contrast`\n",
      "    *   `tf.image.adjust_gamma`\n",
      "    *   `tf.image.adjust_hue`\n",
      "    *   `tf.image.adjust_jpeg_quality`\n",
      "    *   `tf.image.adjust_saturation`\n",
      "    *   `tf.image.random_brightness`\n",
      "    *   `tf.image.random_contrast`\n",
      "    *   `tf.image.random_hue`\n",
      "    *   `tf.image.random_saturation`\n",
      "    *   `tf.image.per_image_standardization`\n",
      "    \n",
      "    ### Working with Bounding Boxes\n",
      "    \n",
      "    *   `tf.image.draw_bounding_boxes`\n",
      "    *   `tf.image.combined_non_max_suppression`\n",
      "    *   `tf.image.generate_bounding_box_proposals`\n",
      "    *   `tf.image.non_max_suppression`\n",
      "    *   `tf.image.non_max_suppression_overlaps`\n",
      "    *   `tf.image.non_max_suppression_padded`\n",
      "    *   `tf.image.non_max_suppression_with_scores`\n",
      "    *   `tf.image.pad_to_bounding_box`\n",
      "    *   `tf.image.sample_distorted_bounding_box`\n",
      "    \n",
      "    ### Cropping\n",
      "    \n",
      "    *   `tf.image.central_crop`\n",
      "    *   `tf.image.crop_and_resize`\n",
      "    *   `tf.image.crop_to_bounding_box`\n",
      "    *   `tf.io.decode_and_crop_jpeg`\n",
      "    *   `tf.image.extract_glimpse`\n",
      "    *   `tf.image.random_crop`\n",
      "    *   `tf.image.resize_with_crop_or_pad`\n",
      "    \n",
      "    ### Flipping, Rotating and Transposing\n",
      "    \n",
      "    *   `tf.image.flip_left_right`\n",
      "    *   `tf.image.flip_up_down`\n",
      "    *   `tf.image.random_flip_left_right`\n",
      "    *   `tf.image.random_flip_up_down`\n",
      "    *   `tf.image.rot90`\n",
      "    *   `tf.image.transpose`\n",
      "    \n",
      "    ## Image decoding and encoding\n",
      "    \n",
      "    TensorFlow provides Ops to decode and encode JPEG and PNG formats.  Encoded\n",
      "    images are represented by scalar string Tensors, decoded images by 3-D uint8\n",
      "    tensors of shape `[height, width, channels]`. (PNG also supports uint16.)\n",
      "    \n",
      "    Note: `decode_gif` returns a 4-D array `[num_frames, height, width, 3]`\n",
      "    \n",
      "    The encode and decode Ops apply to one image at a time.  Their input and output\n",
      "    are all of variable size.  If you need fixed size images, pass the output of\n",
      "    the decode Ops to one of the cropping and resizing Ops.\n",
      "    \n",
      "    *   `tf.io.decode_bmp`\n",
      "    *   `tf.io.decode_gif`\n",
      "    *   `tf.io.decode_image`\n",
      "    *   `tf.io.decode_jpeg`\n",
      "    *   `tf.io.decode_and_crop_jpeg`\n",
      "    *   `tf.io.decode_png`\n",
      "    *   `tf.io.encode_jpeg`\n",
      "    *   `tf.io.encode_png`\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "\n",
      "\n",
      "FILE\n",
      "    /Users/purgatorid/opt/anaconda3/envs/ml-conda/lib/python3.7/site-packages/tensorflow/_api/v2/image/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-conda",
   "language": "python",
   "name": "ml-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
